{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.02665126  0.0990985  -0.03884164 ...  0.03455577  0.01290239\n",
      "   0.07733775]\n",
      " [ 0.10659064  0.20882843  0.07350808 ... -0.01763076 -0.00997995\n",
      "   0.24828523]]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
    "\n",
    "model = SentenceTransformer('/home/ralf/models/S-BioELECTRA')\n",
    "embeddings = model.encode(sentences)\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_decision_forests as tfdf\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import json\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished embedding ROTVRSV\n",
      "Finished embedding DENV\n",
      "Made datasets\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "text_key = \"originalTitle\"\n",
    "dataset_path = \"/home/ralf/IdeaProjects/LitBall-training/EXP-Title+TLDR/\"\n",
    "bsize = 30\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def preprocess_sentence(text):\n",
    "    text = text.replace('/', ' / ')\n",
    "    text = text.replace('.-', ' .- ')\n",
    "    text = text.replace('.', ' . ')\n",
    "    text = text.replace('\\'', ' \\' ')\n",
    "    text = text.lower()\n",
    "\n",
    "    tokens = [token for token in word_tokenize(text) if token not in punctuation and token not in stop_words]\n",
    "\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "farr1 = []\n",
    "larr1 = []\n",
    "with open(dataset_path + \"ROTVRSV\") as file:\n",
    "    lines = file.readlines()\n",
    "    for line in lines:\n",
    "        d = json.loads(line)\n",
    "        farr1.append(np.asarray(model.encode(preprocess_sentence(d[text_key]))).astype('float32'))\n",
    "        larr1.append(int(d[\"label\"]))\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((np.array(farr1), np.array(larr1))).batch(bsize)\n",
    "print(\"Finished embedding ROTVRSV\")\n",
    "\n",
    "farr2 = []\n",
    "larr2 = []\n",
    "with open(dataset_path + \"DENV\") as file:\n",
    "    test_lines = file.readlines()\n",
    "    for line in test_lines:\n",
    "        d = json.loads(line)\n",
    "        farr2.append(np.asarray(model.encode(preprocess_sentence(d[text_key]))).astype('float32'))\n",
    "        larr2.append(int(d[\"label\"]))\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((np.array(farr2), np.array(larr2))).batch(bsize)\n",
    "print(\"Finished embedding DENV\")\n",
    "print(\"Made datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8679\n",
      "Data loaded.\n",
      "Use /tmp/tmpqx6vxa1k as temporary training directory\n",
      "Warning: Model constructor argument batch_size=None not supported. See https://www.tensorflow.org/decision_forests/migration for an explanation about the specificities of TF-DF.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Model constructor argument batch_size=None not supported. See https://www.tensorflow.org/decision_forests/migration for an explanation about the specificities of TF-DF.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading training dataset...\n",
      "Training tensor examples:\n",
      "Features: Tensor(\"data:0\", shape=(None, 768), dtype=float32)\n",
      "Label: Tensor(\"data_1:0\", shape=(None,), dtype=int64)\n",
      "Weights: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-29 19:58:03.608088: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype int64 and shape [22996]\n",
      "\t [[{{node Placeholder/_1}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized tensor features:\n",
      " {'data:0.0': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice:0' shape=(None,) dtype=float32>), 'data:0.1': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_1:0' shape=(None,) dtype=float32>), 'data:0.2': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_2:0' shape=(None,) dtype=float32>), 'data:0.3': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_3:0' shape=(None,) dtype=float32>), 'data:0.4': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_4:0' shape=(None,) dtype=float32>), 'data:0.5': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_5:0' shape=(None,) dtype=float32>), 'data:0.6': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_6:0' shape=(None,) dtype=float32>), 'data:0.7': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_7:0' shape=(None,) dtype=float32>), 'data:0.8': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_8:0' shape=(None,) dtype=float32>), 'data:0.9': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_9:0' shape=(None,) dtype=float32>), 'data:0.10': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_10:0' shape=(None,) dtype=float32>), 'data:0.11': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_11:0' shape=(None,) dtype=float32>), 'data:0.12': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_12:0' shape=(None,) dtype=float32>), 'data:0.13': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_13:0' shape=(None,) dtype=float32>), 'data:0.14': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_14:0' shape=(None,) dtype=float32>), 'data:0.15': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_15:0' shape=(None,) dtype=float32>), 'data:0.16': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_16:0' shape=(None,) dtype=float32>), 'data:0.17': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_17:0' shape=(None,) dtype=float32>), 'data:0.18': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_18:0' shape=(None,) dtype=float32>), 'data:0.19': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_19:0' shape=(None,) dtype=float32>), 'data:0.20': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_20:0' shape=(None,) dtype=float32>), 'data:0.21': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_21:0' shape=(None,) dtype=float32>), 'data:0.22': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_22:0' shape=(None,) dtype=float32>), 'data:0.23': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_23:0' shape=(None,) dtype=float32>), 'data:0.24': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_24:0' shape=(None,) dtype=float32>), 'data:0.25': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_25:0' shape=(None,) dtype=float32>), 'data:0.26': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_26:0' shape=(None,) dtype=float32>), 'data:0.27': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_27:0' shape=(None,) dtype=float32>), 'data:0.28': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_28:0' shape=(None,) dtype=float32>), 'data:0.29': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_29:0' shape=(None,) dtype=float32>), 'data:0.30': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_30:0' shape=(None,) dtype=float32>), 'data:0.31': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_31:0' shape=(None,) dtype=float32>), 'data:0.32': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_32:0' shape=(None,) dtype=float32>), 'data:0.33': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_33:0' shape=(None,) dtype=float32>), 'data:0.34': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_34:0' shape=(None,) dtype=float32>), 'data:0.35': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_35:0' shape=(None,) dtype=float32>), 'data:0.36': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_36:0' shape=(None,) dtype=float32>), 'data:0.37': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_37:0' shape=(None,) dtype=float32>), 'data:0.38': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_38:0' shape=(None,) dtype=float32>), 'data:0.39': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_39:0' shape=(None,) dtype=float32>), 'data:0.40': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_40:0' shape=(None,) dtype=float32>), 'data:0.41': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_41:0' shape=(None,) dtype=float32>), 'data:0.42': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_42:0' shape=(None,) dtype=float32>), 'data:0.43': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_43:0' shape=(None,) dtype=float32>), 'data:0.44': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_44:0' shape=(None,) dtype=float32>), 'data:0.45': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_45:0' shape=(None,) dtype=float32>), 'data:0.46': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_46:0' shape=(None,) dtype=float32>), 'data:0.47': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_47:0' shape=(None,) dtype=float32>), 'data:0.48': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_48:0' shape=(None,) dtype=float32>), 'data:0.49': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_49:0' shape=(None,) dtype=float32>), 'data:0.50': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_50:0' shape=(None,) dtype=float32>), 'data:0.51': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_51:0' shape=(None,) dtype=float32>), 'data:0.52': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_52:0' shape=(None,) dtype=float32>), 'data:0.53': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_53:0' shape=(None,) dtype=float32>), 'data:0.54': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_54:0' shape=(None,) dtype=float32>), 'data:0.55': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_55:0' shape=(None,) dtype=float32>), 'data:0.56': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_56:0' shape=(None,) dtype=float32>), 'data:0.57': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_57:0' shape=(None,) dtype=float32>), 'data:0.58': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_58:0' shape=(None,) dtype=float32>), 'data:0.59': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_59:0' shape=(None,) dtype=float32>), 'data:0.60': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_60:0' shape=(None,) dtype=float32>), 'data:0.61': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_61:0' shape=(None,) dtype=float32>), 'data:0.62': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_62:0' shape=(None,) dtype=float32>), 'data:0.63': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_63:0' shape=(None,) dtype=float32>), 'data:0.64': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_64:0' shape=(None,) dtype=float32>), 'data:0.65': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_65:0' shape=(None,) dtype=float32>), 'data:0.66': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_66:0' shape=(None,) dtype=float32>), 'data:0.67': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_67:0' shape=(None,) dtype=float32>), 'data:0.68': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_68:0' shape=(None,) dtype=float32>), 'data:0.69': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_69:0' shape=(None,) dtype=float32>), 'data:0.70': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_70:0' shape=(None,) dtype=float32>), 'data:0.71': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_71:0' shape=(None,) dtype=float32>), 'data:0.72': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_72:0' shape=(None,) dtype=float32>), 'data:0.73': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_73:0' shape=(None,) dtype=float32>), 'data:0.74': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_74:0' shape=(None,) dtype=float32>), 'data:0.75': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_75:0' shape=(None,) dtype=float32>), 'data:0.76': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_76:0' shape=(None,) dtype=float32>), 'data:0.77': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_77:0' shape=(None,) dtype=float32>), 'data:0.78': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_78:0' shape=(None,) dtype=float32>), 'data:0.79': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_79:0' shape=(None,) dtype=float32>), 'data:0.80': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_80:0' shape=(None,) dtype=float32>), 'data:0.81': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_81:0' shape=(None,) dtype=float32>), 'data:0.82': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_82:0' shape=(None,) dtype=float32>), 'data:0.83': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_83:0' shape=(None,) dtype=float32>), 'data:0.84': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_84:0' shape=(None,) dtype=float32>), 'data:0.85': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_85:0' shape=(None,) dtype=float32>), 'data:0.86': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_86:0' shape=(None,) dtype=float32>), 'data:0.87': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_87:0' shape=(None,) dtype=float32>), 'data:0.88': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_88:0' shape=(None,) dtype=float32>), 'data:0.89': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_89:0' shape=(None,) dtype=float32>), 'data:0.90': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_90:0' shape=(None,) dtype=float32>), 'data:0.91': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_91:0' shape=(None,) dtype=float32>), 'data:0.92': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_92:0' shape=(None,) dtype=float32>), 'data:0.93': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_93:0' shape=(None,) dtype=float32>), 'data:0.94': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_94:0' shape=(None,) dtype=float32>), 'data:0.95': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_95:0' shape=(None,) dtype=float32>), 'data:0.96': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_96:0' shape=(None,) dtype=float32>), 'data:0.97': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_97:0' shape=(None,) dtype=float32>), 'data:0.98': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_98:0' shape=(None,) dtype=float32>), 'data:0.99': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_99:0' shape=(None,) dtype=float32>), 'data:0.100': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_100:0' shape=(None,) dtype=float32>), 'data:0.101': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_101:0' shape=(None,) dtype=float32>), 'data:0.102': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_102:0' shape=(None,) dtype=float32>), 'data:0.103': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_103:0' shape=(None,) dtype=float32>), 'data:0.104': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_104:0' shape=(None,) dtype=float32>), 'data:0.105': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_105:0' shape=(None,) dtype=float32>), 'data:0.106': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_106:0' shape=(None,) dtype=float32>), 'data:0.107': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_107:0' shape=(None,) dtype=float32>), 'data:0.108': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_108:0' shape=(None,) dtype=float32>), 'data:0.109': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_109:0' shape=(None,) dtype=float32>), 'data:0.110': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_110:0' shape=(None,) dtype=float32>), 'data:0.111': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_111:0' shape=(None,) dtype=float32>), 'data:0.112': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_112:0' shape=(None,) dtype=float32>), 'data:0.113': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_113:0' shape=(None,) dtype=float32>), 'data:0.114': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_114:0' shape=(None,) dtype=float32>), 'data:0.115': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_115:0' shape=(None,) dtype=float32>), 'data:0.116': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_116:0' shape=(None,) dtype=float32>), 'data:0.117': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_117:0' shape=(None,) dtype=float32>), 'data:0.118': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_118:0' shape=(None,) dtype=float32>), 'data:0.119': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_119:0' shape=(None,) dtype=float32>), 'data:0.120': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_120:0' shape=(None,) dtype=float32>), 'data:0.121': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_121:0' shape=(None,) dtype=float32>), 'data:0.122': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_122:0' shape=(None,) dtype=float32>), 'data:0.123': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_123:0' shape=(None,) dtype=float32>), 'data:0.124': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_124:0' shape=(None,) dtype=float32>), 'data:0.125': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_125:0' shape=(None,) dtype=float32>), 'data:0.126': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_126:0' shape=(None,) dtype=float32>), 'data:0.127': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_127:0' shape=(None,) dtype=float32>), 'data:0.128': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_128:0' shape=(None,) dtype=float32>), 'data:0.129': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_129:0' shape=(None,) dtype=float32>), 'data:0.130': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_130:0' shape=(None,) dtype=float32>), 'data:0.131': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_131:0' shape=(None,) dtype=float32>), 'data:0.132': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_132:0' shape=(None,) dtype=float32>), 'data:0.133': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_133:0' shape=(None,) dtype=float32>), 'data:0.134': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_134:0' shape=(None,) dtype=float32>), 'data:0.135': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_135:0' shape=(None,) dtype=float32>), 'data:0.136': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_136:0' shape=(None,) dtype=float32>), 'data:0.137': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_137:0' shape=(None,) dtype=float32>), 'data:0.138': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_138:0' shape=(None,) dtype=float32>), 'data:0.139': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_139:0' shape=(None,) dtype=float32>), 'data:0.140': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_140:0' shape=(None,) dtype=float32>), 'data:0.141': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_141:0' shape=(None,) dtype=float32>), 'data:0.142': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_142:0' shape=(None,) dtype=float32>), 'data:0.143': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_143:0' shape=(None,) dtype=float32>), 'data:0.144': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_144:0' shape=(None,) dtype=float32>), 'data:0.145': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_145:0' shape=(None,) dtype=float32>), 'data:0.146': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_146:0' shape=(None,) dtype=float32>), 'data:0.147': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_147:0' shape=(None,) dtype=float32>), 'data:0.148': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_148:0' shape=(None,) dtype=float32>), 'data:0.149': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_149:0' shape=(None,) dtype=float32>), 'data:0.150': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_150:0' shape=(None,) dtype=float32>), 'data:0.151': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_151:0' shape=(None,) dtype=float32>), 'data:0.152': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_152:0' shape=(None,) dtype=float32>), 'data:0.153': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_153:0' shape=(None,) dtype=float32>), 'data:0.154': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_154:0' shape=(None,) dtype=float32>), 'data:0.155': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_155:0' shape=(None,) dtype=float32>), 'data:0.156': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_156:0' shape=(None,) dtype=float32>), 'data:0.157': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_157:0' shape=(None,) dtype=float32>), 'data:0.158': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_158:0' shape=(None,) dtype=float32>), 'data:0.159': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_159:0' shape=(None,) dtype=float32>), 'data:0.160': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_160:0' shape=(None,) dtype=float32>), 'data:0.161': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_161:0' shape=(None,) dtype=float32>), 'data:0.162': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_162:0' shape=(None,) dtype=float32>), 'data:0.163': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_163:0' shape=(None,) dtype=float32>), 'data:0.164': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_164:0' shape=(None,) dtype=float32>), 'data:0.165': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_165:0' shape=(None,) dtype=float32>), 'data:0.166': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_166:0' shape=(None,) dtype=float32>), 'data:0.167': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_167:0' shape=(None,) dtype=float32>), 'data:0.168': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_168:0' shape=(None,) dtype=float32>), 'data:0.169': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_169:0' shape=(None,) dtype=float32>), 'data:0.170': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_170:0' shape=(None,) dtype=float32>), 'data:0.171': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_171:0' shape=(None,) dtype=float32>), 'data:0.172': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_172:0' shape=(None,) dtype=float32>), 'data:0.173': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_173:0' shape=(None,) dtype=float32>), 'data:0.174': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_174:0' shape=(None,) dtype=float32>), 'data:0.175': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_175:0' shape=(None,) dtype=float32>), 'data:0.176': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_176:0' shape=(None,) dtype=float32>), 'data:0.177': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_177:0' shape=(None,) dtype=float32>), 'data:0.178': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_178:0' shape=(None,) dtype=float32>), 'data:0.179': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_179:0' shape=(None,) dtype=float32>), 'data:0.180': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_180:0' shape=(None,) dtype=float32>), 'data:0.181': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_181:0' shape=(None,) dtype=float32>), 'data:0.182': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_182:0' shape=(None,) dtype=float32>), 'data:0.183': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_183:0' shape=(None,) dtype=float32>), 'data:0.184': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_184:0' shape=(None,) dtype=float32>), 'data:0.185': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_185:0' shape=(None,) dtype=float32>), 'data:0.186': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_186:0' shape=(None,) dtype=float32>), 'data:0.187': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_187:0' shape=(None,) dtype=float32>), 'data:0.188': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_188:0' shape=(None,) dtype=float32>), 'data:0.189': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_189:0' shape=(None,) dtype=float32>), 'data:0.190': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_190:0' shape=(None,) dtype=float32>), 'data:0.191': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_191:0' shape=(None,) dtype=float32>), 'data:0.192': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_192:0' shape=(None,) dtype=float32>), 'data:0.193': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_193:0' shape=(None,) dtype=float32>), 'data:0.194': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_194:0' shape=(None,) dtype=float32>), 'data:0.195': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_195:0' shape=(None,) dtype=float32>), 'data:0.196': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_196:0' shape=(None,) dtype=float32>), 'data:0.197': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_197:0' shape=(None,) dtype=float32>), 'data:0.198': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_198:0' shape=(None,) dtype=float32>), 'data:0.199': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_199:0' shape=(None,) dtype=float32>), 'data:0.200': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_200:0' shape=(None,) dtype=float32>), 'data:0.201': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_201:0' shape=(None,) dtype=float32>), 'data:0.202': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_202:0' shape=(None,) dtype=float32>), 'data:0.203': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_203:0' shape=(None,) dtype=float32>), 'data:0.204': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_204:0' shape=(None,) dtype=float32>), 'data:0.205': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_205:0' shape=(None,) dtype=float32>), 'data:0.206': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_206:0' shape=(None,) dtype=float32>), 'data:0.207': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_207:0' shape=(None,) dtype=float32>), 'data:0.208': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_208:0' shape=(None,) dtype=float32>), 'data:0.209': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_209:0' shape=(None,) dtype=float32>), 'data:0.210': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_210:0' shape=(None,) dtype=float32>), 'data:0.211': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_211:0' shape=(None,) dtype=float32>), 'data:0.212': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_212:0' shape=(None,) dtype=float32>), 'data:0.213': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_213:0' shape=(None,) dtype=float32>), 'data:0.214': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_214:0' shape=(None,) dtype=float32>), 'data:0.215': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_215:0' shape=(None,) dtype=float32>), 'data:0.216': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_216:0' shape=(None,) dtype=float32>), 'data:0.217': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_217:0' shape=(None,) dtype=float32>), 'data:0.218': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_218:0' shape=(None,) dtype=float32>), 'data:0.219': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_219:0' shape=(None,) dtype=float32>), 'data:0.220': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_220:0' shape=(None,) dtype=float32>), 'data:0.221': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_221:0' shape=(None,) dtype=float32>), 'data:0.222': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_222:0' shape=(None,) dtype=float32>), 'data:0.223': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_223:0' shape=(None,) dtype=float32>), 'data:0.224': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_224:0' shape=(None,) dtype=float32>), 'data:0.225': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_225:0' shape=(None,) dtype=float32>), 'data:0.226': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_226:0' shape=(None,) dtype=float32>), 'data:0.227': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_227:0' shape=(None,) dtype=float32>), 'data:0.228': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_228:0' shape=(None,) dtype=float32>), 'data:0.229': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_229:0' shape=(None,) dtype=float32>), 'data:0.230': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_230:0' shape=(None,) dtype=float32>), 'data:0.231': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_231:0' shape=(None,) dtype=float32>), 'data:0.232': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_232:0' shape=(None,) dtype=float32>), 'data:0.233': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_233:0' shape=(None,) dtype=float32>), 'data:0.234': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_234:0' shape=(None,) dtype=float32>), 'data:0.235': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_235:0' shape=(None,) dtype=float32>), 'data:0.236': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_236:0' shape=(None,) dtype=float32>), 'data:0.237': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_237:0' shape=(None,) dtype=float32>), 'data:0.238': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_238:0' shape=(None,) dtype=float32>), 'data:0.239': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_239:0' shape=(None,) dtype=float32>), 'data:0.240': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_240:0' shape=(None,) dtype=float32>), 'data:0.241': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_241:0' shape=(None,) dtype=float32>), 'data:0.242': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_242:0' shape=(None,) dtype=float32>), 'data:0.243': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_243:0' shape=(None,) dtype=float32>), 'data:0.244': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_244:0' shape=(None,) dtype=float32>), 'data:0.245': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_245:0' shape=(None,) dtype=float32>), 'data:0.246': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_246:0' shape=(None,) dtype=float32>), 'data:0.247': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_247:0' shape=(None,) dtype=float32>), 'data:0.248': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_248:0' shape=(None,) dtype=float32>), 'data:0.249': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_249:0' shape=(None,) dtype=float32>), 'data:0.250': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_250:0' shape=(None,) dtype=float32>), 'data:0.251': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_251:0' shape=(None,) dtype=float32>), 'data:0.252': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_252:0' shape=(None,) dtype=float32>), 'data:0.253': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_253:0' shape=(None,) dtype=float32>), 'data:0.254': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_254:0' shape=(None,) dtype=float32>), 'data:0.255': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_255:0' shape=(None,) dtype=float32>), 'data:0.256': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_256:0' shape=(None,) dtype=float32>), 'data:0.257': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_257:0' shape=(None,) dtype=float32>), 'data:0.258': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_258:0' shape=(None,) dtype=float32>), 'data:0.259': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_259:0' shape=(None,) dtype=float32>), 'data:0.260': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_260:0' shape=(None,) dtype=float32>), 'data:0.261': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_261:0' shape=(None,) dtype=float32>), 'data:0.262': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_262:0' shape=(None,) dtype=float32>), 'data:0.263': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_263:0' shape=(None,) dtype=float32>), 'data:0.264': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_264:0' shape=(None,) dtype=float32>), 'data:0.265': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_265:0' shape=(None,) dtype=float32>), 'data:0.266': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_266:0' shape=(None,) dtype=float32>), 'data:0.267': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_267:0' shape=(None,) dtype=float32>), 'data:0.268': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_268:0' shape=(None,) dtype=float32>), 'data:0.269': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_269:0' shape=(None,) dtype=float32>), 'data:0.270': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_270:0' shape=(None,) dtype=float32>), 'data:0.271': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_271:0' shape=(None,) dtype=float32>), 'data:0.272': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_272:0' shape=(None,) dtype=float32>), 'data:0.273': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_273:0' shape=(None,) dtype=float32>), 'data:0.274': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_274:0' shape=(None,) dtype=float32>), 'data:0.275': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_275:0' shape=(None,) dtype=float32>), 'data:0.276': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_276:0' shape=(None,) dtype=float32>), 'data:0.277': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_277:0' shape=(None,) dtype=float32>), 'data:0.278': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_278:0' shape=(None,) dtype=float32>), 'data:0.279': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_279:0' shape=(None,) dtype=float32>), 'data:0.280': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_280:0' shape=(None,) dtype=float32>), 'data:0.281': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_281:0' shape=(None,) dtype=float32>), 'data:0.282': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_282:0' shape=(None,) dtype=float32>), 'data:0.283': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_283:0' shape=(None,) dtype=float32>), 'data:0.284': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_284:0' shape=(None,) dtype=float32>), 'data:0.285': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_285:0' shape=(None,) dtype=float32>), 'data:0.286': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_286:0' shape=(None,) dtype=float32>), 'data:0.287': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_287:0' shape=(None,) dtype=float32>), 'data:0.288': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_288:0' shape=(None,) dtype=float32>), 'data:0.289': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_289:0' shape=(None,) dtype=float32>), 'data:0.290': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_290:0' shape=(None,) dtype=float32>), 'data:0.291': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_291:0' shape=(None,) dtype=float32>), 'data:0.292': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_292:0' shape=(None,) dtype=float32>), 'data:0.293': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_293:0' shape=(None,) dtype=float32>), 'data:0.294': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_294:0' shape=(None,) dtype=float32>), 'data:0.295': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_295:0' shape=(None,) dtype=float32>), 'data:0.296': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_296:0' shape=(None,) dtype=float32>), 'data:0.297': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_297:0' shape=(None,) dtype=float32>), 'data:0.298': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_298:0' shape=(None,) dtype=float32>), 'data:0.299': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_299:0' shape=(None,) dtype=float32>), 'data:0.300': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_300:0' shape=(None,) dtype=float32>), 'data:0.301': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_301:0' shape=(None,) dtype=float32>), 'data:0.302': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_302:0' shape=(None,) dtype=float32>), 'data:0.303': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_303:0' shape=(None,) dtype=float32>), 'data:0.304': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_304:0' shape=(None,) dtype=float32>), 'data:0.305': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_305:0' shape=(None,) dtype=float32>), 'data:0.306': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_306:0' shape=(None,) dtype=float32>), 'data:0.307': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_307:0' shape=(None,) dtype=float32>), 'data:0.308': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_308:0' shape=(None,) dtype=float32>), 'data:0.309': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_309:0' shape=(None,) dtype=float32>), 'data:0.310': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_310:0' shape=(None,) dtype=float32>), 'data:0.311': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_311:0' shape=(None,) dtype=float32>), 'data:0.312': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_312:0' shape=(None,) dtype=float32>), 'data:0.313': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_313:0' shape=(None,) dtype=float32>), 'data:0.314': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_314:0' shape=(None,) dtype=float32>), 'data:0.315': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_315:0' shape=(None,) dtype=float32>), 'data:0.316': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_316:0' shape=(None,) dtype=float32>), 'data:0.317': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_317:0' shape=(None,) dtype=float32>), 'data:0.318': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_318:0' shape=(None,) dtype=float32>), 'data:0.319': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_319:0' shape=(None,) dtype=float32>), 'data:0.320': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_320:0' shape=(None,) dtype=float32>), 'data:0.321': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_321:0' shape=(None,) dtype=float32>), 'data:0.322': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_322:0' shape=(None,) dtype=float32>), 'data:0.323': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_323:0' shape=(None,) dtype=float32>), 'data:0.324': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_324:0' shape=(None,) dtype=float32>), 'data:0.325': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_325:0' shape=(None,) dtype=float32>), 'data:0.326': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_326:0' shape=(None,) dtype=float32>), 'data:0.327': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_327:0' shape=(None,) dtype=float32>), 'data:0.328': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_328:0' shape=(None,) dtype=float32>), 'data:0.329': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_329:0' shape=(None,) dtype=float32>), 'data:0.330': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_330:0' shape=(None,) dtype=float32>), 'data:0.331': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_331:0' shape=(None,) dtype=float32>), 'data:0.332': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_332:0' shape=(None,) dtype=float32>), 'data:0.333': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_333:0' shape=(None,) dtype=float32>), 'data:0.334': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_334:0' shape=(None,) dtype=float32>), 'data:0.335': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_335:0' shape=(None,) dtype=float32>), 'data:0.336': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_336:0' shape=(None,) dtype=float32>), 'data:0.337': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_337:0' shape=(None,) dtype=float32>), 'data:0.338': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_338:0' shape=(None,) dtype=float32>), 'data:0.339': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_339:0' shape=(None,) dtype=float32>), 'data:0.340': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_340:0' shape=(None,) dtype=float32>), 'data:0.341': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_341:0' shape=(None,) dtype=float32>), 'data:0.342': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_342:0' shape=(None,) dtype=float32>), 'data:0.343': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_343:0' shape=(None,) dtype=float32>), 'data:0.344': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_344:0' shape=(None,) dtype=float32>), 'data:0.345': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_345:0' shape=(None,) dtype=float32>), 'data:0.346': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_346:0' shape=(None,) dtype=float32>), 'data:0.347': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_347:0' shape=(None,) dtype=float32>), 'data:0.348': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_348:0' shape=(None,) dtype=float32>), 'data:0.349': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_349:0' shape=(None,) dtype=float32>), 'data:0.350': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_350:0' shape=(None,) dtype=float32>), 'data:0.351': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_351:0' shape=(None,) dtype=float32>), 'data:0.352': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_352:0' shape=(None,) dtype=float32>), 'data:0.353': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_353:0' shape=(None,) dtype=float32>), 'data:0.354': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_354:0' shape=(None,) dtype=float32>), 'data:0.355': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_355:0' shape=(None,) dtype=float32>), 'data:0.356': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_356:0' shape=(None,) dtype=float32>), 'data:0.357': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_357:0' shape=(None,) dtype=float32>), 'data:0.358': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_358:0' shape=(None,) dtype=float32>), 'data:0.359': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_359:0' shape=(None,) dtype=float32>), 'data:0.360': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_360:0' shape=(None,) dtype=float32>), 'data:0.361': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_361:0' shape=(None,) dtype=float32>), 'data:0.362': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_362:0' shape=(None,) dtype=float32>), 'data:0.363': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_363:0' shape=(None,) dtype=float32>), 'data:0.364': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_364:0' shape=(None,) dtype=float32>), 'data:0.365': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_365:0' shape=(None,) dtype=float32>), 'data:0.366': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_366:0' shape=(None,) dtype=float32>), 'data:0.367': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_367:0' shape=(None,) dtype=float32>), 'data:0.368': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_368:0' shape=(None,) dtype=float32>), 'data:0.369': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_369:0' shape=(None,) dtype=float32>), 'data:0.370': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_370:0' shape=(None,) dtype=float32>), 'data:0.371': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_371:0' shape=(None,) dtype=float32>), 'data:0.372': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_372:0' shape=(None,) dtype=float32>), 'data:0.373': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_373:0' shape=(None,) dtype=float32>), 'data:0.374': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_374:0' shape=(None,) dtype=float32>), 'data:0.375': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_375:0' shape=(None,) dtype=float32>), 'data:0.376': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_376:0' shape=(None,) dtype=float32>), 'data:0.377': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_377:0' shape=(None,) dtype=float32>), 'data:0.378': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_378:0' shape=(None,) dtype=float32>), 'data:0.379': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_379:0' shape=(None,) dtype=float32>), 'data:0.380': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_380:0' shape=(None,) dtype=float32>), 'data:0.381': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_381:0' shape=(None,) dtype=float32>), 'data:0.382': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_382:0' shape=(None,) dtype=float32>), 'data:0.383': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_383:0' shape=(None,) dtype=float32>), 'data:0.384': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_384:0' shape=(None,) dtype=float32>), 'data:0.385': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_385:0' shape=(None,) dtype=float32>), 'data:0.386': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_386:0' shape=(None,) dtype=float32>), 'data:0.387': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_387:0' shape=(None,) dtype=float32>), 'data:0.388': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_388:0' shape=(None,) dtype=float32>), 'data:0.389': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_389:0' shape=(None,) dtype=float32>), 'data:0.390': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_390:0' shape=(None,) dtype=float32>), 'data:0.391': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_391:0' shape=(None,) dtype=float32>), 'data:0.392': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_392:0' shape=(None,) dtype=float32>), 'data:0.393': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_393:0' shape=(None,) dtype=float32>), 'data:0.394': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_394:0' shape=(None,) dtype=float32>), 'data:0.395': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_395:0' shape=(None,) dtype=float32>), 'data:0.396': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_396:0' shape=(None,) dtype=float32>), 'data:0.397': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_397:0' shape=(None,) dtype=float32>), 'data:0.398': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_398:0' shape=(None,) dtype=float32>), 'data:0.399': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_399:0' shape=(None,) dtype=float32>), 'data:0.400': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_400:0' shape=(None,) dtype=float32>), 'data:0.401': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_401:0' shape=(None,) dtype=float32>), 'data:0.402': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_402:0' shape=(None,) dtype=float32>), 'data:0.403': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_403:0' shape=(None,) dtype=float32>), 'data:0.404': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_404:0' shape=(None,) dtype=float32>), 'data:0.405': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_405:0' shape=(None,) dtype=float32>), 'data:0.406': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_406:0' shape=(None,) dtype=float32>), 'data:0.407': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_407:0' shape=(None,) dtype=float32>), 'data:0.408': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_408:0' shape=(None,) dtype=float32>), 'data:0.409': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_409:0' shape=(None,) dtype=float32>), 'data:0.410': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_410:0' shape=(None,) dtype=float32>), 'data:0.411': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_411:0' shape=(None,) dtype=float32>), 'data:0.412': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_412:0' shape=(None,) dtype=float32>), 'data:0.413': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_413:0' shape=(None,) dtype=float32>), 'data:0.414': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_414:0' shape=(None,) dtype=float32>), 'data:0.415': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_415:0' shape=(None,) dtype=float32>), 'data:0.416': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_416:0' shape=(None,) dtype=float32>), 'data:0.417': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_417:0' shape=(None,) dtype=float32>), 'data:0.418': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_418:0' shape=(None,) dtype=float32>), 'data:0.419': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_419:0' shape=(None,) dtype=float32>), 'data:0.420': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_420:0' shape=(None,) dtype=float32>), 'data:0.421': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_421:0' shape=(None,) dtype=float32>), 'data:0.422': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_422:0' shape=(None,) dtype=float32>), 'data:0.423': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_423:0' shape=(None,) dtype=float32>), 'data:0.424': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_424:0' shape=(None,) dtype=float32>), 'data:0.425': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_425:0' shape=(None,) dtype=float32>), 'data:0.426': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_426:0' shape=(None,) dtype=float32>), 'data:0.427': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_427:0' shape=(None,) dtype=float32>), 'data:0.428': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_428:0' shape=(None,) dtype=float32>), 'data:0.429': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_429:0' shape=(None,) dtype=float32>), 'data:0.430': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_430:0' shape=(None,) dtype=float32>), 'data:0.431': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_431:0' shape=(None,) dtype=float32>), 'data:0.432': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_432:0' shape=(None,) dtype=float32>), 'data:0.433': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_433:0' shape=(None,) dtype=float32>), 'data:0.434': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_434:0' shape=(None,) dtype=float32>), 'data:0.435': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_435:0' shape=(None,) dtype=float32>), 'data:0.436': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_436:0' shape=(None,) dtype=float32>), 'data:0.437': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_437:0' shape=(None,) dtype=float32>), 'data:0.438': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_438:0' shape=(None,) dtype=float32>), 'data:0.439': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_439:0' shape=(None,) dtype=float32>), 'data:0.440': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_440:0' shape=(None,) dtype=float32>), 'data:0.441': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_441:0' shape=(None,) dtype=float32>), 'data:0.442': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_442:0' shape=(None,) dtype=float32>), 'data:0.443': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_443:0' shape=(None,) dtype=float32>), 'data:0.444': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_444:0' shape=(None,) dtype=float32>), 'data:0.445': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_445:0' shape=(None,) dtype=float32>), 'data:0.446': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_446:0' shape=(None,) dtype=float32>), 'data:0.447': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_447:0' shape=(None,) dtype=float32>), 'data:0.448': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_448:0' shape=(None,) dtype=float32>), 'data:0.449': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_449:0' shape=(None,) dtype=float32>), 'data:0.450': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_450:0' shape=(None,) dtype=float32>), 'data:0.451': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_451:0' shape=(None,) dtype=float32>), 'data:0.452': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_452:0' shape=(None,) dtype=float32>), 'data:0.453': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_453:0' shape=(None,) dtype=float32>), 'data:0.454': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_454:0' shape=(None,) dtype=float32>), 'data:0.455': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_455:0' shape=(None,) dtype=float32>), 'data:0.456': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_456:0' shape=(None,) dtype=float32>), 'data:0.457': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_457:0' shape=(None,) dtype=float32>), 'data:0.458': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_458:0' shape=(None,) dtype=float32>), 'data:0.459': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_459:0' shape=(None,) dtype=float32>), 'data:0.460': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_460:0' shape=(None,) dtype=float32>), 'data:0.461': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_461:0' shape=(None,) dtype=float32>), 'data:0.462': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_462:0' shape=(None,) dtype=float32>), 'data:0.463': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_463:0' shape=(None,) dtype=float32>), 'data:0.464': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_464:0' shape=(None,) dtype=float32>), 'data:0.465': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_465:0' shape=(None,) dtype=float32>), 'data:0.466': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_466:0' shape=(None,) dtype=float32>), 'data:0.467': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_467:0' shape=(None,) dtype=float32>), 'data:0.468': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_468:0' shape=(None,) dtype=float32>), 'data:0.469': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_469:0' shape=(None,) dtype=float32>), 'data:0.470': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_470:0' shape=(None,) dtype=float32>), 'data:0.471': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_471:0' shape=(None,) dtype=float32>), 'data:0.472': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_472:0' shape=(None,) dtype=float32>), 'data:0.473': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_473:0' shape=(None,) dtype=float32>), 'data:0.474': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_474:0' shape=(None,) dtype=float32>), 'data:0.475': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_475:0' shape=(None,) dtype=float32>), 'data:0.476': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_476:0' shape=(None,) dtype=float32>), 'data:0.477': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_477:0' shape=(None,) dtype=float32>), 'data:0.478': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_478:0' shape=(None,) dtype=float32>), 'data:0.479': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_479:0' shape=(None,) dtype=float32>), 'data:0.480': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_480:0' shape=(None,) dtype=float32>), 'data:0.481': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_481:0' shape=(None,) dtype=float32>), 'data:0.482': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_482:0' shape=(None,) dtype=float32>), 'data:0.483': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_483:0' shape=(None,) dtype=float32>), 'data:0.484': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_484:0' shape=(None,) dtype=float32>), 'data:0.485': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_485:0' shape=(None,) dtype=float32>), 'data:0.486': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_486:0' shape=(None,) dtype=float32>), 'data:0.487': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_487:0' shape=(None,) dtype=float32>), 'data:0.488': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_488:0' shape=(None,) dtype=float32>), 'data:0.489': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_489:0' shape=(None,) dtype=float32>), 'data:0.490': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_490:0' shape=(None,) dtype=float32>), 'data:0.491': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_491:0' shape=(None,) dtype=float32>), 'data:0.492': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_492:0' shape=(None,) dtype=float32>), 'data:0.493': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_493:0' shape=(None,) dtype=float32>), 'data:0.494': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_494:0' shape=(None,) dtype=float32>), 'data:0.495': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_495:0' shape=(None,) dtype=float32>), 'data:0.496': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_496:0' shape=(None,) dtype=float32>), 'data:0.497': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_497:0' shape=(None,) dtype=float32>), 'data:0.498': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_498:0' shape=(None,) dtype=float32>), 'data:0.499': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_499:0' shape=(None,) dtype=float32>), 'data:0.500': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_500:0' shape=(None,) dtype=float32>), 'data:0.501': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_501:0' shape=(None,) dtype=float32>), 'data:0.502': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_502:0' shape=(None,) dtype=float32>), 'data:0.503': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_503:0' shape=(None,) dtype=float32>), 'data:0.504': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_504:0' shape=(None,) dtype=float32>), 'data:0.505': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_505:0' shape=(None,) dtype=float32>), 'data:0.506': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_506:0' shape=(None,) dtype=float32>), 'data:0.507': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_507:0' shape=(None,) dtype=float32>), 'data:0.508': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_508:0' shape=(None,) dtype=float32>), 'data:0.509': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_509:0' shape=(None,) dtype=float32>), 'data:0.510': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_510:0' shape=(None,) dtype=float32>), 'data:0.511': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_511:0' shape=(None,) dtype=float32>), 'data:0.512': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_512:0' shape=(None,) dtype=float32>), 'data:0.513': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_513:0' shape=(None,) dtype=float32>), 'data:0.514': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_514:0' shape=(None,) dtype=float32>), 'data:0.515': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_515:0' shape=(None,) dtype=float32>), 'data:0.516': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_516:0' shape=(None,) dtype=float32>), 'data:0.517': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_517:0' shape=(None,) dtype=float32>), 'data:0.518': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_518:0' shape=(None,) dtype=float32>), 'data:0.519': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_519:0' shape=(None,) dtype=float32>), 'data:0.520': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_520:0' shape=(None,) dtype=float32>), 'data:0.521': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_521:0' shape=(None,) dtype=float32>), 'data:0.522': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_522:0' shape=(None,) dtype=float32>), 'data:0.523': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_523:0' shape=(None,) dtype=float32>), 'data:0.524': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_524:0' shape=(None,) dtype=float32>), 'data:0.525': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_525:0' shape=(None,) dtype=float32>), 'data:0.526': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_526:0' shape=(None,) dtype=float32>), 'data:0.527': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_527:0' shape=(None,) dtype=float32>), 'data:0.528': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_528:0' shape=(None,) dtype=float32>), 'data:0.529': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_529:0' shape=(None,) dtype=float32>), 'data:0.530': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_530:0' shape=(None,) dtype=float32>), 'data:0.531': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_531:0' shape=(None,) dtype=float32>), 'data:0.532': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_532:0' shape=(None,) dtype=float32>), 'data:0.533': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_533:0' shape=(None,) dtype=float32>), 'data:0.534': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_534:0' shape=(None,) dtype=float32>), 'data:0.535': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_535:0' shape=(None,) dtype=float32>), 'data:0.536': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_536:0' shape=(None,) dtype=float32>), 'data:0.537': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_537:0' shape=(None,) dtype=float32>), 'data:0.538': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_538:0' shape=(None,) dtype=float32>), 'data:0.539': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_539:0' shape=(None,) dtype=float32>), 'data:0.540': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_540:0' shape=(None,) dtype=float32>), 'data:0.541': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_541:0' shape=(None,) dtype=float32>), 'data:0.542': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_542:0' shape=(None,) dtype=float32>), 'data:0.543': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_543:0' shape=(None,) dtype=float32>), 'data:0.544': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_544:0' shape=(None,) dtype=float32>), 'data:0.545': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_545:0' shape=(None,) dtype=float32>), 'data:0.546': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_546:0' shape=(None,) dtype=float32>), 'data:0.547': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_547:0' shape=(None,) dtype=float32>), 'data:0.548': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_548:0' shape=(None,) dtype=float32>), 'data:0.549': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_549:0' shape=(None,) dtype=float32>), 'data:0.550': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_550:0' shape=(None,) dtype=float32>), 'data:0.551': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_551:0' shape=(None,) dtype=float32>), 'data:0.552': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_552:0' shape=(None,) dtype=float32>), 'data:0.553': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_553:0' shape=(None,) dtype=float32>), 'data:0.554': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_554:0' shape=(None,) dtype=float32>), 'data:0.555': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_555:0' shape=(None,) dtype=float32>), 'data:0.556': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_556:0' shape=(None,) dtype=float32>), 'data:0.557': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_557:0' shape=(None,) dtype=float32>), 'data:0.558': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_558:0' shape=(None,) dtype=float32>), 'data:0.559': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_559:0' shape=(None,) dtype=float32>), 'data:0.560': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_560:0' shape=(None,) dtype=float32>), 'data:0.561': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_561:0' shape=(None,) dtype=float32>), 'data:0.562': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_562:0' shape=(None,) dtype=float32>), 'data:0.563': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_563:0' shape=(None,) dtype=float32>), 'data:0.564': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_564:0' shape=(None,) dtype=float32>), 'data:0.565': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_565:0' shape=(None,) dtype=float32>), 'data:0.566': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_566:0' shape=(None,) dtype=float32>), 'data:0.567': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_567:0' shape=(None,) dtype=float32>), 'data:0.568': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_568:0' shape=(None,) dtype=float32>), 'data:0.569': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_569:0' shape=(None,) dtype=float32>), 'data:0.570': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_570:0' shape=(None,) dtype=float32>), 'data:0.571': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_571:0' shape=(None,) dtype=float32>), 'data:0.572': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_572:0' shape=(None,) dtype=float32>), 'data:0.573': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_573:0' shape=(None,) dtype=float32>), 'data:0.574': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_574:0' shape=(None,) dtype=float32>), 'data:0.575': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_575:0' shape=(None,) dtype=float32>), 'data:0.576': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_576:0' shape=(None,) dtype=float32>), 'data:0.577': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_577:0' shape=(None,) dtype=float32>), 'data:0.578': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_578:0' shape=(None,) dtype=float32>), 'data:0.579': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_579:0' shape=(None,) dtype=float32>), 'data:0.580': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_580:0' shape=(None,) dtype=float32>), 'data:0.581': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_581:0' shape=(None,) dtype=float32>), 'data:0.582': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_582:0' shape=(None,) dtype=float32>), 'data:0.583': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_583:0' shape=(None,) dtype=float32>), 'data:0.584': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_584:0' shape=(None,) dtype=float32>), 'data:0.585': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_585:0' shape=(None,) dtype=float32>), 'data:0.586': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_586:0' shape=(None,) dtype=float32>), 'data:0.587': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_587:0' shape=(None,) dtype=float32>), 'data:0.588': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_588:0' shape=(None,) dtype=float32>), 'data:0.589': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_589:0' shape=(None,) dtype=float32>), 'data:0.590': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_590:0' shape=(None,) dtype=float32>), 'data:0.591': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_591:0' shape=(None,) dtype=float32>), 'data:0.592': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_592:0' shape=(None,) dtype=float32>), 'data:0.593': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_593:0' shape=(None,) dtype=float32>), 'data:0.594': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_594:0' shape=(None,) dtype=float32>), 'data:0.595': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_595:0' shape=(None,) dtype=float32>), 'data:0.596': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_596:0' shape=(None,) dtype=float32>), 'data:0.597': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_597:0' shape=(None,) dtype=float32>), 'data:0.598': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_598:0' shape=(None,) dtype=float32>), 'data:0.599': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_599:0' shape=(None,) dtype=float32>), 'data:0.600': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_600:0' shape=(None,) dtype=float32>), 'data:0.601': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_601:0' shape=(None,) dtype=float32>), 'data:0.602': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_602:0' shape=(None,) dtype=float32>), 'data:0.603': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_603:0' shape=(None,) dtype=float32>), 'data:0.604': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_604:0' shape=(None,) dtype=float32>), 'data:0.605': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_605:0' shape=(None,) dtype=float32>), 'data:0.606': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_606:0' shape=(None,) dtype=float32>), 'data:0.607': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_607:0' shape=(None,) dtype=float32>), 'data:0.608': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_608:0' shape=(None,) dtype=float32>), 'data:0.609': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_609:0' shape=(None,) dtype=float32>), 'data:0.610': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_610:0' shape=(None,) dtype=float32>), 'data:0.611': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_611:0' shape=(None,) dtype=float32>), 'data:0.612': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_612:0' shape=(None,) dtype=float32>), 'data:0.613': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_613:0' shape=(None,) dtype=float32>), 'data:0.614': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_614:0' shape=(None,) dtype=float32>), 'data:0.615': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_615:0' shape=(None,) dtype=float32>), 'data:0.616': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_616:0' shape=(None,) dtype=float32>), 'data:0.617': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_617:0' shape=(None,) dtype=float32>), 'data:0.618': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_618:0' shape=(None,) dtype=float32>), 'data:0.619': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_619:0' shape=(None,) dtype=float32>), 'data:0.620': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_620:0' shape=(None,) dtype=float32>), 'data:0.621': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_621:0' shape=(None,) dtype=float32>), 'data:0.622': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_622:0' shape=(None,) dtype=float32>), 'data:0.623': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_623:0' shape=(None,) dtype=float32>), 'data:0.624': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_624:0' shape=(None,) dtype=float32>), 'data:0.625': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_625:0' shape=(None,) dtype=float32>), 'data:0.626': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_626:0' shape=(None,) dtype=float32>), 'data:0.627': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_627:0' shape=(None,) dtype=float32>), 'data:0.628': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_628:0' shape=(None,) dtype=float32>), 'data:0.629': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_629:0' shape=(None,) dtype=float32>), 'data:0.630': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_630:0' shape=(None,) dtype=float32>), 'data:0.631': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_631:0' shape=(None,) dtype=float32>), 'data:0.632': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_632:0' shape=(None,) dtype=float32>), 'data:0.633': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_633:0' shape=(None,) dtype=float32>), 'data:0.634': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_634:0' shape=(None,) dtype=float32>), 'data:0.635': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_635:0' shape=(None,) dtype=float32>), 'data:0.636': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_636:0' shape=(None,) dtype=float32>), 'data:0.637': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_637:0' shape=(None,) dtype=float32>), 'data:0.638': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_638:0' shape=(None,) dtype=float32>), 'data:0.639': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_639:0' shape=(None,) dtype=float32>), 'data:0.640': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_640:0' shape=(None,) dtype=float32>), 'data:0.641': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_641:0' shape=(None,) dtype=float32>), 'data:0.642': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_642:0' shape=(None,) dtype=float32>), 'data:0.643': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_643:0' shape=(None,) dtype=float32>), 'data:0.644': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_644:0' shape=(None,) dtype=float32>), 'data:0.645': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_645:0' shape=(None,) dtype=float32>), 'data:0.646': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_646:0' shape=(None,) dtype=float32>), 'data:0.647': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_647:0' shape=(None,) dtype=float32>), 'data:0.648': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_648:0' shape=(None,) dtype=float32>), 'data:0.649': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_649:0' shape=(None,) dtype=float32>), 'data:0.650': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_650:0' shape=(None,) dtype=float32>), 'data:0.651': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_651:0' shape=(None,) dtype=float32>), 'data:0.652': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_652:0' shape=(None,) dtype=float32>), 'data:0.653': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_653:0' shape=(None,) dtype=float32>), 'data:0.654': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_654:0' shape=(None,) dtype=float32>), 'data:0.655': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_655:0' shape=(None,) dtype=float32>), 'data:0.656': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_656:0' shape=(None,) dtype=float32>), 'data:0.657': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_657:0' shape=(None,) dtype=float32>), 'data:0.658': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_658:0' shape=(None,) dtype=float32>), 'data:0.659': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_659:0' shape=(None,) dtype=float32>), 'data:0.660': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_660:0' shape=(None,) dtype=float32>), 'data:0.661': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_661:0' shape=(None,) dtype=float32>), 'data:0.662': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_662:0' shape=(None,) dtype=float32>), 'data:0.663': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_663:0' shape=(None,) dtype=float32>), 'data:0.664': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_664:0' shape=(None,) dtype=float32>), 'data:0.665': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_665:0' shape=(None,) dtype=float32>), 'data:0.666': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_666:0' shape=(None,) dtype=float32>), 'data:0.667': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_667:0' shape=(None,) dtype=float32>), 'data:0.668': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_668:0' shape=(None,) dtype=float32>), 'data:0.669': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_669:0' shape=(None,) dtype=float32>), 'data:0.670': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_670:0' shape=(None,) dtype=float32>), 'data:0.671': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_671:0' shape=(None,) dtype=float32>), 'data:0.672': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_672:0' shape=(None,) dtype=float32>), 'data:0.673': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_673:0' shape=(None,) dtype=float32>), 'data:0.674': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_674:0' shape=(None,) dtype=float32>), 'data:0.675': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_675:0' shape=(None,) dtype=float32>), 'data:0.676': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_676:0' shape=(None,) dtype=float32>), 'data:0.677': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_677:0' shape=(None,) dtype=float32>), 'data:0.678': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_678:0' shape=(None,) dtype=float32>), 'data:0.679': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_679:0' shape=(None,) dtype=float32>), 'data:0.680': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_680:0' shape=(None,) dtype=float32>), 'data:0.681': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_681:0' shape=(None,) dtype=float32>), 'data:0.682': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_682:0' shape=(None,) dtype=float32>), 'data:0.683': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_683:0' shape=(None,) dtype=float32>), 'data:0.684': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_684:0' shape=(None,) dtype=float32>), 'data:0.685': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_685:0' shape=(None,) dtype=float32>), 'data:0.686': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_686:0' shape=(None,) dtype=float32>), 'data:0.687': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_687:0' shape=(None,) dtype=float32>), 'data:0.688': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_688:0' shape=(None,) dtype=float32>), 'data:0.689': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_689:0' shape=(None,) dtype=float32>), 'data:0.690': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_690:0' shape=(None,) dtype=float32>), 'data:0.691': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_691:0' shape=(None,) dtype=float32>), 'data:0.692': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_692:0' shape=(None,) dtype=float32>), 'data:0.693': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_693:0' shape=(None,) dtype=float32>), 'data:0.694': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_694:0' shape=(None,) dtype=float32>), 'data:0.695': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_695:0' shape=(None,) dtype=float32>), 'data:0.696': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_696:0' shape=(None,) dtype=float32>), 'data:0.697': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_697:0' shape=(None,) dtype=float32>), 'data:0.698': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_698:0' shape=(None,) dtype=float32>), 'data:0.699': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_699:0' shape=(None,) dtype=float32>), 'data:0.700': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_700:0' shape=(None,) dtype=float32>), 'data:0.701': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_701:0' shape=(None,) dtype=float32>), 'data:0.702': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_702:0' shape=(None,) dtype=float32>), 'data:0.703': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_703:0' shape=(None,) dtype=float32>), 'data:0.704': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_704:0' shape=(None,) dtype=float32>), 'data:0.705': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_705:0' shape=(None,) dtype=float32>), 'data:0.706': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_706:0' shape=(None,) dtype=float32>), 'data:0.707': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_707:0' shape=(None,) dtype=float32>), 'data:0.708': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_708:0' shape=(None,) dtype=float32>), 'data:0.709': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_709:0' shape=(None,) dtype=float32>), 'data:0.710': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_710:0' shape=(None,) dtype=float32>), 'data:0.711': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_711:0' shape=(None,) dtype=float32>), 'data:0.712': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_712:0' shape=(None,) dtype=float32>), 'data:0.713': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_713:0' shape=(None,) dtype=float32>), 'data:0.714': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_714:0' shape=(None,) dtype=float32>), 'data:0.715': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_715:0' shape=(None,) dtype=float32>), 'data:0.716': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_716:0' shape=(None,) dtype=float32>), 'data:0.717': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_717:0' shape=(None,) dtype=float32>), 'data:0.718': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_718:0' shape=(None,) dtype=float32>), 'data:0.719': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_719:0' shape=(None,) dtype=float32>), 'data:0.720': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_720:0' shape=(None,) dtype=float32>), 'data:0.721': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_721:0' shape=(None,) dtype=float32>), 'data:0.722': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_722:0' shape=(None,) dtype=float32>), 'data:0.723': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_723:0' shape=(None,) dtype=float32>), 'data:0.724': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_724:0' shape=(None,) dtype=float32>), 'data:0.725': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_725:0' shape=(None,) dtype=float32>), 'data:0.726': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_726:0' shape=(None,) dtype=float32>), 'data:0.727': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_727:0' shape=(None,) dtype=float32>), 'data:0.728': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_728:0' shape=(None,) dtype=float32>), 'data:0.729': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_729:0' shape=(None,) dtype=float32>), 'data:0.730': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_730:0' shape=(None,) dtype=float32>), 'data:0.731': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_731:0' shape=(None,) dtype=float32>), 'data:0.732': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_732:0' shape=(None,) dtype=float32>), 'data:0.733': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_733:0' shape=(None,) dtype=float32>), 'data:0.734': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_734:0' shape=(None,) dtype=float32>), 'data:0.735': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_735:0' shape=(None,) dtype=float32>), 'data:0.736': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_736:0' shape=(None,) dtype=float32>), 'data:0.737': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_737:0' shape=(None,) dtype=float32>), 'data:0.738': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_738:0' shape=(None,) dtype=float32>), 'data:0.739': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_739:0' shape=(None,) dtype=float32>), 'data:0.740': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_740:0' shape=(None,) dtype=float32>), 'data:0.741': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_741:0' shape=(None,) dtype=float32>), 'data:0.742': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_742:0' shape=(None,) dtype=float32>), 'data:0.743': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_743:0' shape=(None,) dtype=float32>), 'data:0.744': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_744:0' shape=(None,) dtype=float32>), 'data:0.745': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_745:0' shape=(None,) dtype=float32>), 'data:0.746': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_746:0' shape=(None,) dtype=float32>), 'data:0.747': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_747:0' shape=(None,) dtype=float32>), 'data:0.748': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_748:0' shape=(None,) dtype=float32>), 'data:0.749': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_749:0' shape=(None,) dtype=float32>), 'data:0.750': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_750:0' shape=(None,) dtype=float32>), 'data:0.751': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_751:0' shape=(None,) dtype=float32>), 'data:0.752': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_752:0' shape=(None,) dtype=float32>), 'data:0.753': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_753:0' shape=(None,) dtype=float32>), 'data:0.754': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_754:0' shape=(None,) dtype=float32>), 'data:0.755': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_755:0' shape=(None,) dtype=float32>), 'data:0.756': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_756:0' shape=(None,) dtype=float32>), 'data:0.757': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_757:0' shape=(None,) dtype=float32>), 'data:0.758': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_758:0' shape=(None,) dtype=float32>), 'data:0.759': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_759:0' shape=(None,) dtype=float32>), 'data:0.760': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_760:0' shape=(None,) dtype=float32>), 'data:0.761': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_761:0' shape=(None,) dtype=float32>), 'data:0.762': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_762:0' shape=(None,) dtype=float32>), 'data:0.763': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_763:0' shape=(None,) dtype=float32>), 'data:0.764': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_764:0' shape=(None,) dtype=float32>), 'data:0.765': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_765:0' shape=(None,) dtype=float32>), 'data:0.766': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_766:0' shape=(None,) dtype=float32>), 'data:0.767': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_767:0' shape=(None,) dtype=float32>)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset read in 0:00:05.957837. Found 22996 examples.\n",
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 23-07-29 19:58:09.5408 CEST kernel.cc:773] Start Yggdrasil model training\n",
      "[INFO 23-07-29 19:58:09.5408 CEST kernel.cc:774] Collect training examples\n",
      "[INFO 23-07-29 19:58:09.5409 CEST kernel.cc:787] Dataspec guide:\n",
      "column_guides {\n",
      "  column_name_pattern: \"^__LABEL$\"\n",
      "  type: CATEGORICAL\n",
      "  categorial {\n",
      "    min_vocab_frequency: 0\n",
      "    max_vocab_count: -1\n",
      "  }\n",
      "}\n",
      "default_column_guide {\n",
      "  categorial {\n",
      "    max_vocab_count: 2000\n",
      "  }\n",
      "  discretized_numerical {\n",
      "    maximum_num_bins: 255\n",
      "  }\n",
      "}\n",
      "ignore_columns_without_guides: false\n",
      "detect_numerical_as_discretized_numerical: false\n",
      "\n",
      "[INFO 23-07-29 19:58:09.5436 CEST kernel.cc:393] Number of batches: 767\n",
      "[INFO 23-07-29 19:58:09.5436 CEST kernel.cc:394] Number of examples: 22996\n",
      "[INFO 23-07-29 19:58:09.7010 CEST kernel.cc:794] Training dataset:\n",
      "Number of records: 22996\n",
      "Number of columns: 769\n",
      "\n",
      "Number of columns by type:\n",
      "\tNUMERICAL: 768 (99.87%)\n",
      "\tCATEGORICAL: 1 (0.130039%)\n",
      "\n",
      "Columns:\n",
      "\n",
      "NUMERICAL: 768 (99.87%)\n",
      "\t1: \"data:0.0\" NUMERICAL mean:0.0180247 min:-0.695612 max:0.750331 sd:0.172882\n",
      "\t2: \"data:0.1\" NUMERICAL mean:0.0876243 min:-0.409084 max:0.6224 sd:0.134575\n",
      "\t3: \"data:0.10\" NUMERICAL mean:0.0112266 min:-0.754706 max:0.932425 sd:0.190522\n",
      "\t4: \"data:0.100\" NUMERICAL mean:0.128553 min:-0.223497 max:0.431056 sd:0.0924593\n",
      "\t5: \"data:0.101\" NUMERICAL mean:0.0754677 min:-0.508431 max:0.648341 sd:0.145502\n",
      "\t6: \"data:0.102\" NUMERICAL mean:0.0344458 min:-0.972416 max:0.822584 sd:0.197645\n",
      "\t7: \"data:0.103\" NUMERICAL mean:0.243659 min:-0.52905 max:0.934539 sd:0.184219\n",
      "\t8: \"data:0.104\" NUMERICAL mean:-0.0171654 min:-0.901228 max:0.77107 sd:0.207188\n",
      "\t9: \"data:0.105\" NUMERICAL mean:0.115743 min:-0.711449 max:0.842877 sd:0.197086\n",
      "\t10: \"data:0.106\" NUMERICAL mean:0.0842269 min:-0.607062 max:0.761431 sd:0.175426\n",
      "\t11: \"data:0.107\" NUMERICAL mean:-0.254523 min:-1.16767 max:0.777693 sd:0.235473\n",
      "\t12: \"data:0.108\" NUMERICAL mean:0.0128764 min:-0.324237 max:0.47013 sd:0.0904345\n",
      "\t13: \"data:0.109\" NUMERICAL mean:0.10182 min:-0.623418 max:0.726438 sd:0.168093\n",
      "\t14: \"data:0.11\" NUMERICAL mean:-0.247918 min:-0.929621 max:0.790059 sd:0.204154\n",
      "\t15: \"data:0.110\" NUMERICAL mean:0.0533946 min:-0.753843 max:0.878085 sd:0.199958\n",
      "\t16: \"data:0.111\" NUMERICAL mean:0.101416 min:-0.61731 max:0.776678 sd:0.173914\n",
      "\t17: \"data:0.112\" NUMERICAL mean:-0.134906 min:-0.966758 max:0.738267 sd:0.193992\n",
      "\t18: \"data:0.113\" NUMERICAL mean:0.0215897 min:-0.720931 max:0.963936 sd:0.232062\n",
      "\t19: \"data:0.114\" NUMERICAL mean:0.192098 min:-0.740226 max:1.24412 sd:0.245745\n",
      "\t20: \"data:0.115\" NUMERICAL mean:0.0935368 min:-0.7206 max:0.901221 sd:0.207359\n",
      "\t21: \"data:0.116\" NUMERICAL mean:0.00696854 min:-0.308371 max:0.236214 sd:0.0686177\n",
      "\t22: \"data:0.117\" NUMERICAL mean:0.237319 min:-0.710895 max:1.05662 sd:0.239773\n",
      "\t23: \"data:0.118\" NUMERICAL mean:-0.0914624 min:-0.962901 max:0.966728 sd:0.257175\n",
      "\t24: \"data:0.119\" NUMERICAL mean:-0.210936 min:-0.492208 max:0.0887214 sd:0.0730494\n",
      "\t25: \"data:0.12\" NUMERICAL mean:0.162512 min:-0.680482 max:0.799761 sd:0.178974\n",
      "\t26: \"data:0.120\" NUMERICAL mean:-0.0871265 min:-1.04203 max:0.646274 sd:0.198087\n",
      "\t27: \"data:0.121\" NUMERICAL mean:0.124514 min:-0.844812 max:0.978246 sd:0.197789\n",
      "\t28: \"data:0.122\" NUMERICAL mean:-0.183434 min:-1.18202 max:0.787146 sd:0.233404\n",
      "\t29: \"data:0.123\" NUMERICAL mean:0.092761 min:-0.868877 max:0.923161 sd:0.189018\n",
      "\t30: \"data:0.124\" NUMERICAL mean:0.0161442 min:-1.07766 max:1.07884 sd:0.244212\n",
      "\t31: \"data:0.125\" NUMERICAL mean:0.108493 min:-0.973425 max:0.970771 sd:0.236424\n",
      "\t32: \"data:0.126\" NUMERICAL mean:0.00805516 min:-1.0663 max:0.83218 sd:0.230867\n",
      "\t33: \"data:0.127\" NUMERICAL mean:0.0478214 min:-0.763272 max:0.715379 sd:0.185984\n",
      "\t34: \"data:0.128\" NUMERICAL mean:0.115036 min:-0.711655 max:0.976734 sd:0.24039\n",
      "\t35: \"data:0.129\" NUMERICAL mean:0.158561 min:-0.883336 max:1.1365 sd:0.253944\n",
      "\t36: \"data:0.13\" NUMERICAL mean:-0.014556 min:-1.03239 max:0.936769 sd:0.201156\n",
      "\t37: \"data:0.130\" NUMERICAL mean:1.34129 min:-0.794521 max:4.10514 sd:0.542917\n",
      "\t38: \"data:0.131\" NUMERICAL mean:0.160896 min:-0.297831 max:0.823734 sd:0.142547\n",
      "\t39: \"data:0.132\" NUMERICAL mean:-0.096517 min:-0.902348 max:0.635776 sd:0.168904\n",
      "\t40: \"data:0.133\" NUMERICAL mean:0.0449275 min:-0.746487 max:0.773027 sd:0.163269\n",
      "\t41: \"data:0.134\" NUMERICAL mean:0.116565 min:-0.816774 max:1.11356 sd:0.23367\n",
      "\t42: \"data:0.135\" NUMERICAL mean:-0.0702231 min:-0.631853 max:0.502334 sd:0.132128\n",
      "\t43: \"data:0.136\" NUMERICAL mean:-0.216295 min:-1.06208 max:0.872905 sd:0.252904\n",
      "\t44: \"data:0.137\" NUMERICAL mean:-0.022904 min:-1.00051 max:0.894188 sd:0.218478\n",
      "\t45: \"data:0.138\" NUMERICAL mean:0.0382873 min:-0.797732 max:1.00384 sd:0.22221\n",
      "\t46: \"data:0.139\" NUMERICAL mean:0.097567 min:-0.765936 max:0.914981 sd:0.208871\n",
      "\t47: \"data:0.14\" NUMERICAL mean:-0.0357176 min:-0.740833 max:0.757986 sd:0.21722\n",
      "\t48: \"data:0.140\" NUMERICAL mean:0.0428677 min:-0.799134 max:0.89579 sd:0.19303\n",
      "\t49: \"data:0.141\" NUMERICAL mean:-0.147774 min:-0.766638 max:0.692776 sd:0.186783\n",
      "\t50: \"data:0.142\" NUMERICAL mean:-0.0347654 min:-0.784324 max:0.804002 sd:0.185059\n",
      "\t51: \"data:0.143\" NUMERICAL mean:-0.0796928 min:-0.734857 max:0.551474 sd:0.157385\n",
      "\t52: \"data:0.144\" NUMERICAL mean:-0.0523619 min:-0.865627 max:0.73404 sd:0.191187\n",
      "\t53: \"data:0.145\" NUMERICAL mean:0.00537078 min:-0.756481 max:1.097 sd:0.189063\n",
      "\t54: \"data:0.146\" NUMERICAL mean:0.245407 min:-0.55427 max:1.02105 sd:0.198326\n",
      "\t55: \"data:0.147\" NUMERICAL mean:0.0871061 min:-0.776574 max:0.913273 sd:0.197941\n",
      "\t56: \"data:0.148\" NUMERICAL mean:0.0814979 min:-0.844108 max:1.08543 sd:0.245929\n",
      "\t57: \"data:0.149\" NUMERICAL mean:0.0514259 min:-0.878253 max:0.85628 sd:0.252524\n",
      "\t58: \"data:0.15\" NUMERICAL mean:0.101162 min:-0.73231 max:0.969112 sd:0.213926\n",
      "\t59: \"data:0.150\" NUMERICAL mean:-0.151337 min:-0.389422 max:0.103388 sd:0.0595808\n",
      "\t60: \"data:0.151\" NUMERICAL mean:-0.165918 min:-0.938392 max:0.956587 sd:0.228553\n",
      "\t61: \"data:0.152\" NUMERICAL mean:-0.0814118 min:-0.97763 max:0.865143 sd:0.23654\n",
      "\t62: \"data:0.153\" NUMERICAL mean:-0.111033 min:-0.691869 max:0.58754 sd:0.149006\n",
      "\t63: \"data:0.154\" NUMERICAL mean:-0.0470061 min:-0.815323 max:0.783232 sd:0.200554\n",
      "\t64: \"data:0.155\" NUMERICAL mean:-0.0509387 min:-0.877014 max:0.858351 sd:0.225361\n",
      "\t65: \"data:0.156\" NUMERICAL mean:0.0316184 min:-0.685028 max:0.663422 sd:0.180339\n",
      "\t66: \"data:0.157\" NUMERICAL mean:-0.0898359 min:-0.612921 max:0.57051 sd:0.165343\n",
      "\t67: \"data:0.158\" NUMERICAL mean:0.183085 min:-0.882008 max:1.01523 sd:0.25864\n",
      "\t68: \"data:0.159\" NUMERICAL mean:0.21525 min:-0.714033 max:0.96239 sd:0.196125\n",
      "\t69: \"data:0.16\" NUMERICAL mean:-0.300486 min:-1.30174 max:0.866114 sd:0.322684\n",
      "\t70: \"data:0.160\" NUMERICAL mean:-0.000708412 min:-0.678764 max:0.82721 sd:0.184761\n",
      "\t71: \"data:0.161\" NUMERICAL mean:-0.0718238 min:-0.841302 max:0.863513 sd:0.20397\n",
      "\t72: \"data:0.162\" NUMERICAL mean:-0.254378 min:-0.918314 max:0.711545 sd:0.204036\n",
      "\t73: \"data:0.163\" NUMERICAL mean:0.151473 min:-0.174762 max:0.473438 sd:0.0854142\n",
      "\t74: \"data:0.164\" NUMERICAL mean:0.187507 min:-0.731229 max:0.945922 sd:0.264833\n",
      "\t75: \"data:0.165\" NUMERICAL mean:0.0900136 min:-0.680206 max:0.963446 sd:0.201899\n",
      "\t76: \"data:0.166\" NUMERICAL mean:0.0345858 min:-0.804118 max:0.787197 sd:0.186116\n",
      "\t77: \"data:0.167\" NUMERICAL mean:0.146417 min:-0.624058 max:0.865243 sd:0.188259\n",
      "\t78: \"data:0.168\" NUMERICAL mean:0.127272 min:-0.674467 max:0.896516 sd:0.220885\n",
      "\t79: \"data:0.169\" NUMERICAL mean:0.0294204 min:-0.92634 max:0.995346 sd:0.267999\n",
      "\t80: \"data:0.17\" NUMERICAL mean:-0.155867 min:-0.660493 max:0.591645 sd:0.146168\n",
      "\t81: \"data:0.170\" NUMERICAL mean:-0.0101211 min:-0.980758 max:0.923984 sd:0.241422\n",
      "\t82: \"data:0.171\" NUMERICAL mean:-0.0364939 min:-0.802797 max:0.833631 sd:0.204174\n",
      "\t83: \"data:0.172\" NUMERICAL mean:0.328403 min:-0.702346 max:0.987169 sd:0.220099\n",
      "\t84: \"data:0.173\" NUMERICAL mean:0.0658731 min:-0.642002 max:0.814781 sd:0.205018\n",
      "\t85: \"data:0.174\" NUMERICAL mean:-0.0453927 min:-0.850344 max:0.761188 sd:0.18879\n",
      "\t86: \"data:0.175\" NUMERICAL mean:0.0259835 min:-0.3434 max:0.448619 sd:0.0979252\n",
      "\t87: \"data:0.176\" NUMERICAL mean:-0.157971 min:-0.618768 max:0.173366 sd:0.0906388\n",
      "\t88: \"data:0.177\" NUMERICAL mean:-0.0359908 min:-1.05548 max:0.858289 sd:0.225052\n",
      "\t89: \"data:0.178\" NUMERICAL mean:0.0581143 min:-1.05953 max:0.912039 sd:0.207245\n",
      "\t90: \"data:0.179\" NUMERICAL mean:-0.104688 min:-0.939463 max:0.717421 sd:0.203313\n",
      "\t91: \"data:0.18\" NUMERICAL mean:-0.109207 min:-1.02843 max:0.874045 sd:0.220385\n",
      "\t92: \"data:0.180\" NUMERICAL mean:-0.0446491 min:-0.764443 max:0.881527 sd:0.200866\n",
      "\t93: \"data:0.181\" NUMERICAL mean:0.154202 min:-0.67517 max:1.04396 sd:0.229992\n",
      "\t94: \"data:0.182\" NUMERICAL mean:-0.00396949 min:-0.777739 max:1.17446 sd:0.227126\n",
      "\t95: \"data:0.183\" NUMERICAL mean:-0.107421 min:-1.34673 max:0.899133 sd:0.309494\n",
      "\t96: \"data:0.184\" NUMERICAL mean:-0.0523389 min:-0.88746 max:0.774192 sd:0.218742\n",
      "\t97: \"data:0.185\" NUMERICAL mean:0.104581 min:-0.789748 max:0.901821 sd:0.241328\n",
      "\t98: \"data:0.186\" NUMERICAL mean:0.106626 min:-0.708657 max:0.806174 sd:0.187485\n",
      "\t99: \"data:0.187\" NUMERICAL mean:0.0697578 min:-0.669476 max:0.858018 sd:0.179084\n",
      "\t100: \"data:0.188\" NUMERICAL mean:0.0923922 min:-0.0878678 max:0.298754 sd:0.0494159\n",
      "\t101: \"data:0.189\" NUMERICAL mean:0.0899846 min:-0.702462 max:0.98157 sd:0.246642\n",
      "\t102: \"data:0.19\" NUMERICAL mean:0.086541 min:-0.707044 max:0.970019 sd:0.202692\n",
      "\t103: \"data:0.190\" NUMERICAL mean:0.0180886 min:-1.019 max:0.969814 sd:0.270009\n",
      "\t104: \"data:0.191\" NUMERICAL mean:-0.0239221 min:-0.596983 max:0.565569 sd:0.150636\n",
      "\t105: \"data:0.192\" NUMERICAL mean:-0.105684 min:-1.2271 max:0.846373 sd:0.262847\n",
      "\t106: \"data:0.193\" NUMERICAL mean:-0.146372 min:-0.780493 max:0.699439 sd:0.163615\n",
      "\t107: \"data:0.194\" NUMERICAL mean:-0.0591962 min:-0.875155 max:0.582123 sd:0.182437\n",
      "\t108: \"data:0.195\" NUMERICAL mean:-0.00956276 min:-0.849669 max:0.858681 sd:0.227254\n",
      "\t109: \"data:0.196\" NUMERICAL mean:0.21205 min:-0.787218 max:0.732642 sd:0.176441\n",
      "\t110: \"data:0.197\" NUMERICAL mean:0.0268109 min:-0.411981 max:0.49753 sd:0.110633\n",
      "\t111: \"data:0.198\" NUMERICAL mean:0.113341 min:-0.757157 max:0.858692 sd:0.201223\n",
      "\t112: \"data:0.199\" NUMERICAL mean:0.105461 min:-0.774902 max:0.875383 sd:0.166949\n",
      "\t113: \"data:0.2\" NUMERICAL mean:0.270333 min:-0.747017 max:1.17586 sd:0.239752\n",
      "\t114: \"data:0.20\" NUMERICAL mean:0.330814 min:-0.634404 max:1.20288 sd:0.267372\n",
      "\t115: \"data:0.200\" NUMERICAL mean:0.0576235 min:-0.798882 max:0.867243 sd:0.21262\n",
      "\t116: \"data:0.201\" NUMERICAL mean:-0.050146 min:-0.911126 max:0.837841 sd:0.21102\n",
      "\t117: \"data:0.202\" NUMERICAL mean:-0.0374683 min:-0.276363 max:0.167126 sd:0.0579727\n",
      "\t118: \"data:0.203\" NUMERICAL mean:0.00200537 min:-0.958699 max:0.978871 sd:0.253788\n",
      "\t119: \"data:0.204\" NUMERICAL mean:0.268992 min:-0.866631 max:1.11099 sd:0.233653\n",
      "\t120: \"data:0.205\" NUMERICAL mean:0.159168 min:-0.654629 max:0.95988 sd:0.224587\n",
      "\t121: \"data:0.206\" NUMERICAL mean:-0.140872 min:-0.960383 max:0.701421 sd:0.218269\n",
      "\t122: \"data:0.207\" NUMERICAL mean:0.0578315 min:-0.646353 max:0.81761 sd:0.184081\n",
      "\t123: \"data:0.208\" NUMERICAL mean:-0.0141888 min:-0.893331 max:0.900678 sd:0.205879\n",
      "\t124: \"data:0.209\" NUMERICAL mean:-0.19838 min:-0.966727 max:0.929232 sd:0.229738\n",
      "\t125: \"data:0.21\" NUMERICAL mean:-0.0237281 min:-0.651557 max:0.673171 sd:0.185823\n",
      "\t126: \"data:0.210\" NUMERICAL mean:-0.0125685 min:-0.761718 max:0.772896 sd:0.186035\n",
      "\t127: \"data:0.211\" NUMERICAL mean:-0.0551015 min:-0.931747 max:0.885616 sd:0.201574\n",
      "\t128: \"data:0.212\" NUMERICAL mean:-0.0782212 min:-0.930587 max:0.793908 sd:0.207851\n",
      "\t129: \"data:0.213\" NUMERICAL mean:0.063769 min:-0.183129 max:0.263356 sd:0.0567424\n",
      "\t130: \"data:0.214\" NUMERICAL mean:-0.0228946 min:-0.749456 max:0.767657 sd:0.19607\n",
      "\t131: \"data:0.215\" NUMERICAL mean:-0.161854 min:-0.431088 max:0.0949471 sd:0.0559013\n",
      "\t132: \"data:0.216\" NUMERICAL mean:0.0936161 min:-0.863186 max:1.03295 sd:0.23925\n",
      "\t133: \"data:0.217\" NUMERICAL mean:-0.284934 min:-1.37095 max:1.04229 sd:0.292416\n",
      "\t134: \"data:0.218\" NUMERICAL mean:-0.233375 min:-1.08901 max:0.8017 sd:0.202715\n",
      "\t135: \"data:0.219\" NUMERICAL mean:-0.0662356 min:-0.936722 max:0.738371 sd:0.217723\n",
      "\t136: \"data:0.22\" NUMERICAL mean:0.0003868 min:-0.878426 max:0.651043 sd:0.20118\n",
      "\t137: \"data:0.220\" NUMERICAL mean:0.0533733 min:-0.845013 max:0.727517 sd:0.18531\n",
      "\t138: \"data:0.221\" NUMERICAL mean:0.111143 min:-0.689064 max:0.724562 sd:0.184952\n",
      "\t139: \"data:0.222\" NUMERICAL mean:0.0369733 min:-1.00276 max:1.09773 sd:0.202425\n",
      "\t140: \"data:0.223\" NUMERICAL mean:0.200205 min:-0.97095 max:1.23067 sd:0.264104\n",
      "\t141: \"data:0.224\" NUMERICAL mean:0.011253 min:-0.649515 max:0.568998 sd:0.142584\n",
      "\t142: \"data:0.225\" NUMERICAL mean:-0.0788952 min:-0.807216 max:0.571752 sd:0.151652\n",
      "\t143: \"data:0.226\" NUMERICAL mean:0.199123 min:-0.853248 max:1.05024 sd:0.214906\n",
      "\t144: \"data:0.227\" NUMERICAL mean:0.103839 min:-0.930831 max:1.00504 sd:0.233515\n",
      "\t145: \"data:0.228\" NUMERICAL mean:0.215922 min:-0.814652 max:0.991507 sd:0.249334\n",
      "\t146: \"data:0.229\" NUMERICAL mean:0.142236 min:-0.68828 max:0.94095 sd:0.217139\n",
      "\t147: \"data:0.23\" NUMERICAL mean:0.17895 min:-0.561787 max:0.706344 sd:0.173227\n",
      "\t148: \"data:0.230\" NUMERICAL mean:0.123057 min:-0.78181 max:1.04382 sd:0.206506\n",
      "\t149: \"data:0.231\" NUMERICAL mean:0.167929 min:-1.12588 max:1.22783 sd:0.271444\n",
      "\t150: \"data:0.232\" NUMERICAL mean:0.0705831 min:-0.727236 max:0.959416 sd:0.239119\n",
      "\t151: \"data:0.233\" NUMERICAL mean:-0.0951246 min:-0.894519 max:0.830839 sd:0.208349\n",
      "\t152: \"data:0.234\" NUMERICAL mean:-0.0181259 min:-0.688089 max:0.736998 sd:0.171241\n",
      "\t153: \"data:0.235\" NUMERICAL mean:0.273907 min:-1.3351 max:2.2189 sd:0.359771\n",
      "\t154: \"data:0.236\" NUMERICAL mean:0.22236 min:-0.766128 max:0.991965 sd:0.233545\n",
      "\t155: \"data:0.237\" NUMERICAL mean:0.133688 min:-0.180124 max:0.454237 sd:0.0725882\n",
      "\t156: \"data:0.238\" NUMERICAL mean:-0.323381 min:-1.38686 max:0.933422 sd:0.270561\n",
      "\t157: \"data:0.239\" NUMERICAL mean:0.0460595 min:-0.827917 max:0.991782 sd:0.246898\n",
      "\t158: \"data:0.24\" NUMERICAL mean:0.102833 min:-0.600559 max:0.80196 sd:0.185681\n",
      "\t159: \"data:0.240\" NUMERICAL mean:0.11845 min:-0.161972 max:0.425581 sd:0.0778737\n",
      "\t160: \"data:0.241\" NUMERICAL mean:0.144368 min:-0.139715 max:0.472816 sd:0.075709\n",
      "\t161: \"data:0.242\" NUMERICAL mean:0.119244 min:-0.7934 max:0.910102 sd:0.239484\n",
      "\t162: \"data:0.243\" NUMERICAL mean:0.171706 min:-0.802934 max:0.864681 sd:0.199382\n",
      "\t163: \"data:0.244\" NUMERICAL mean:-0.0881206 min:-0.927295 max:0.599665 sd:0.174809\n",
      "\t164: \"data:0.245\" NUMERICAL mean:-0.16171 min:-0.923959 max:1.02054 sd:0.220283\n",
      "\t165: \"data:0.246\" NUMERICAL mean:-0.14328 min:-0.968582 max:0.944427 sd:0.252303\n",
      "\t166: \"data:0.247\" NUMERICAL mean:0.063787 min:-0.881456 max:0.858893 sd:0.214616\n",
      "\t167: \"data:0.248\" NUMERICAL mean:0.0631471 min:-0.673071 max:0.702803 sd:0.175833\n",
      "\t168: \"data:0.249\" NUMERICAL mean:0.0978427 min:-0.151781 max:0.32861 sd:0.0596634\n",
      "\t169: \"data:0.25\" NUMERICAL mean:-0.211617 min:-1.08604 max:0.732417 sd:0.242711\n",
      "\t170: \"data:0.250\" NUMERICAL mean:-0.134921 min:-1.12109 max:0.643487 sd:0.203183\n",
      "\t171: \"data:0.251\" NUMERICAL mean:-0.0416623 min:-0.809166 max:0.780109 sd:0.190087\n",
      "\t172: \"data:0.252\" NUMERICAL mean:-0.140559 min:-1.00098 max:0.634943 sd:0.196359\n",
      "\t173: \"data:0.253\" NUMERICAL mean:-0.0321428 min:-0.783314 max:1.01484 sd:0.214211\n",
      "\t174: \"data:0.254\" NUMERICAL mean:-0.0872222 min:-0.736835 max:0.584359 sd:0.167708\n",
      "\t175: \"data:0.255\" NUMERICAL mean:0.0387256 min:-0.767564 max:0.803868 sd:0.204802\n",
      "\t176: \"data:0.256\" NUMERICAL mean:-0.33273 min:-1.28379 max:0.726925 sd:0.247131\n",
      "\t177: \"data:0.257\" NUMERICAL mean:0.0933652 min:-0.528335 max:0.634771 sd:0.126817\n",
      "\t178: \"data:0.258\" NUMERICAL mean:0.0628581 min:-0.70167 max:0.898498 sd:0.199375\n",
      "\t179: \"data:0.259\" NUMERICAL mean:0.139908 min:-0.772311 max:1.02695 sd:0.231156\n",
      "\t180: \"data:0.26\" NUMERICAL mean:-0.0342263 min:-0.854821 max:0.851737 sd:0.207208\n",
      "\t181: \"data:0.260\" NUMERICAL mean:-0.129538 min:-0.815569 max:0.629703 sd:0.16789\n",
      "\t182: \"data:0.261\" NUMERICAL mean:-0.115429 min:-0.771899 max:0.466384 sd:0.159458\n",
      "\t183: \"data:0.262\" NUMERICAL mean:-0.0396956 min:-0.725842 max:0.620376 sd:0.182672\n",
      "\t184: \"data:0.263\" NUMERICAL mean:0.0398478 min:-0.934769 max:0.918203 sd:0.251062\n",
      "\t185: \"data:0.264\" NUMERICAL mean:0.0505661 min:-0.752593 max:0.713583 sd:0.1698\n",
      "\t186: \"data:0.265\" NUMERICAL mean:0.113798 min:-0.852773 max:0.958759 sd:0.218828\n",
      "\t187: \"data:0.266\" NUMERICAL mean:-0.0284709 min:-0.250932 max:0.22396 sd:0.0551638\n",
      "\t188: \"data:0.267\" NUMERICAL mean:-0.129582 min:-0.784287 max:0.633031 sd:0.150674\n",
      "\t189: \"data:0.268\" NUMERICAL mean:0.0167892 min:-0.329669 max:0.291006 sd:0.0651678\n",
      "\t190: \"data:0.269\" NUMERICAL mean:0.0719602 min:-0.525379 max:0.690977 sd:0.184565\n",
      "\t191: \"data:0.27\" NUMERICAL mean:-0.00885858 min:-0.661394 max:0.72007 sd:0.186232\n",
      "\t192: \"data:0.270\" NUMERICAL mean:0.0251664 min:-0.80501 max:0.789468 sd:0.188377\n",
      "\t193: \"data:0.271\" NUMERICAL mean:-0.00807504 min:-0.514539 max:0.541955 sd:0.132122\n",
      "\t194: \"data:0.272\" NUMERICAL mean:0.132135 min:-0.913851 max:1.0014 sd:0.22871\n",
      "\t195: \"data:0.273\" NUMERICAL mean:-0.00727243 min:-0.554746 max:0.595857 sd:0.136479\n",
      "\t196: \"data:0.274\" NUMERICAL mean:0.169827 min:-0.711194 max:0.818292 sd:0.191113\n",
      "\t197: \"data:0.275\" NUMERICAL mean:0.16178 min:-0.898641 max:0.953615 sd:0.220808\n",
      "\t198: \"data:0.276\" NUMERICAL mean:0.104833 min:-0.853842 max:0.931999 sd:0.223066\n",
      "\t199: \"data:0.277\" NUMERICAL mean:0.0322414 min:-0.94071 max:0.827249 sd:0.213449\n",
      "\t200: \"data:0.278\" NUMERICAL mean:0.0403611 min:-0.703624 max:0.670959 sd:0.195228\n",
      "\t201: \"data:0.279\" NUMERICAL mean:-0.00186772 min:-0.917962 max:1.1853 sd:0.265433\n",
      "\t202: \"data:0.28\" NUMERICAL mean:0.0679966 min:-0.873204 max:1.0884 sd:0.246245\n",
      "\t203: \"data:0.280\" NUMERICAL mean:0.149059 min:-0.823556 max:1.06187 sd:0.261594\n",
      "\t204: \"data:0.281\" NUMERICAL mean:0.00127499 min:-0.815098 max:0.779174 sd:0.231778\n",
      "\t205: \"data:0.282\" NUMERICAL mean:0.222159 min:-0.316866 max:0.751922 sd:0.130907\n",
      "\t206: \"data:0.283\" NUMERICAL mean:0.00356531 min:-0.751239 max:0.850822 sd:0.213937\n",
      "\t207: \"data:0.284\" NUMERICAL mean:0.0699175 min:-0.902658 max:0.906014 sd:0.253197\n",
      "\t208: \"data:0.285\" NUMERICAL mean:-0.123897 min:-0.972848 max:1.02632 sd:0.197782\n",
      "\t209: \"data:0.286\" NUMERICAL mean:-0.00247264 min:-0.84579 max:0.798602 sd:0.205926\n",
      "\t210: \"data:0.287\" NUMERICAL mean:0.0555332 min:-0.813981 max:0.8716 sd:0.204077\n",
      "\t211: \"data:0.288\" NUMERICAL mean:-0.0558234 min:-0.679534 max:0.514015 sd:0.136286\n",
      "\t212: \"data:0.289\" NUMERICAL mean:-0.147099 min:-0.970846 max:0.72498 sd:0.243738\n",
      "\t213: \"data:0.29\" NUMERICAL mean:0.0177696 min:-0.557582 max:0.714123 sd:0.158643\n",
      "\t214: \"data:0.290\" NUMERICAL mean:-0.115423 min:-0.745589 max:0.550004 sd:0.192845\n",
      "\t215: \"data:0.291\" NUMERICAL mean:0.247302 min:0.00236851 max:0.440582 sd:0.0553764\n",
      "\t216: \"data:0.292\" NUMERICAL mean:-0.248089 min:-1.07562 max:0.664383 sd:0.200317\n",
      "\t217: \"data:0.293\" NUMERICAL mean:-0.0850081 min:-0.91297 max:0.769563 sd:0.215302\n",
      "\t218: \"data:0.294\" NUMERICAL mean:-0.158832 min:-1.22108 max:0.885997 sd:0.262747\n",
      "\t219: \"data:0.295\" NUMERICAL mean:0.0500528 min:-0.363373 max:0.470714 sd:0.0928707\n",
      "\t220: \"data:0.296\" NUMERICAL mean:-0.0479945 min:-0.557859 max:0.596234 sd:0.130872\n",
      "\t221: \"data:0.297\" NUMERICAL mean:-0.0344671 min:-0.959604 max:0.80908 sd:0.21207\n",
      "\t222: \"data:0.298\" NUMERICAL mean:0.0925107 min:-0.661657 max:0.906635 sd:0.180589\n",
      "\t223: \"data:0.299\" NUMERICAL mean:-0.0397 min:-0.533183 max:0.509357 sd:0.131741\n",
      "\t224: \"data:0.3\" NUMERICAL mean:-0.155695 min:-0.987472 max:0.707371 sd:0.238285\n",
      "\t225: \"data:0.30\" NUMERICAL mean:0.0147502 min:-0.795099 max:0.803909 sd:0.18695\n",
      "\t226: \"data:0.300\" NUMERICAL mean:0.0540586 min:-0.893812 max:1.04933 sd:0.219174\n",
      "\t227: \"data:0.301\" NUMERICAL mean:-0.152916 min:-0.691879 max:0.605477 sd:0.155728\n",
      "\t228: \"data:0.302\" NUMERICAL mean:0.0489831 min:-0.287509 max:0.325833 sd:0.070731\n",
      "\t229: \"data:0.303\" NUMERICAL mean:0.0515596 min:-0.646656 max:0.950901 sd:0.180739\n",
      "\t230: \"data:0.304\" NUMERICAL mean:-0.256746 min:-1.09595 max:0.59868 sd:0.232461\n",
      "\t231: \"data:0.305\" NUMERICAL mean:-0.430679 min:-1.34858 max:0.544873 sd:0.287843\n",
      "\t232: \"data:0.306\" NUMERICAL mean:-0.173129 min:-0.754163 max:0.693798 sd:0.174899\n",
      "\t233: \"data:0.307\" NUMERICAL mean:-0.0617163 min:-0.953548 max:0.877165 sd:0.224019\n",
      "\t234: \"data:0.308\" NUMERICAL mean:-0.123105 min:-0.946716 max:0.659698 sd:0.189372\n",
      "\t235: \"data:0.309\" NUMERICAL mean:0.126147 min:-0.754457 max:0.841044 sd:0.19128\n",
      "\t236: \"data:0.31\" NUMERICAL mean:-0.0847513 min:-1.06968 max:0.97066 sd:0.294087\n",
      "\t237: \"data:0.310\" NUMERICAL mean:0.0232576 min:-0.910811 max:0.92164 sd:0.237329\n",
      "\t238: \"data:0.311\" NUMERICAL mean:-0.0697773 min:-0.869423 max:0.74665 sd:0.213778\n",
      "\t239: \"data:0.312\" NUMERICAL mean:-0.227744 min:-0.955067 max:0.539667 sd:0.164556\n",
      "\t240: \"data:0.313\" NUMERICAL mean:0.00521864 min:-0.676391 max:0.839713 sd:0.178407\n",
      "\t241: \"data:0.314\" NUMERICAL mean:-0.00932566 min:-0.775477 max:0.952387 sd:0.209754\n",
      "\t242: \"data:0.315\" NUMERICAL mean:0.149155 min:-0.488891 max:0.82811 sd:0.1453\n",
      "\t243: \"data:0.316\" NUMERICAL mean:0.0221163 min:-0.710054 max:0.637649 sd:0.168729\n",
      "\t244: \"data:0.317\" NUMERICAL mean:-0.0871166 min:-1.10488 max:0.742321 sd:0.198397\n",
      "\t245: \"data:0.318\" NUMERICAL mean:-0.0046075 min:-0.605803 max:0.620543 sd:0.160035\n",
      "\t246: \"data:0.319\" NUMERICAL mean:-0.306798 min:-1.20434 max:0.8747 sd:0.243759\n",
      "\t247: \"data:0.32\" NUMERICAL mean:-0.42301 min:-2.75995 max:1.40031 sd:0.465765\n",
      "\t248: \"data:0.320\" NUMERICAL mean:-0.0622015 min:-0.778726 max:0.849615 sd:0.200666\n",
      "\t249: \"data:0.321\" NUMERICAL mean:0.0571242 min:-0.685196 max:0.722908 sd:0.197785\n",
      "\t250: \"data:0.322\" NUMERICAL mean:0.10466 min:-1.03202 max:0.972474 sd:0.270958\n",
      "\t251: \"data:0.323\" NUMERICAL mean:0.00454574 min:-0.961131 max:0.815442 sd:0.223906\n",
      "\t252: \"data:0.324\" NUMERICAL mean:-0.16774 min:-0.958047 max:0.676853 sd:0.200095\n",
      "\t253: \"data:0.325\" NUMERICAL mean:0.0809981 min:-0.827493 max:0.623107 sd:0.152709\n",
      "\t254: \"data:0.326\" NUMERICAL mean:0.0944542 min:-0.527922 max:0.859055 sd:0.177788\n",
      "\t255: \"data:0.327\" NUMERICAL mean:0.0699764 min:-0.688877 max:0.759005 sd:0.19904\n",
      "\t256: \"data:0.328\" NUMERICAL mean:-0.0379135 min:-0.780289 max:0.782171 sd:0.192564\n",
      "\t257: \"data:0.329\" NUMERICAL mean:0.00352921 min:-0.526362 max:0.736293 sd:0.156335\n",
      "\t258: \"data:0.33\" NUMERICAL mean:0.13185 min:-0.655559 max:0.725439 sd:0.197577\n",
      "\t259: \"data:0.330\" NUMERICAL mean:-0.14451 min:-0.649914 max:0.242097 sd:0.106846\n",
      "\t260: \"data:0.331\" NUMERICAL mean:-0.313392 min:-1.34699 max:0.771475 sd:0.308866\n",
      "\t261: \"data:0.332\" NUMERICAL mean:0.127714 min:-0.855876 max:0.933932 sd:0.233505\n",
      "\t262: \"data:0.333\" NUMERICAL mean:0.237811 min:-0.656017 max:0.989058 sd:0.200076\n",
      "\t263: \"data:0.334\" NUMERICAL mean:-0.0227911 min:-0.957845 max:0.68928 sd:0.222608\n",
      "\t264: \"data:0.335\" NUMERICAL mean:0.0871508 min:-0.848931 max:1.02534 sd:0.213662\n",
      "\t265: \"data:0.336\" NUMERICAL mean:-0.151766 min:-0.716826 max:0.467633 sd:0.136756\n",
      "\t266: \"data:0.337\" NUMERICAL mean:0.0352896 min:-0.653414 max:0.826494 sd:0.19504\n",
      "\t267: \"data:0.338\" NUMERICAL mean:-0.16801 min:-0.853015 max:0.718563 sd:0.188794\n",
      "\t268: \"data:0.339\" NUMERICAL mean:0.00276866 min:-0.903186 max:0.961319 sd:0.233796\n",
      "\t269: \"data:0.34\" NUMERICAL mean:0.0712597 min:-0.691911 max:0.823615 sd:0.188536\n",
      "\t270: \"data:0.340\" NUMERICAL mean:0.0643096 min:-0.343513 max:0.334426 sd:0.0855326\n",
      "\t271: \"data:0.341\" NUMERICAL mean:-0.0119011 min:-0.766115 max:0.850406 sd:0.167201\n",
      "\t272: \"data:0.342\" NUMERICAL mean:0.178656 min:-0.718536 max:0.833849 sd:0.22314\n",
      "\t273: \"data:0.343\" NUMERICAL mean:0.154326 min:-0.516068 max:1.07323 sd:0.192884\n",
      "\t274: \"data:0.344\" NUMERICAL mean:0.0427267 min:-0.417212 max:0.465101 sd:0.107835\n",
      "\t275: \"data:0.345\" NUMERICAL mean:0.111825 min:-0.435139 max:0.465262 sd:0.117434\n",
      "\t276: \"data:0.346\" NUMERICAL mean:0.104605 min:-0.507679 max:0.62628 sd:0.137402\n",
      "\t277: \"data:0.347\" NUMERICAL mean:-0.0805938 min:-0.692911 max:0.665991 sd:0.164812\n",
      "\t278: \"data:0.348\" NUMERICAL mean:-0.0189204 min:-0.964309 max:0.697874 sd:0.208904\n",
      "\t279: \"data:0.349\" NUMERICAL mean:0.137582 min:-0.704758 max:0.881731 sd:0.204268\n",
      "\t280: \"data:0.35\" NUMERICAL mean:0.225857 min:-0.930906 max:1.0777 sd:0.268278\n",
      "\t281: \"data:0.350\" NUMERICAL mean:0.254866 min:-0.661469 max:1.15557 sd:0.24559\n",
      "\t282: \"data:0.351\" NUMERICAL mean:0.0204433 min:-0.703743 max:0.721497 sd:0.160408\n",
      "\t283: \"data:0.352\" NUMERICAL mean:-0.00179043 min:-1.03544 max:0.941602 sd:0.232843\n",
      "\t284: \"data:0.353\" NUMERICAL mean:0.128239 min:-0.730114 max:0.903739 sd:0.213936\n",
      "\t285: \"data:0.354\" NUMERICAL mean:0.00299807 min:-0.788281 max:0.717612 sd:0.18112\n",
      "\t286: \"data:0.355\" NUMERICAL mean:0.0059131 min:-0.828143 max:0.871188 sd:0.209022\n",
      "\t287: \"data:0.356\" NUMERICAL mean:-0.0956511 min:-0.858917 max:0.613796 sd:0.210953\n",
      "\t288: \"data:0.357\" NUMERICAL mean:-0.10728 min:-1.02012 max:0.756004 sd:0.240332\n",
      "\t289: \"data:0.358\" NUMERICAL mean:-0.139799 min:-0.765657 max:0.874514 sd:0.217691\n",
      "\t290: \"data:0.359\" NUMERICAL mean:-0.0651481 min:-0.816325 max:0.778246 sd:0.17823\n",
      "\t291: \"data:0.36\" NUMERICAL mean:0.0717147 min:-0.626755 max:0.798397 sd:0.207822\n",
      "\t292: \"data:0.360\" NUMERICAL mean:-0.0164316 min:-0.771191 max:0.685015 sd:0.189892\n",
      "\t293: \"data:0.361\" NUMERICAL mean:0.0642175 min:-0.847629 max:0.831103 sd:0.194922\n",
      "\t294: \"data:0.362\" NUMERICAL mean:0.0326018 min:-1.00072 max:0.769243 sd:0.219232\n",
      "\t295: \"data:0.363\" NUMERICAL mean:0.0516085 min:-0.601965 max:0.61345 sd:0.152544\n",
      "\t296: \"data:0.364\" NUMERICAL mean:0.0967885 min:-1.0267 max:1.19599 sd:0.297239\n",
      "\t297: \"data:0.365\" NUMERICAL mean:0.0856659 min:-0.780654 max:0.752336 sd:0.207948\n",
      "\t298: \"data:0.366\" NUMERICAL mean:0.0836253 min:-0.969057 max:0.712748 sd:0.225133\n",
      "\t299: \"data:0.367\" NUMERICAL mean:-0.0758578 min:-1.22143 max:0.738925 sd:0.232522\n",
      "\t300: \"data:0.368\" NUMERICAL mean:0.138045 min:-0.934662 max:1.13815 sd:0.282086\n",
      "\t301: \"data:0.369\" NUMERICAL mean:0.119891 min:-0.867738 max:0.983423 sd:0.20109\n",
      "\t302: \"data:0.37\" NUMERICAL mean:-0.0490445 min:-0.515897 max:0.57979 sd:0.148121\n",
      "\t303: \"data:0.370\" NUMERICAL mean:0.291298 min:-0.745023 max:1.36272 sd:0.249681\n",
      "\t304: \"data:0.371\" NUMERICAL mean:-0.222144 min:-1.09522 max:0.693737 sd:0.240049\n",
      "\t305: \"data:0.372\" NUMERICAL mean:-0.0596443 min:-0.917507 max:0.783436 sd:0.193109\n",
      "\t306: \"data:0.373\" NUMERICAL mean:0.12726 min:-0.71547 max:0.903626 sd:0.19164\n",
      "\t307: \"data:0.374\" NUMERICAL mean:0.280295 min:-0.664095 max:1.32958 sd:0.2344\n",
      "\t308: \"data:0.375\" NUMERICAL mean:0.0182113 min:-0.531574 max:0.558473 sd:0.140509\n",
      "\t309: \"data:0.376\" NUMERICAL mean:-0.48174 min:-1.03074 max:-0.0281662 sd:0.102154\n",
      "\t310: \"data:0.377\" NUMERICAL mean:0.0503144 min:-1.03745 max:0.801368 sd:0.226854\n",
      "\t311: \"data:0.378\" NUMERICAL mean:0.167909 min:-0.685946 max:0.881155 sd:0.214395\n",
      "\t312: \"data:0.379\" NUMERICAL mean:0.0618159 min:-0.621666 max:0.616112 sd:0.140548\n",
      "\t313: \"data:0.38\" NUMERICAL mean:0.0250727 min:-0.726962 max:0.706981 sd:0.165801\n",
      "\t314: \"data:0.380\" NUMERICAL mean:-0.0379656 min:-0.977711 max:0.878596 sd:0.202788\n",
      "\t315: \"data:0.381\" NUMERICAL mean:-0.0534032 min:-0.709084 max:0.729833 sd:0.163099\n",
      "\t316: \"data:0.382\" NUMERICAL mean:-0.122186 min:-1.13299 max:0.702749 sd:0.22907\n",
      "\t317: \"data:0.383\" NUMERICAL mean:-0.015431 min:-0.923544 max:0.782994 sd:0.226026\n",
      "\t318: \"data:0.384\" NUMERICAL mean:-0.0627455 min:-0.770216 max:0.7189 sd:0.199892\n",
      "\t319: \"data:0.385\" NUMERICAL mean:-0.00635053 min:-0.770943 max:0.637986 sd:0.179631\n",
      "\t320: \"data:0.386\" NUMERICAL mean:-0.00246451 min:-0.831534 max:0.787302 sd:0.202898\n",
      "\t321: \"data:0.387\" NUMERICAL mean:0.0336386 min:-0.71487 max:0.812951 sd:0.206001\n",
      "\t322: \"data:0.388\" NUMERICAL mean:-0.0498408 min:-0.902594 max:0.876776 sd:0.208086\n",
      "\t323: \"data:0.389\" NUMERICAL mean:-0.12624 min:-0.964063 max:0.994313 sd:0.208387\n",
      "\t324: \"data:0.39\" NUMERICAL mean:0.0358844 min:-0.788204 max:0.828826 sd:0.190359\n",
      "\t325: \"data:0.390\" NUMERICAL mean:-0.0179656 min:-0.750762 max:0.963291 sd:0.195853\n",
      "\t326: \"data:0.391\" NUMERICAL mean:0.134594 min:-0.741671 max:0.758747 sd:0.178371\n",
      "\t327: \"data:0.392\" NUMERICAL mean:0.226669 min:-0.677844 max:0.806743 sd:0.191948\n",
      "\t328: \"data:0.393\" NUMERICAL mean:-0.0648916 min:-0.85696 max:1.0045 sd:0.232363\n",
      "\t329: \"data:0.394\" NUMERICAL mean:0.0606542 min:-0.783999 max:1.0887 sd:0.254983\n",
      "\t330: \"data:0.395\" NUMERICAL mean:0.250963 min:-0.774536 max:1.12373 sd:0.215646\n",
      "\t331: \"data:0.396\" NUMERICAL mean:-0.00538784 min:-0.868722 max:0.851865 sd:0.222092\n",
      "\t332: \"data:0.397\" NUMERICAL mean:-0.00327422 min:-0.310488 max:0.433356 sd:0.0984358\n",
      "\t333: \"data:0.398\" NUMERICAL mean:-0.007098 min:-0.667322 max:0.737738 sd:0.172358\n",
      "\t334: \"data:0.399\" NUMERICAL mean:-0.0195823 min:-0.481621 max:0.450665 sd:0.125858\n",
      "\t335: \"data:0.4\" NUMERICAL mean:0.0379678 min:-0.635271 max:0.804178 sd:0.180214\n",
      "\t336: \"data:0.40\" NUMERICAL mean:-0.0787227 min:-0.857795 max:0.835196 sd:0.160539\n",
      "\t337: \"data:0.400\" NUMERICAL mean:0.168283 min:-0.894351 max:1.00919 sd:0.225813\n",
      "\t338: \"data:0.401\" NUMERICAL mean:-0.0186524 min:-0.739189 max:0.825177 sd:0.159463\n",
      "\t339: \"data:0.402\" NUMERICAL mean:-0.039034 min:-0.333248 max:0.381233 sd:0.0829837\n",
      "\t340: \"data:0.403\" NUMERICAL mean:0.0644913 min:-0.456904 max:0.587889 sd:0.146089\n",
      "\t341: \"data:0.404\" NUMERICAL mean:0.0905858 min:-0.875771 max:1.30094 sd:0.276724\n",
      "\t342: \"data:0.405\" NUMERICAL mean:0.0778174 min:-1.0468 max:1.11362 sd:0.229842\n",
      "\t343: \"data:0.406\" NUMERICAL mean:0.142609 min:-0.74552 max:0.862841 sd:0.214495\n",
      "\t344: \"data:0.407\" NUMERICAL mean:-0.142826 min:-0.824658 max:0.608262 sd:0.160536\n",
      "\t345: \"data:0.408\" NUMERICAL mean:0.112678 min:-0.854222 max:0.714171 sd:0.181487\n",
      "\t346: \"data:0.409\" NUMERICAL mean:0.0950904 min:-0.973629 max:0.997003 sd:0.202295\n",
      "\t347: \"data:0.41\" NUMERICAL mean:-0.0141891 min:-0.904549 max:0.993042 sd:0.233675\n",
      "\t348: \"data:0.410\" NUMERICAL mean:-0.17323 min:-0.845013 max:0.89671 sd:0.194083\n",
      "\t349: \"data:0.411\" NUMERICAL mean:0.0584195 min:-0.871345 max:1.06224 sd:0.267586\n",
      "\t350: \"data:0.412\" NUMERICAL mean:0.0126858 min:-0.853219 max:1.18595 sd:0.210527\n",
      "\t351: \"data:0.413\" NUMERICAL mean:0.339997 min:-0.00907973 max:0.895004 sd:0.104558\n",
      "\t352: \"data:0.414\" NUMERICAL mean:0.0967779 min:-0.693131 max:0.890416 sd:0.187722\n",
      "\t353: \"data:0.415\" NUMERICAL mean:0.16759 min:-0.5985 max:0.791385 sd:0.176522\n",
      "\t354: \"data:0.416\" NUMERICAL mean:-0.046951 min:-0.641932 max:0.643123 sd:0.165628\n",
      "\t355: \"data:0.417\" NUMERICAL mean:0.123059 min:-0.540275 max:0.76935 sd:0.15639\n",
      "\t356: \"data:0.418\" NUMERICAL mean:0.305873 min:-0.232044 max:0.885332 sd:0.14697\n",
      "\t357: \"data:0.419\" NUMERICAL mean:-0.00263803 min:-0.880123 max:1.12155 sd:0.261949\n",
      "\t358: \"data:0.42\" NUMERICAL mean:0.00544274 min:-0.798485 max:0.85535 sd:0.183717\n",
      "\t359: \"data:0.420\" NUMERICAL mean:0.286066 min:-0.782486 max:1.17072 sd:0.230343\n",
      "\t360: \"data:0.421\" NUMERICAL mean:0.257858 min:-0.569584 max:1.21462 sd:0.219314\n",
      "\t361: \"data:0.422\" NUMERICAL mean:-0.129595 min:-0.889798 max:0.64371 sd:0.183111\n",
      "\t362: \"data:0.423\" NUMERICAL mean:0.196631 min:-0.729698 max:1.00566 sd:0.222253\n",
      "\t363: \"data:0.424\" NUMERICAL mean:-0.0866983 min:-0.834216 max:0.747411 sd:0.189644\n",
      "\t364: \"data:0.425\" NUMERICAL mean:-0.0931318 min:-0.777028 max:0.730794 sd:0.206872\n",
      "\t365: \"data:0.426\" NUMERICAL mean:-0.0755689 min:-0.715246 max:0.725196 sd:0.181531\n",
      "\t366: \"data:0.427\" NUMERICAL mean:-0.186534 min:-0.979544 max:0.601282 sd:0.204776\n",
      "\t367: \"data:0.428\" NUMERICAL mean:-0.0054693 min:-0.866758 max:0.985715 sd:0.24152\n",
      "\t368: \"data:0.429\" NUMERICAL mean:0.21 min:-0.656947 max:0.840921 sd:0.205393\n",
      "\t369: \"data:0.43\" NUMERICAL mean:-0.0589199 min:-0.963718 max:0.812293 sd:0.209162\n",
      "\t370: \"data:0.430\" NUMERICAL mean:-0.0378064 min:-0.904458 max:0.772912 sd:0.198318\n",
      "\t371: \"data:0.431\" NUMERICAL mean:0.112015 min:-0.338039 max:0.464999 sd:0.104997\n",
      "\t372: \"data:0.432\" NUMERICAL mean:0.00305256 min:-0.728482 max:0.721267 sd:0.230522\n",
      "\t373: \"data:0.433\" NUMERICAL mean:0.0950844 min:-0.675355 max:0.779676 sd:0.179529\n",
      "\t374: \"data:0.434\" NUMERICAL mean:0.0232966 min:-0.89783 max:0.665288 sd:0.170472\n",
      "\t375: \"data:0.435\" NUMERICAL mean:0.0719237 min:-0.701863 max:0.84164 sd:0.213332\n",
      "\t376: \"data:0.436\" NUMERICAL mean:-0.0401643 min:-0.611853 max:0.636772 sd:0.136548\n",
      "\t377: \"data:0.437\" NUMERICAL mean:0.165168 min:-0.555914 max:0.790517 sd:0.195615\n",
      "\t378: \"data:0.438\" NUMERICAL mean:0.052772 min:-0.837304 max:0.913048 sd:0.224885\n",
      "\t379: \"data:0.439\" NUMERICAL mean:0.163611 min:-0.738943 max:0.812008 sd:0.198858\n",
      "\t380: \"data:0.44\" NUMERICAL mean:-0.17655 min:-0.961058 max:0.697336 sd:0.197685\n",
      "\t381: \"data:0.440\" NUMERICAL mean:0.0319731 min:-0.945575 max:0.847146 sd:0.199294\n",
      "\t382: \"data:0.441\" NUMERICAL mean:-0.0165812 min:-0.835175 max:0.88353 sd:0.212903\n",
      "\t383: \"data:0.442\" NUMERICAL mean:0.188759 min:-1.19629 max:1.23028 sd:0.304072\n",
      "\t384: \"data:0.443\" NUMERICAL mean:0.00371017 min:-0.977211 max:0.994302 sd:0.213016\n",
      "\t385: \"data:0.444\" NUMERICAL mean:-0.121915 min:-1.07201 max:0.62353 sd:0.198228\n",
      "\t386: \"data:0.445\" NUMERICAL mean:0.0542413 min:-0.538135 max:0.576959 sd:0.161906\n",
      "\t387: \"data:0.446\" NUMERICAL mean:0.0476147 min:-0.661902 max:0.610342 sd:0.154093\n",
      "\t388: \"data:0.447\" NUMERICAL mean:-0.0188504 min:-0.946417 max:0.71462 sd:0.227424\n",
      "\t389: \"data:0.448\" NUMERICAL mean:-0.0280796 min:-0.752928 max:0.617348 sd:0.17533\n",
      "\t390: \"data:0.449\" NUMERICAL mean:0.182222 min:-0.585874 max:0.86935 sd:0.200718\n",
      "\t391: \"data:0.45\" NUMERICAL mean:0.232041 min:-0.536406 max:1.08777 sd:0.216342\n",
      "\t392: \"data:0.450\" NUMERICAL mean:-0.163421 min:-0.988875 max:0.718541 sd:0.221597\n",
      "\t393: \"data:0.451\" NUMERICAL mean:-0.0494876 min:-1.59727 max:1.07149 sd:0.303368\n",
      "\t394: \"data:0.452\" NUMERICAL mean:0.0588977 min:-0.165447 max:0.391077 sd:0.0576446\n",
      "\t395: \"data:0.453\" NUMERICAL mean:0.0759999 min:-1.00892 max:0.999837 sd:0.218982\n",
      "\t396: \"data:0.454\" NUMERICAL mean:0.3092 min:-0.687253 max:1.27706 sd:0.241776\n",
      "\t397: \"data:0.455\" NUMERICAL mean:0.0940793 min:-0.363515 max:0.598864 sd:0.125089\n",
      "\t398: \"data:0.456\" NUMERICAL mean:-0.049075 min:-0.874542 max:0.853321 sd:0.193862\n",
      "\t399: \"data:0.457\" NUMERICAL mean:-0.22428 min:-0.996768 max:0.915511 sd:0.228906\n",
      "\t400: \"data:0.458\" NUMERICAL mean:0.135985 min:-0.642945 max:1.02961 sd:0.203934\n",
      "\t401: \"data:0.459\" NUMERICAL mean:-0.0952248 min:-0.712145 max:0.718892 sd:0.172396\n",
      "\t402: \"data:0.46\" NUMERICAL mean:-0.16678 min:-0.898475 max:0.59665 sd:0.207242\n",
      "\t403: \"data:0.460\" NUMERICAL mean:0.191761 min:-1.0477 max:1.08442 sd:0.259393\n",
      "\t404: \"data:0.461\" NUMERICAL mean:-0.0824029 min:-1.35292 max:0.926195 sd:0.255235\n",
      "\t405: \"data:0.462\" NUMERICAL mean:-0.059336 min:-1.19591 max:0.751129 sd:0.218812\n",
      "\t406: \"data:0.463\" NUMERICAL mean:0.121447 min:-0.678005 max:0.833419 sd:0.201071\n",
      "\t407: \"data:0.464\" NUMERICAL mean:0.0719599 min:-0.880893 max:0.956326 sd:0.239196\n",
      "\t408: \"data:0.465\" NUMERICAL mean:0.0119661 min:-0.611246 max:0.631042 sd:0.166086\n",
      "\t409: \"data:0.466\" NUMERICAL mean:-0.143921 min:-0.855407 max:0.694752 sd:0.183111\n",
      "\t410: \"data:0.467\" NUMERICAL mean:-0.0513052 min:-0.68926 max:0.795136 sd:0.175781\n",
      "\t411: \"data:0.468\" NUMERICAL mean:0.16697 min:-0.632268 max:0.916878 sd:0.194376\n",
      "\t412: \"data:0.469\" NUMERICAL mean:0.0157469 min:-0.751253 max:0.845547 sd:0.200058\n",
      "\t413: \"data:0.47\" NUMERICAL mean:0.00153925 min:-0.812311 max:1.03019 sd:0.254927\n",
      "\t414: \"data:0.470\" NUMERICAL mean:0.108925 min:-0.742729 max:0.910709 sd:0.224962\n",
      "\t415: \"data:0.471\" NUMERICAL mean:0.219928 min:-0.699014 max:1.16904 sd:0.234344\n",
      "\t416: \"data:0.472\" NUMERICAL mean:-0.0164296 min:-1.03118 max:0.813125 sd:0.212603\n",
      "\t417: \"data:0.473\" NUMERICAL mean:-0.0984176 min:-0.586029 max:0.398677 sd:0.112401\n",
      "\t418: \"data:0.474\" NUMERICAL mean:-0.044938 min:-1.0027 max:0.692635 sd:0.207448\n",
      "\t419: \"data:0.475\" NUMERICAL mean:0.0988342 min:-0.723182 max:0.806212 sd:0.196787\n",
      "\t420: \"data:0.476\" NUMERICAL mean:0.127146 min:-0.498382 max:0.714731 sd:0.175487\n",
      "\t421: \"data:0.477\" NUMERICAL mean:0.13615 min:-0.788176 max:0.853473 sd:0.21825\n",
      "\t422: \"data:0.478\" NUMERICAL mean:0.139868 min:-0.603331 max:0.794152 sd:0.171839\n",
      "\t423: \"data:0.479\" NUMERICAL mean:0.188657 min:-0.835421 max:0.930893 sd:0.187198\n",
      "\t424: \"data:0.48\" NUMERICAL mean:0.0873733 min:-1.0092 max:1.22939 sd:0.263938\n",
      "\t425: \"data:0.480\" NUMERICAL mean:0.120429 min:-0.446683 max:0.778602 sd:0.144413\n",
      "\t426: \"data:0.481\" NUMERICAL mean:-0.00976093 min:-0.532428 max:0.542964 sd:0.134551\n",
      "\t427: \"data:0.482\" NUMERICAL mean:-0.212797 min:-1.07819 max:0.790833 sd:0.234237\n",
      "\t428: \"data:0.483\" NUMERICAL mean:-0.0198998 min:-0.878406 max:1.18608 sd:0.233979\n",
      "\t429: \"data:0.484\" NUMERICAL mean:-0.144282 min:-0.886346 max:0.744501 sd:0.193865\n",
      "\t430: \"data:0.485\" NUMERICAL mean:0.124562 min:-0.692761 max:0.8722 sd:0.182428\n",
      "\t431: \"data:0.486\" NUMERICAL mean:0.101287 min:-0.826762 max:0.78596 sd:0.209935\n",
      "\t432: \"data:0.487\" NUMERICAL mean:-0.0807714 min:-0.850279 max:0.725738 sd:0.209308\n",
      "\t433: \"data:0.488\" NUMERICAL mean:-0.0563924 min:-0.954604 max:0.832123 sd:0.234666\n",
      "\t434: \"data:0.489\" NUMERICAL mean:-0.00741863 min:-0.596202 max:0.764008 sd:0.162468\n",
      "\t435: \"data:0.49\" NUMERICAL mean:0.0539097 min:-0.75216 max:1.03569 sd:0.232387\n",
      "\t436: \"data:0.490\" NUMERICAL mean:-0.157399 min:-0.884674 max:0.786168 sd:0.200013\n",
      "\t437: \"data:0.491\" NUMERICAL mean:0.120524 min:-0.659371 max:0.720353 sd:0.189451\n",
      "\t438: \"data:0.492\" NUMERICAL mean:-0.0999763 min:-0.580236 max:0.377621 sd:0.135736\n",
      "\t439: \"data:0.493\" NUMERICAL mean:0.098277 min:-0.756492 max:0.869447 sd:0.206542\n",
      "\t440: \"data:0.494\" NUMERICAL mean:-0.0584153 min:-0.728669 max:0.706586 sd:0.155341\n",
      "\t441: \"data:0.495\" NUMERICAL mean:0.000546028 min:-0.883404 max:0.913729 sd:0.23853\n",
      "\t442: \"data:0.496\" NUMERICAL mean:0.0659404 min:-0.164 max:0.312806 sd:0.0608295\n",
      "\t443: \"data:0.497\" NUMERICAL mean:-0.070114 min:-0.741453 max:0.64807 sd:0.170845\n",
      "\t444: \"data:0.498\" NUMERICAL mean:-0.268192 min:-0.88014 max:0.759135 sd:0.198959\n",
      "\t445: \"data:0.499\" NUMERICAL mean:0.148726 min:-0.876736 max:1.14645 sd:0.219096\n",
      "\t446: \"data:0.5\" NUMERICAL mean:-0.0654766 min:-0.836991 max:1.1724 sd:0.248902\n",
      "\t447: \"data:0.50\" NUMERICAL mean:-0.0942507 min:-0.578025 max:0.36474 sd:0.115713\n",
      "\t448: \"data:0.500\" NUMERICAL mean:0.0973999 min:-0.707479 max:0.897733 sd:0.216282\n",
      "\t449: \"data:0.501\" NUMERICAL mean:-0.0369405 min:-0.904865 max:0.742577 sd:0.180635\n",
      "\t450: \"data:0.502\" NUMERICAL mean:0.199196 min:-0.788112 max:1.00867 sd:0.243203\n",
      "\t451: \"data:0.503\" NUMERICAL mean:-0.131928 min:-0.838359 max:0.715937 sd:0.193544\n",
      "\t452: \"data:0.504\" NUMERICAL mean:0.0283342 min:-0.612324 max:0.55288 sd:0.160704\n",
      "\t453: \"data:0.505\" NUMERICAL mean:0.0229809 min:-1.01364 max:0.827548 sd:0.228839\n",
      "\t454: \"data:0.506\" NUMERICAL mean:-0.0546379 min:-0.810604 max:0.698204 sd:0.182143\n",
      "\t455: \"data:0.507\" NUMERICAL mean:-0.00679024 min:-0.850557 max:0.787988 sd:0.202069\n",
      "\t456: \"data:0.508\" NUMERICAL mean:0.214232 min:-0.765651 max:1.0263 sd:0.241188\n",
      "\t457: \"data:0.509\" NUMERICAL mean:-0.235512 min:-1.19828 max:0.909513 sd:0.263434\n",
      "\t458: \"data:0.51\" NUMERICAL mean:0.284387 min:-0.795375 max:1.36146 sd:0.267254\n",
      "\t459: \"data:0.510\" NUMERICAL mean:-0.0743009 min:-1.06949 max:0.826905 sd:0.246288\n",
      "\t460: \"data:0.511\" NUMERICAL mean:0.0708362 min:-0.787779 max:0.805696 sd:0.194181\n",
      "\t461: \"data:0.512\" NUMERICAL mean:0.0930808 min:-0.854948 max:0.954975 sd:0.272505\n",
      "\t462: \"data:0.513\" NUMERICAL mean:-0.0448266 min:-0.460154 max:0.290578 sd:0.083502\n",
      "\t463: \"data:0.514\" NUMERICAL mean:0.0204723 min:-1.02771 max:1.09398 sd:0.241001\n",
      "\t464: \"data:0.515\" NUMERICAL mean:-0.13185 min:-1.03347 max:0.7218 sd:0.228902\n",
      "\t465: \"data:0.516\" NUMERICAL mean:0.103394 min:-0.873079 max:0.931666 sd:0.218955\n",
      "\t466: \"data:0.517\" NUMERICAL mean:-0.276803 min:-1.33827 max:1.23845 sd:0.314508\n",
      "\t467: \"data:0.518\" NUMERICAL mean:-0.0296717 min:-1.09438 max:0.733373 sd:0.229038\n",
      "\t468: \"data:0.519\" NUMERICAL mean:-0.164575 min:-0.862834 max:0.768322 sd:0.193541\n",
      "\t469: \"data:0.52\" NUMERICAL mean:-1.20681 min:-4.48789 max:2.13109 sd:0.817634\n",
      "\t470: \"data:0.520\" NUMERICAL mean:0.0672099 min:-0.866445 max:0.81689 sd:0.238849\n",
      "\t471: \"data:0.521\" NUMERICAL mean:0.0334177 min:-0.767611 max:0.8458 sd:0.23423\n",
      "\t472: \"data:0.522\" NUMERICAL mean:-0.0563587 min:-1.03363 max:0.765333 sd:0.236801\n",
      "\t473: \"data:0.523\" NUMERICAL mean:0.0537096 min:-0.270991 max:0.395833 sd:0.0970124\n",
      "\t474: \"data:0.524\" NUMERICAL mean:0.0184726 min:-0.535364 max:0.599528 sd:0.128317\n",
      "\t475: \"data:0.525\" NUMERICAL mean:-0.125973 min:-0.518653 max:0.499466 sd:0.108441\n",
      "\t476: \"data:0.526\" NUMERICAL mean:0.0873566 min:-0.900068 max:0.799979 sd:0.222712\n",
      "\t477: \"data:0.527\" NUMERICAL mean:-0.133454 min:-0.788063 max:0.600555 sd:0.173509\n",
      "\t478: \"data:0.528\" NUMERICAL mean:0.0468183 min:-0.459723 max:0.628409 sd:0.12972\n",
      "\t479: \"data:0.529\" NUMERICAL mean:0.0206046 min:-0.65325 max:0.74965 sd:0.171524\n",
      "\t480: \"data:0.53\" NUMERICAL mean:0.181633 min:-0.395328 max:0.721897 sd:0.150267\n",
      "\t481: \"data:0.530\" NUMERICAL mean:-0.0155089 min:-0.855728 max:0.880224 sd:0.240578\n",
      "\t482: \"data:0.531\" NUMERICAL mean:0.0444947 min:-0.736523 max:0.685555 sd:0.173783\n",
      "\t483: \"data:0.532\" NUMERICAL mean:0.0483873 min:-0.835231 max:0.764238 sd:0.206934\n",
      "\t484: \"data:0.533\" NUMERICAL mean:0.135708 min:-0.842349 max:0.865436 sd:0.217787\n",
      "\t485: \"data:0.534\" NUMERICAL mean:0.0957891 min:-0.776984 max:1.01581 sd:0.214607\n",
      "\t486: \"data:0.535\" NUMERICAL mean:0.109847 min:-0.238274 max:0.427989 sd:0.0747605\n",
      "\t487: \"data:0.536\" NUMERICAL mean:-0.0115659 min:-1.17329 max:0.832837 sd:0.219796\n",
      "\t488: \"data:0.537\" NUMERICAL mean:0.00134482 min:-0.831694 max:0.79421 sd:0.2139\n",
      "\t489: \"data:0.538\" NUMERICAL mean:0.011362 min:-0.965435 max:0.95392 sd:0.243509\n",
      "\t490: \"data:0.539\" NUMERICAL mean:0.0699526 min:-0.836102 max:1.09711 sd:0.246693\n",
      "\t491: \"data:0.54\" NUMERICAL mean:-0.00562888 min:-0.814906 max:0.777772 sd:0.195712\n",
      "\t492: \"data:0.540\" NUMERICAL mean:0.154186 min:-0.827368 max:0.824143 sd:0.237853\n",
      "\t493: \"data:0.541\" NUMERICAL mean:0.182374 min:-0.887406 max:1.06832 sd:0.227489\n",
      "\t494: \"data:0.542\" NUMERICAL mean:-0.0958431 min:-1.14694 max:1.03904 sd:0.241846\n",
      "\t495: \"data:0.543\" NUMERICAL mean:-0.0973697 min:-0.906375 max:0.869857 sd:0.217218\n",
      "\t496: \"data:0.544\" NUMERICAL mean:-0.225683 min:-2.1989 max:2.0651 sd:0.57161\n",
      "\t497: \"data:0.545\" NUMERICAL mean:0.0169392 min:-0.841983 max:0.843877 sd:0.235453\n",
      "\t498: \"data:0.546\" NUMERICAL mean:0.138712 min:-0.391567 max:0.589027 sd:0.138585\n",
      "\t499: \"data:0.547\" NUMERICAL mean:0.0223463 min:-0.677662 max:0.981239 sd:0.196887\n",
      "\t500: \"data:0.548\" NUMERICAL mean:0.0145929 min:-0.82914 max:0.656933 sd:0.164232\n",
      "\t501: \"data:0.549\" NUMERICAL mean:-0.120077 min:-1.00764 max:0.815813 sd:0.24846\n",
      "\t502: \"data:0.55\" NUMERICAL mean:-0.115033 min:-2.34369 max:1.94183 sd:0.594691\n",
      "\t503: \"data:0.550\" NUMERICAL mean:-0.0651532 min:-1.24981 max:0.816248 sd:0.238447\n",
      "\t504: \"data:0.551\" NUMERICAL mean:0.141785 min:-0.690054 max:0.947892 sd:0.219938\n",
      "\t505: \"data:0.552\" NUMERICAL mean:-0.0035974 min:-1.0343 max:0.946227 sd:0.284526\n",
      "\t506: \"data:0.553\" NUMERICAL mean:-0.10469 min:-0.906402 max:0.75108 sd:0.20321\n",
      "\t507: \"data:0.554\" NUMERICAL mean:-0.0973227 min:-0.954291 max:0.758236 sd:0.228998\n",
      "\t508: \"data:0.555\" NUMERICAL mean:-0.0274575 min:-0.703443 max:0.943602 sd:0.193639\n",
      "\t509: \"data:0.556\" NUMERICAL mean:0.0775231 min:-0.549787 max:0.732863 sd:0.182315\n",
      "\t510: \"data:0.557\" NUMERICAL mean:0.0196236 min:-0.526039 max:0.674711 sd:0.131938\n",
      "\t511: \"data:0.558\" NUMERICAL mean:0.0286006 min:-0.236763 max:0.225519 sd:0.0524831\n",
      "\t512: \"data:0.559\" NUMERICAL mean:-0.238969 min:-1.33953 max:0.935344 sd:0.258045\n",
      "\t513: \"data:0.56\" NUMERICAL mean:-0.0556486 min:-0.891708 max:0.864392 sd:0.217447\n",
      "\t514: \"data:0.560\" NUMERICAL mean:0.0943154 min:-0.772103 max:0.862251 sd:0.194791\n",
      "\t515: \"data:0.561\" NUMERICAL mean:0.0304682 min:-0.818266 max:0.980327 sd:0.220394\n",
      "\t516: \"data:0.562\" NUMERICAL mean:0.00217519 min:-0.592365 max:0.721024 sd:0.158348\n",
      "\t517: \"data:0.563\" NUMERICAL mean:0.120739 min:-0.587668 max:0.865705 sd:0.176451\n",
      "\t518: \"data:0.564\" NUMERICAL mean:0.085263 min:-0.175986 max:0.269847 sd:0.0525976\n",
      "\t519: \"data:0.565\" NUMERICAL mean:0.0656494 min:-0.655464 max:0.670075 sd:0.160791\n",
      "\t520: \"data:0.566\" NUMERICAL mean:0.0500539 min:-0.844533 max:0.748992 sd:0.202111\n",
      "\t521: \"data:0.567\" NUMERICAL mean:0.195407 min:-0.725994 max:0.921701 sd:0.19525\n",
      "\t522: \"data:0.568\" NUMERICAL mean:0.224139 min:-0.790858 max:0.929546 sd:0.183105\n",
      "\t523: \"data:0.569\" NUMERICAL mean:-0.139677 min:-0.910396 max:0.805424 sd:0.208805\n",
      "\t524: \"data:0.57\" NUMERICAL mean:0.207792 min:-0.738619 max:0.815286 sd:0.181268\n",
      "\t525: \"data:0.570\" NUMERICAL mean:0.0153002 min:-0.373056 max:0.351291 sd:0.106058\n",
      "\t526: \"data:0.571\" NUMERICAL mean:-0.0174163 min:-0.411975 max:0.294762 sd:0.0919889\n",
      "\t527: \"data:0.572\" NUMERICAL mean:0.069624 min:-1.03066 max:1.21451 sd:0.250255\n",
      "\t528: \"data:0.573\" NUMERICAL mean:-0.0168815 min:-0.693461 max:0.891652 sd:0.184082\n",
      "\t529: \"data:0.574\" NUMERICAL mean:-0.00828571 min:-0.87217 max:0.781405 sd:0.196014\n",
      "\t530: \"data:0.575\" NUMERICAL mean:0.132006 min:-0.192588 max:0.634601 sd:0.107719\n",
      "\t531: \"data:0.576\" NUMERICAL mean:0.113523 min:-0.601705 max:0.657242 sd:0.161876\n",
      "\t532: \"data:0.577\" NUMERICAL mean:-0.04329 min:-0.717657 max:0.743499 sd:0.176601\n",
      "\t533: \"data:0.578\" NUMERICAL mean:0.00128462 min:-0.902508 max:0.787163 sd:0.204344\n",
      "\t534: \"data:0.579\" NUMERICAL mean:0.0578318 min:-0.702213 max:0.851611 sd:0.193926\n",
      "\t535: \"data:0.58\" NUMERICAL mean:0.101323 min:-0.285035 max:0.445624 sd:0.0808907\n",
      "\t536: \"data:0.580\" NUMERICAL mean:-0.101915 min:-1.09997 max:1.37732 sd:0.288587\n",
      "\t537: \"data:0.581\" NUMERICAL mean:0.158113 min:-0.502695 max:0.663461 sd:0.142993\n",
      "\t538: \"data:0.582\" NUMERICAL mean:0.106115 min:-0.641791 max:0.840803 sd:0.177878\n",
      "\t539: \"data:0.583\" NUMERICAL mean:-0.183523 min:-1.08664 max:0.752628 sd:0.216317\n",
      "\t540: \"data:0.584\" NUMERICAL mean:-0.00323947 min:-0.668688 max:0.823421 sd:0.191424\n",
      "\t541: \"data:0.585\" NUMERICAL mean:-0.167188 min:-1.02886 max:0.73356 sd:0.218282\n",
      "\t542: \"data:0.586\" NUMERICAL mean:0.0817671 min:-0.72303 max:0.989744 sd:0.205101\n",
      "\t543: \"data:0.587\" NUMERICAL mean:0.0882397 min:-0.884036 max:1.0817 sd:0.251411\n",
      "\t544: \"data:0.588\" NUMERICAL mean:0.0110529 min:-0.924383 max:0.75564 sd:0.228781\n",
      "\t545: \"data:0.589\" NUMERICAL mean:-0.0421428 min:-0.869656 max:0.730935 sd:0.175165\n",
      "\t546: \"data:0.59\" NUMERICAL mean:-0.0421201 min:-0.814142 max:0.809271 sd:0.203373\n",
      "\t547: \"data:0.590\" NUMERICAL mean:-0.0731704 min:-0.843967 max:0.911088 sd:0.20023\n",
      "\t548: \"data:0.591\" NUMERICAL mean:0.00803288 min:-0.802568 max:0.835647 sd:0.222669\n",
      "\t549: \"data:0.592\" NUMERICAL mean:-0.0893879 min:-0.97402 max:0.902944 sd:0.252182\n",
      "\t550: \"data:0.593\" NUMERICAL mean:0.0478374 min:-0.745797 max:0.842905 sd:0.211186\n",
      "\t551: \"data:0.594\" NUMERICAL mean:-0.079951 min:-0.938621 max:0.94451 sd:0.252376\n",
      "\t552: \"data:0.595\" NUMERICAL mean:-0.018236 min:-0.840146 max:0.706011 sd:0.171549\n",
      "\t553: \"data:0.596\" NUMERICAL mean:0.0234324 min:-0.844327 max:0.797471 sd:0.207706\n",
      "\t554: \"data:0.597\" NUMERICAL mean:-0.0640917 min:-0.360312 max:0.325572 sd:0.08349\n",
      "\t555: \"data:0.598\" NUMERICAL mean:0.187735 min:-0.608359 max:0.87162 sd:0.17979\n",
      "\t556: \"data:0.599\" NUMERICAL mean:-0.0192716 min:-0.718431 max:0.743882 sd:0.212753\n",
      "\t557: \"data:0.6\" NUMERICAL mean:-0.114522 min:-1.11221 max:0.705992 sd:0.213555\n",
      "\t558: \"data:0.60\" NUMERICAL mean:0.279271 min:-0.696211 max:1.20923 sd:0.23142\n",
      "\t559: \"data:0.600\" NUMERICAL mean:0.0297422 min:-0.416343 max:0.551041 sd:0.128032\n",
      "\t560: \"data:0.601\" NUMERICAL mean:-0.075762 min:-0.76052 max:0.401598 sd:0.143299\n",
      "\t561: \"data:0.602\" NUMERICAL mean:-0.0377073 min:-0.865996 max:0.761965 sd:0.170771\n",
      "\t562: \"data:0.603\" NUMERICAL mean:-0.227528 min:-0.818987 max:0.582776 sd:0.189357\n",
      "\t563: \"data:0.604\" NUMERICAL mean:-0.0362506 min:-0.894268 max:0.955811 sd:0.255867\n",
      "\t564: \"data:0.605\" NUMERICAL mean:-0.0651794 min:-0.700974 max:0.446702 sd:0.141232\n",
      "\t565: \"data:0.606\" NUMERICAL mean:0.0159886 min:-0.572108 max:0.642059 sd:0.14884\n",
      "\t566: \"data:0.607\" NUMERICAL mean:0.0588366 min:-0.797418 max:0.824185 sd:0.185692\n",
      "\t567: \"data:0.608\" NUMERICAL mean:0.15985 min:-1.2717 max:1.5917 sd:0.297327\n",
      "\t568: \"data:0.609\" NUMERICAL mean:0.0531745 min:-0.531906 max:0.426752 sd:0.121566\n",
      "\t569: \"data:0.61\" NUMERICAL mean:-0.150751 min:-1.03129 max:0.959337 sd:0.274734\n",
      "\t570: \"data:0.610\" NUMERICAL mean:0.215218 min:-0.62764 max:1.15628 sd:0.259375\n",
      "\t571: \"data:0.611\" NUMERICAL mean:0.175203 min:-0.704444 max:1.10629 sd:0.210886\n",
      "\t572: \"data:0.612\" NUMERICAL mean:0.150345 min:-0.276835 max:0.627289 sd:0.108068\n",
      "\t573: \"data:0.613\" NUMERICAL mean:0.0399894 min:-0.610846 max:0.546652 sd:0.133696\n",
      "\t574: \"data:0.614\" NUMERICAL mean:0.0565607 min:-0.901248 max:1.22569 sd:0.249845\n",
      "\t575: \"data:0.615\" NUMERICAL mean:-0.139691 min:-0.954945 max:0.920299 sd:0.20656\n",
      "\t576: \"data:0.616\" NUMERICAL mean:0.0401836 min:-0.891787 max:0.982834 sd:0.221271\n",
      "\t577: \"data:0.617\" NUMERICAL mean:0.0247479 min:-0.405267 max:0.610002 sd:0.145913\n",
      "\t578: \"data:0.618\" NUMERICAL mean:-0.345913 min:-0.906882 max:0.443248 sd:0.174134\n",
      "\t579: \"data:0.619\" NUMERICAL mean:0.253823 min:-0.90135 max:1.2413 sd:0.32728\n",
      "\t580: \"data:0.62\" NUMERICAL mean:0.0463504 min:-0.832688 max:0.80991 sd:0.201977\n",
      "\t581: \"data:0.620\" NUMERICAL mean:0.0832135 min:-0.780114 max:0.809277 sd:0.201622\n",
      "\t582: \"data:0.621\" NUMERICAL mean:-0.0637654 min:-0.683904 max:0.667775 sd:0.184059\n",
      "\t583: \"data:0.622\" NUMERICAL mean:-0.0618591 min:-0.855718 max:0.837543 sd:0.246469\n",
      "\t584: \"data:0.623\" NUMERICAL mean:0.130049 min:-0.676126 max:0.927196 sd:0.19284\n",
      "\t585: \"data:0.624\" NUMERICAL mean:-0.0865969 min:-1.03305 max:1.03965 sd:0.243773\n",
      "\t586: \"data:0.625\" NUMERICAL mean:0.0678641 min:-0.644114 max:0.963741 sd:0.179533\n",
      "\t587: \"data:0.626\" NUMERICAL mean:0.153779 min:-0.888423 max:0.969814 sd:0.198743\n",
      "\t588: \"data:0.627\" NUMERICAL mean:-0.102333 min:-0.847443 max:0.742314 sd:0.215604\n",
      "\t589: \"data:0.628\" NUMERICAL mean:-0.00239553 min:-0.882322 max:0.702955 sd:0.198337\n",
      "\t590: \"data:0.629\" NUMERICAL mean:0.278755 min:-0.731215 max:1.07202 sd:0.228723\n",
      "\t591: \"data:0.63\" NUMERICAL mean:-0.101913 min:-1.01955 max:0.839923 sd:0.24378\n",
      "\t592: \"data:0.630\" NUMERICAL mean:-0.0293001 min:-0.964605 max:0.865267 sd:0.210308\n",
      "\t593: \"data:0.631\" NUMERICAL mean:-0.00795455 min:-0.212004 max:0.260296 sd:0.0549372\n",
      "\t594: \"data:0.632\" NUMERICAL mean:0.0116708 min:-0.611544 max:0.776908 sd:0.190021\n",
      "\t595: \"data:0.633\" NUMERICAL mean:0.00260196 min:-0.797296 max:0.999188 sd:0.18782\n",
      "\t596: \"data:0.634\" NUMERICAL mean:-0.13247 min:-1.07488 max:0.922719 sd:0.276353\n",
      "\t597: \"data:0.635\" NUMERICAL mean:-0.18604 min:-0.95494 max:0.748699 sd:0.195734\n",
      "\t598: \"data:0.636\" NUMERICAL mean:-0.115886 min:-0.80689 max:0.71281 sd:0.183777\n",
      "\t599: \"data:0.637\" NUMERICAL mean:0.170335 min:-0.702039 max:1.21514 sd:0.228125\n",
      "\t600: \"data:0.638\" NUMERICAL mean:0.0428286 min:-0.7284 max:0.92306 sd:0.189167\n",
      "\t601: \"data:0.639\" NUMERICAL mean:0.0548427 min:-0.715877 max:0.768744 sd:0.193364\n",
      "\t602: \"data:0.64\" NUMERICAL mean:-0.00951276 min:-0.770198 max:0.718605 sd:0.21744\n",
      "\t603: \"data:0.640\" NUMERICAL mean:-0.157463 min:-0.961155 max:0.704492 sd:0.202618\n",
      "\t604: \"data:0.641\" NUMERICAL mean:-0.0497766 min:-0.763843 max:0.884426 sd:0.218298\n",
      "\t605: \"data:0.642\" NUMERICAL mean:-0.0435656 min:-0.762586 max:0.608316 sd:0.187686\n",
      "\t606: \"data:0.643\" NUMERICAL mean:0.117778 min:-0.665901 max:1.03105 sd:0.190781\n",
      "\t607: \"data:0.644\" NUMERICAL mean:0.22217 min:-0.96896 max:1.13112 sd:0.251364\n",
      "\t608: \"data:0.645\" NUMERICAL mean:-0.00577914 min:-0.646891 max:0.652712 sd:0.166662\n",
      "\t609: \"data:0.646\" NUMERICAL mean:-0.148198 min:-1.1153 max:0.981371 sd:0.24986\n",
      "\t610: \"data:0.647\" NUMERICAL mean:0.299423 min:-0.882597 max:1.12521 sd:0.217133\n",
      "\t611: \"data:0.648\" NUMERICAL mean:0.0431302 min:-0.865689 max:0.82323 sd:0.205505\n",
      "\t612: \"data:0.649\" NUMERICAL mean:0.222821 min:-0.575604 max:1.08954 sd:0.20727\n",
      "\t613: \"data:0.65\" NUMERICAL mean:0.179665 min:-0.624501 max:0.949588 sd:0.20763\n",
      "\t614: \"data:0.650\" NUMERICAL mean:0.193936 min:-0.0123819 max:0.420055 sd:0.0533232\n",
      "\t615: \"data:0.651\" NUMERICAL mean:0.118949 min:-0.628244 max:0.796151 sd:0.167202\n",
      "\t616: \"data:0.652\" NUMERICAL mean:-0.118216 min:-0.83848 max:0.855602 sd:0.221623\n",
      "\t617: \"data:0.653\" NUMERICAL mean:0.142066 min:-0.792909 max:1.12526 sd:0.22398\n",
      "\t618: \"data:0.654\" NUMERICAL mean:0.0252617 min:-0.829509 max:0.87131 sd:0.218149\n",
      "\t619: \"data:0.655\" NUMERICAL mean:0.0129779 min:-1.08916 max:1.30994 sd:0.260946\n",
      "\t620: \"data:0.656\" NUMERICAL mean:0.0166477 min:-0.757207 max:0.583544 sd:0.185424\n",
      "\t621: \"data:0.657\" NUMERICAL mean:-0.104003 min:-0.911497 max:0.913205 sd:0.228301\n",
      "\t622: \"data:0.658\" NUMERICAL mean:-0.0269035 min:-0.832647 max:0.733597 sd:0.203034\n",
      "\t623: \"data:0.659\" NUMERICAL mean:-0.165108 min:-1.2738 max:1.09276 sd:0.297614\n",
      "\t624: \"data:0.66\" NUMERICAL mean:0.0420281 min:-0.617543 max:0.618357 sd:0.146178\n",
      "\t625: \"data:0.660\" NUMERICAL mean:0.0599969 min:-0.655401 max:0.86858 sd:0.182858\n",
      "\t626: \"data:0.661\" NUMERICAL mean:0.029968 min:-0.74364 max:0.691854 sd:0.143874\n",
      "\t627: \"data:0.662\" NUMERICAL mean:0.0851507 min:-0.787989 max:0.786916 sd:0.197777\n",
      "\t628: \"data:0.663\" NUMERICAL mean:-0.0369989 min:-0.979578 max:0.866502 sd:0.22223\n",
      "\t629: \"data:0.664\" NUMERICAL mean:-0.0191195 min:-0.852072 max:0.779777 sd:0.203961\n",
      "\t630: \"data:0.665\" NUMERICAL mean:0.0617459 min:-0.808592 max:0.871335 sd:0.169562\n",
      "\t631: \"data:0.666\" NUMERICAL mean:-0.0213657 min:-0.764748 max:0.907126 sd:0.188472\n",
      "\t632: \"data:0.667\" NUMERICAL mean:-0.0640452 min:-0.855246 max:0.619537 sd:0.159736\n",
      "\t633: \"data:0.668\" NUMERICAL mean:-0.257156 min:-1.07554 max:0.929707 sd:0.256687\n",
      "\t634: \"data:0.669\" NUMERICAL mean:-0.0815831 min:-1.03846 max:0.876829 sd:0.233595\n",
      "\t635: \"data:0.67\" NUMERICAL mean:0.00429351 min:-0.81199 max:0.831833 sd:0.198043\n",
      "\t636: \"data:0.670\" NUMERICAL mean:0.0010166 min:-0.717676 max:0.607793 sd:0.169838\n",
      "\t637: \"data:0.671\" NUMERICAL mean:-0.00606462 min:-0.843169 max:0.915229 sd:0.233555\n",
      "\t638: \"data:0.672\" NUMERICAL mean:-0.0349192 min:-0.730364 max:0.766837 sd:0.197066\n",
      "\t639: \"data:0.673\" NUMERICAL mean:-0.379492 min:-1.29488 max:0.69565 sd:0.272852\n",
      "\t640: \"data:0.674\" NUMERICAL mean:-0.254085 min:-1.10748 max:0.865699 sd:0.248362\n",
      "\t641: \"data:0.675\" NUMERICAL mean:-0.0450938 min:-0.346091 max:0.186114 sd:0.0652563\n",
      "\t642: \"data:0.676\" NUMERICAL mean:-0.0947081 min:-0.748511 max:0.961949 sd:0.183883\n",
      "\t643: \"data:0.677\" NUMERICAL mean:0.0358589 min:-0.562855 max:0.758314 sd:0.176735\n",
      "\t644: \"data:0.678\" NUMERICAL mean:0.0629099 min:-0.887255 max:0.82274 sd:0.203628\n",
      "\t645: \"data:0.679\" NUMERICAL mean:-0.118826 min:-1.21486 max:1.03509 sd:0.289315\n",
      "\t646: \"data:0.68\" NUMERICAL mean:-0.0539692 min:-0.272475 max:0.2307 sd:0.0580662\n",
      "\t647: \"data:0.680\" NUMERICAL mean:-0.246964 min:-2.95404 max:1.98403 sd:0.464704\n",
      "\t648: \"data:0.681\" NUMERICAL mean:-0.097469 min:-0.620639 max:0.625941 sd:0.140813\n",
      "\t649: \"data:0.682\" NUMERICAL mean:0.0198098 min:-0.955653 max:1.04593 sd:0.255566\n",
      "\t650: \"data:0.683\" NUMERICAL mean:-0.032787 min:-0.549563 max:0.378792 sd:0.110807\n",
      "\t651: \"data:0.684\" NUMERICAL mean:-0.16716 min:-0.89098 max:0.524629 sd:0.152148\n",
      "\t652: \"data:0.685\" NUMERICAL mean:0.0547598 min:-0.574457 max:0.662394 sd:0.149581\n",
      "\t653: \"data:0.686\" NUMERICAL mean:-0.036492 min:-0.803266 max:0.859112 sd:0.185689\n",
      "\t654: \"data:0.687\" NUMERICAL mean:-0.0710519 min:-0.767808 max:0.735264 sd:0.167511\n",
      "\t655: \"data:0.688\" NUMERICAL mean:0.117534 min:-0.979097 max:1.13213 sd:0.253207\n",
      "\t656: \"data:0.689\" NUMERICAL mean:0.016329 min:-0.948223 max:0.787027 sd:0.213848\n",
      "\t657: \"data:0.69\" NUMERICAL mean:-0.0229418 min:-0.787265 max:0.746626 sd:0.17952\n",
      "\t658: \"data:0.690\" NUMERICAL mean:-0.108521 min:-0.974818 max:0.67778 sd:0.212466\n",
      "\t659: \"data:0.691\" NUMERICAL mean:-0.184805 min:-0.74377 max:0.820993 sd:0.137451\n",
      "\t660: \"data:0.692\" NUMERICAL mean:0.0603246 min:-0.814308 max:0.93656 sd:0.214371\n",
      "\t661: \"data:0.693\" NUMERICAL mean:0.000886323 min:-0.382191 max:0.27051 sd:0.0813349\n",
      "\t662: \"data:0.694\" NUMERICAL mean:0.0359231 min:-0.901271 max:0.794622 sd:0.224259\n",
      "\t663: \"data:0.695\" NUMERICAL mean:0.0987244 min:-1.08171 max:1.1396 sd:0.252488\n",
      "\t664: \"data:0.696\" NUMERICAL mean:0.251588 min:-0.765984 max:0.960953 sd:0.237662\n",
      "\t665: \"data:0.697\" NUMERICAL mean:-0.0403203 min:-1.1502 max:0.810173 sd:0.301455\n",
      "\t666: \"data:0.698\" NUMERICAL mean:-0.241997 min:-0.927243 max:0.495474 sd:0.197265\n",
      "\t667: \"data:0.699\" NUMERICAL mean:-0.0310781 min:-0.984696 max:0.807858 sd:0.179454\n",
      "\t668: \"data:0.7\" NUMERICAL mean:0.224607 min:-0.724502 max:0.995057 sd:0.241009\n",
      "\t669: \"data:0.70\" NUMERICAL mean:0.1229 min:-0.572417 max:0.851235 sd:0.184852\n",
      "\t670: \"data:0.700\" NUMERICAL mean:0.0666398 min:-0.253531 max:0.422401 sd:0.0756549\n",
      "\t671: \"data:0.701\" NUMERICAL mean:0.0992934 min:-0.30503 max:0.57092 sd:0.106775\n",
      "\t672: \"data:0.702\" NUMERICAL mean:-0.0244579 min:-0.594683 max:0.590738 sd:0.153004\n",
      "\t673: \"data:0.703\" NUMERICAL mean:0.0982668 min:-0.95954 max:0.989026 sd:0.256791\n",
      "\t674: \"data:0.704\" NUMERICAL mean:0.0705649 min:-0.825778 max:0.828169 sd:0.20727\n",
      "\t675: \"data:0.705\" NUMERICAL mean:0.139031 min:-0.659974 max:0.807863 sd:0.187868\n",
      "\t676: \"data:0.706\" NUMERICAL mean:0.0473308 min:-0.797442 max:0.824035 sd:0.229347\n",
      "\t677: \"data:0.707\" NUMERICAL mean:0.101801 min:-1.02451 max:0.826491 sd:0.213978\n",
      "\t678: \"data:0.708\" NUMERICAL mean:-0.288118 min:-1.26387 max:0.684632 sd:0.225791\n",
      "\t679: \"data:0.709\" NUMERICAL mean:-0.0217637 min:-0.75251 max:0.731052 sd:0.174444\n",
      "\t680: \"data:0.71\" NUMERICAL mean:-0.134258 min:-1.03624 max:0.72378 sd:0.251996\n",
      "\t681: \"data:0.710\" NUMERICAL mean:-0.0622221 min:-0.522211 max:0.440249 sd:0.100152\n",
      "\t682: \"data:0.711\" NUMERICAL mean:-0.12539 min:-0.995613 max:0.872468 sd:0.204984\n",
      "\t683: \"data:0.712\" NUMERICAL mean:0.0363128 min:-0.945692 max:1.06392 sd:0.232276\n",
      "\t684: \"data:0.713\" NUMERICAL mean:-0.0251791 min:-1.26029 max:0.832934 sd:0.247633\n",
      "\t685: \"data:0.714\" NUMERICAL mean:-0.0834848 min:-1.06139 max:0.734993 sd:0.236148\n",
      "\t686: \"data:0.715\" NUMERICAL mean:-0.258458 min:-1.06675 max:0.630816 sd:0.222623\n",
      "\t687: \"data:0.716\" NUMERICAL mean:-0.190753 min:-1.00533 max:1.0517 sd:0.24633\n",
      "\t688: \"data:0.717\" NUMERICAL mean:-0.024161 min:-0.771965 max:0.677164 sd:0.181211\n",
      "\t689: \"data:0.718\" NUMERICAL mean:0.14602 min:-0.595234 max:0.982988 sd:0.187541\n",
      "\t690: \"data:0.719\" NUMERICAL mean:0.0548582 min:-0.491146 max:0.45854 sd:0.100583\n",
      "\t691: \"data:0.72\" NUMERICAL mean:-0.0871129 min:-0.993834 max:0.782142 sd:0.230468\n",
      "\t692: \"data:0.720\" NUMERICAL mean:0.0910978 min:-0.729629 max:0.981615 sd:0.188532\n",
      "\t693: \"data:0.721\" NUMERICAL mean:0.04152 min:-0.65672 max:0.729645 sd:0.169701\n",
      "\t694: \"data:0.722\" NUMERICAL mean:0.176841 min:0.00203766 max:0.384414 sd:0.0491836\n",
      "\t695: \"data:0.723\" NUMERICAL mean:-0.0471257 min:-0.863038 max:0.806787 sd:0.19319\n",
      "\t696: \"data:0.724\" NUMERICAL mean:0.0195938 min:-0.837978 max:0.917141 sd:0.220236\n",
      "\t697: \"data:0.725\" NUMERICAL mean:0.215225 min:-0.795371 max:1.00621 sd:0.219214\n",
      "\t698: \"data:0.726\" NUMERICAL mean:0.0130719 min:-0.606657 max:0.536262 sd:0.142564\n",
      "\t699: \"data:0.727\" NUMERICAL mean:-0.0413701 min:-0.954646 max:0.727079 sd:0.205605\n",
      "\t700: \"data:0.728\" NUMERICAL mean:-0.157906 min:-1.06717 max:0.758797 sd:0.232019\n",
      "\t701: \"data:0.729\" NUMERICAL mean:0.168976 min:-1.33835 max:2.19473 sd:0.437531\n",
      "\t702: \"data:0.73\" NUMERICAL mean:-0.0414203 min:-0.917336 max:0.957657 sd:0.239613\n",
      "\t703: \"data:0.730\" NUMERICAL mean:0.105698 min:-0.697132 max:0.996166 sd:0.22864\n",
      "\t704: \"data:0.731\" NUMERICAL mean:-0.265914 min:-0.966311 max:0.567216 sd:0.180218\n",
      "\t705: \"data:0.732\" NUMERICAL mean:0.0254267 min:-0.528966 max:0.645215 sd:0.132884\n",
      "\t706: \"data:0.733\" NUMERICAL mean:0.0652042 min:-0.416811 max:0.423814 sd:0.0949062\n",
      "\t707: \"data:0.734\" NUMERICAL mean:-0.0192918 min:-0.579854 max:0.480897 sd:0.111866\n",
      "\t708: \"data:0.735\" NUMERICAL mean:-0.149861 min:-1.0411 max:1.12073 sd:0.260544\n",
      "\t709: \"data:0.736\" NUMERICAL mean:0.105772 min:-0.6948 max:0.809784 sd:0.186321\n",
      "\t710: \"data:0.737\" NUMERICAL mean:0.178115 min:-0.848396 max:1.13481 sd:0.239648\n",
      "\t711: \"data:0.738\" NUMERICAL mean:0.255011 min:-0.662677 max:1.18542 sd:0.266966\n",
      "\t712: \"data:0.739\" NUMERICAL mean:-0.343224 min:-1.52161 max:0.776729 sd:0.269528\n",
      "\t713: \"data:0.74\" NUMERICAL mean:0.205015 min:-0.64338 max:0.982717 sd:0.231132\n",
      "\t714: \"data:0.740\" NUMERICAL mean:0.122763 min:-0.585039 max:0.8038 sd:0.162775\n",
      "\t715: \"data:0.741\" NUMERICAL mean:0.0839845 min:-0.938293 max:0.86049 sd:0.247757\n",
      "\t716: \"data:0.742\" NUMERICAL mean:-0.0678944 min:-0.85929 max:0.731627 sd:0.19942\n",
      "\t717: \"data:0.743\" NUMERICAL mean:0.00052806 min:-0.967767 max:0.929743 sd:0.220676\n",
      "\t718: \"data:0.744\" NUMERICAL mean:0.0191912 min:-0.517916 max:0.603134 sd:0.14184\n",
      "\t719: \"data:0.745\" NUMERICAL mean:0.0249534 min:-0.730736 max:0.67285 sd:0.202066\n",
      "\t720: \"data:0.746\" NUMERICAL mean:0.170244 min:-0.641899 max:0.969383 sd:0.193811\n",
      "\t721: \"data:0.747\" NUMERICAL mean:-0.0118755 min:-0.721153 max:0.713946 sd:0.161716\n",
      "\t722: \"data:0.748\" NUMERICAL mean:-0.0624617 min:-0.750475 max:0.776532 sd:0.209938\n",
      "\t723: \"data:0.749\" NUMERICAL mean:-0.0486774 min:-0.609685 max:0.671371 sd:0.16399\n",
      "\t724: \"data:0.75\" NUMERICAL mean:0.00530568 min:-0.418072 max:0.561775 sd:0.126281\n",
      "\t725: \"data:0.750\" NUMERICAL mean:0.0211216 min:-0.942198 max:0.692273 sd:0.196528\n",
      "\t726: \"data:0.751\" NUMERICAL mean:0.154553 min:-0.576361 max:0.884268 sd:0.193586\n",
      "\t727: \"data:0.752\" NUMERICAL mean:-0.162916 min:-0.971485 max:1.07177 sd:0.218204\n",
      "\t728: \"data:0.753\" NUMERICAL mean:0.151678 min:-0.783366 max:0.922752 sd:0.206722\n",
      "\t729: \"data:0.754\" NUMERICAL mean:-0.207562 min:-0.877865 max:0.658548 sd:0.195067\n",
      "\t730: \"data:0.755\" NUMERICAL mean:-0.109614 min:-0.781815 max:0.86235 sd:0.186828\n",
      "\t731: \"data:0.756\" NUMERICAL mean:0.0286516 min:-1.09705 max:0.974339 sd:0.238514\n",
      "\t732: \"data:0.757\" NUMERICAL mean:-0.0499437 min:-0.880895 max:0.831827 sd:0.232329\n",
      "\t733: \"data:0.758\" NUMERICAL mean:-0.100767 min:-0.854075 max:0.654241 sd:0.210087\n",
      "\t734: \"data:0.759\" NUMERICAL mean:0.0704659 min:-0.92912 max:0.791672 sd:0.20116\n",
      "\t735: \"data:0.76\" NUMERICAL mean:-0.0636198 min:-0.819681 max:0.74107 sd:0.178689\n",
      "\t736: \"data:0.760\" NUMERICAL mean:-0.0360419 min:-0.720638 max:0.775336 sd:0.206992\n",
      "\t737: \"data:0.761\" NUMERICAL mean:0.130498 min:-0.625247 max:0.842253 sd:0.209698\n",
      "\t738: \"data:0.762\" NUMERICAL mean:0.131304 min:-0.763285 max:0.748552 sd:0.179211\n",
      "\t739: \"data:0.763\" NUMERICAL mean:0.198764 min:-0.773325 max:0.878666 sd:0.221863\n",
      "\t740: \"data:0.764\" NUMERICAL mean:0.138939 min:-0.719582 max:0.895118 sd:0.224973\n",
      "\t741: \"data:0.765\" NUMERICAL mean:-0.0302087 min:-0.88258 max:0.712806 sd:0.205384\n",
      "\t742: \"data:0.766\" NUMERICAL mean:0.0402205 min:-1.01376 max:0.94957 sd:0.233669\n",
      "\t743: \"data:0.767\" NUMERICAL mean:0.0148941 min:-0.64889 max:0.605781 sd:0.157388\n",
      "\t744: \"data:0.77\" NUMERICAL mean:-0.0659723 min:-0.834132 max:0.704463 sd:0.192145\n",
      "\t745: \"data:0.78\" NUMERICAL mean:-0.0941924 min:-0.91454 max:0.795285 sd:0.207391\n",
      "\t746: \"data:0.79\" NUMERICAL mean:0.182787 min:-0.750724 max:0.924765 sd:0.168824\n",
      "\t747: \"data:0.8\" NUMERICAL mean:-0.026562 min:-0.881656 max:0.798564 sd:0.191752\n",
      "\t748: \"data:0.80\" NUMERICAL mean:0.121236 min:-0.522896 max:0.697291 sd:0.15563\n",
      "\t749: \"data:0.81\" NUMERICAL mean:-0.00866447 min:-0.648868 max:0.967677 sd:0.172236\n",
      "\t750: \"data:0.82\" NUMERICAL mean:0.148469 min:-0.714378 max:0.783199 sd:0.191676\n",
      "\t751: \"data:0.83\" NUMERICAL mean:-0.136424 min:-0.71386 max:0.38982 sd:0.140318\n",
      "\t752: \"data:0.84\" NUMERICAL mean:-0.105716 min:-0.659156 max:0.525154 sd:0.148395\n",
      "\t753: \"data:0.85\" NUMERICAL mean:0.129841 min:-0.697865 max:0.821683 sd:0.208276\n",
      "\t754: \"data:0.86\" NUMERICAL mean:-0.0279645 min:-0.897747 max:0.972041 sd:0.203945\n",
      "\t755: \"data:0.87\" NUMERICAL mean:0.142708 min:-0.569087 max:0.736085 sd:0.165072\n",
      "\t756: \"data:0.88\" NUMERICAL mean:0.0864879 min:-0.378287 max:0.49629 sd:0.109342\n",
      "\t757: \"data:0.89\" NUMERICAL mean:0.0331927 min:-0.630983 max:0.704147 sd:0.168108\n",
      "\t758: \"data:0.9\" NUMERICAL mean:-0.112618 min:-1.34316 max:0.977498 sd:0.263328\n",
      "\t759: \"data:0.90\" NUMERICAL mean:0.193519 min:-0.913828 max:0.946306 sd:0.225562\n",
      "\t760: \"data:0.91\" NUMERICAL mean:-0.0113288 min:-0.724197 max:0.853648 sd:0.225076\n",
      "\t761: \"data:0.92\" NUMERICAL mean:-0.114818 min:-0.976984 max:0.792411 sd:0.225273\n",
      "\t762: \"data:0.93\" NUMERICAL mean:0.220548 min:-0.651418 max:1.07229 sd:0.264775\n",
      "\t763: \"data:0.94\" NUMERICAL mean:0.114399 min:-0.68058 max:0.89453 sd:0.202293\n",
      "\t764: \"data:0.95\" NUMERICAL mean:-0.0872901 min:-1.068 max:0.801338 sd:0.235341\n",
      "\t765: \"data:0.96\" NUMERICAL mean:0.0828943 min:-0.258183 max:0.453624 sd:0.0975086\n",
      "\t766: \"data:0.97\" NUMERICAL mean:0.276957 min:-0.802897 max:1.26025 sd:0.230045\n",
      "\t767: \"data:0.98\" NUMERICAL mean:0.0207146 min:-0.554861 max:0.740326 sd:0.161082\n",
      "\t768: \"data:0.99\" NUMERICAL mean:-0.0552584 min:-0.76688 max:0.799437 sd:0.190945\n",
      "\n",
      "CATEGORICAL: 1 (0.130039%)\n",
      "\t0: \"__LABEL\" CATEGORICAL integerized vocab-size:3 no-ood-item\n",
      "\n",
      "Terminology:\n",
      "\tnas: Number of non-available (i.e. missing) values.\n",
      "\tood: Out of dictionary.\n",
      "\tmanually-defined: Attribute which type is manually defined by the user i.e. the type was not automatically inferred.\n",
      "\ttokenized: The attribute value is obtained through tokenization.\n",
      "\thas-dict: The attribute is attached to a string dictionary e.g. a categorical attribute stored as a string.\n",
      "\tvocab-size: Number of unique values.\n",
      "\n",
      "[INFO 23-07-29 19:58:09.7013 CEST kernel.cc:810] Configure learner\n",
      "[INFO 23-07-29 19:58:09.7017 CEST kernel.cc:824] Training config:\n",
      "learner: \"RANDOM_FOREST\"\n",
      "features: \"^data:0\\\\.0$\"\n",
      "features: \"^data:0\\\\.1$\"\n",
      "features: \"^data:0\\\\.10$\"\n",
      "features: \"^data:0\\\\.100$\"\n",
      "features: \"^data:0\\\\.101$\"\n",
      "features: \"^data:0\\\\.102$\"\n",
      "features: \"^data:0\\\\.103$\"\n",
      "features: \"^data:0\\\\.104$\"\n",
      "features: \"^data:0\\\\.105$\"\n",
      "features: \"^data:0\\\\.106$\"\n",
      "features: \"^data:0\\\\.107$\"\n",
      "features: \"^data:0\\\\.108$\"\n",
      "features: \"^data:0\\\\.109$\"\n",
      "features: \"^data:0\\\\.11$\"\n",
      "features: \"^data:0\\\\.110$\"\n",
      "features: \"^data:0\\\\.111$\"\n",
      "features: \"^data:0\\\\.112$\"\n",
      "features: \"^data:0\\\\.113$\"\n",
      "features: \"^data:0\\\\.114$\"\n",
      "features: \"^data:0\\\\.115$\"\n",
      "features: \"^data:0\\\\.116$\"\n",
      "features: \"^data:0\\\\.117$\"\n",
      "features: \"^data:0\\\\.118$\"\n",
      "features: \"^data:0\\\\.119$\"\n",
      "features: \"^data:0\\\\.12$\"\n",
      "features: \"^data:0\\\\.120$\"\n",
      "features: \"^data:0\\\\.121$\"\n",
      "features: \"^data:0\\\\.122$\"\n",
      "features: \"^data:0\\\\.123$\"\n",
      "features: \"^data:0\\\\.124$\"\n",
      "features: \"^data:0\\\\.125$\"\n",
      "features: \"^data:0\\\\.126$\"\n",
      "features: \"^data:0\\\\.127$\"\n",
      "features: \"^data:0\\\\.128$\"\n",
      "features: \"^data:0\\\\.129$\"\n",
      "features: \"^data:0\\\\.13$\"\n",
      "features: \"^data:0\\\\.130$\"\n",
      "features: \"^data:0\\\\.131$\"\n",
      "features: \"^data:0\\\\.132$\"\n",
      "features: \"^data:0\\\\.133$\"\n",
      "features: \"^data:0\\\\.134$\"\n",
      "features: \"^data:0\\\\.135$\"\n",
      "features: \"^data:0\\\\.136$\"\n",
      "features: \"^data:0\\\\.137$\"\n",
      "features: \"^data:0\\\\.138$\"\n",
      "features: \"^data:0\\\\.139$\"\n",
      "features: \"^data:0\\\\.14$\"\n",
      "features: \"^data:0\\\\.140$\"\n",
      "features: \"^data:0\\\\.141$\"\n",
      "features: \"^data:0\\\\.142$\"\n",
      "features: \"^data:0\\\\.143$\"\n",
      "features: \"^data:0\\\\.144$\"\n",
      "features: \"^data:0\\\\.145$\"\n",
      "features: \"^data:0\\\\.146$\"\n",
      "features: \"^data:0\\\\.147$\"\n",
      "features: \"^data:0\\\\.148$\"\n",
      "features: \"^data:0\\\\.149$\"\n",
      "features: \"^data:0\\\\.15$\"\n",
      "features: \"^data:0\\\\.150$\"\n",
      "features: \"^data:0\\\\.151$\"\n",
      "features: \"^data:0\\\\.152$\"\n",
      "features: \"^data:0\\\\.153$\"\n",
      "features: \"^data:0\\\\.154$\"\n",
      "features: \"^data:0\\\\.155$\"\n",
      "features: \"^data:0\\\\.156$\"\n",
      "features: \"^data:0\\\\.157$\"\n",
      "features: \"^data:0\\\\.158$\"\n",
      "features: \"^data:0\\\\.159$\"\n",
      "features: \"^data:0\\\\.16$\"\n",
      "features: \"^data:0\\\\.160$\"\n",
      "features: \"^data:0\\\\.161$\"\n",
      "features: \"^data:0\\\\.162$\"\n",
      "features: \"^data:0\\\\.163$\"\n",
      "features: \"^data:0\\\\.164$\"\n",
      "features: \"^data:0\\\\.165$\"\n",
      "features: \"^data:0\\\\.166$\"\n",
      "features: \"^data:0\\\\.167$\"\n",
      "features: \"^data:0\\\\.168$\"\n",
      "features: \"^data:0\\\\.169$\"\n",
      "features: \"^data:0\\\\.17$\"\n",
      "features: \"^data:0\\\\.170$\"\n",
      "features: \"^data:0\\\\.171$\"\n",
      "features: \"^data:0\\\\.172$\"\n",
      "features: \"^data:0\\\\.173$\"\n",
      "features: \"^data:0\\\\.174$\"\n",
      "features: \"^data:0\\\\.175$\"\n",
      "features: \"^data:0\\\\.176$\"\n",
      "features: \"^data:0\\\\.177$\"\n",
      "features: \"^data:0\\\\.178$\"\n",
      "features: \"^data:0\\\\.179$\"\n",
      "features: \"^data:0\\\\.18$\"\n",
      "features: \"^data:0\\\\.180$\"\n",
      "features: \"^data:0\\\\.181$\"\n",
      "features: \"^data:0\\\\.182$\"\n",
      "features: \"^data:0\\\\.183$\"\n",
      "features: \"^data:0\\\\.184$\"\n",
      "features: \"^data:0\\\\.185$\"\n",
      "features: \"^data:0\\\\.186$\"\n",
      "features: \"^data:0\\\\.187$\"\n",
      "features: \"^data:0\\\\.188$\"\n",
      "features: \"^data:0\\\\.189$\"\n",
      "features: \"^data:0\\\\.19$\"\n",
      "features: \"^data:0\\\\.190$\"\n",
      "features: \"^data:0\\\\.191$\"\n",
      "features: \"^data:0\\\\.192$\"\n",
      "features: \"^data:0\\\\.193$\"\n",
      "features: \"^data:0\\\\.194$\"\n",
      "features: \"^data:0\\\\.195$\"\n",
      "features: \"^data:0\\\\.196$\"\n",
      "features: \"^data:0\\\\.197$\"\n",
      "features: \"^data:0\\\\.198$\"\n",
      "features: \"^data:0\\\\.199$\"\n",
      "features: \"^data:0\\\\.2$\"\n",
      "features: \"^data:0\\\\.20$\"\n",
      "features: \"^data:0\\\\.200$\"\n",
      "features: \"^data:0\\\\.201$\"\n",
      "features: \"^data:0\\\\.202$\"\n",
      "features: \"^data:0\\\\.203$\"\n",
      "features: \"^data:0\\\\.204$\"\n",
      "features: \"^data:0\\\\.205$\"\n",
      "features: \"^data:0\\\\.206$\"\n",
      "features: \"^data:0\\\\.207$\"\n",
      "features: \"^data:0\\\\.208$\"\n",
      "features: \"^data:0\\\\.209$\"\n",
      "features: \"^data:0\\\\.21$\"\n",
      "features: \"^data:0\\\\.210$\"\n",
      "features: \"^data:0\\\\.211$\"\n",
      "features: \"^data:0\\\\.212$\"\n",
      "features: \"^data:0\\\\.213$\"\n",
      "features: \"^data:0\\\\.214$\"\n",
      "features: \"^data:0\\\\.215$\"\n",
      "features: \"^data:0\\\\.216$\"\n",
      "features: \"^data:0\\\\.217$\"\n",
      "features: \"^data:0\\\\.218$\"\n",
      "features: \"^data:0\\\\.219$\"\n",
      "features: \"^data:0\\\\.22$\"\n",
      "features: \"^data:0\\\\.220$\"\n",
      "features: \"^data:0\\\\.221$\"\n",
      "features: \"^data:0\\\\.222$\"\n",
      "features: \"^data:0\\\\.223$\"\n",
      "features: \"^data:0\\\\.224$\"\n",
      "features: \"^data:0\\\\.225$\"\n",
      "features: \"^data:0\\\\.226$\"\n",
      "features: \"^data:0\\\\.227$\"\n",
      "features: \"^data:0\\\\.228$\"\n",
      "features: \"^data:0\\\\.229$\"\n",
      "features: \"^data:0\\\\.23$\"\n",
      "features: \"^data:0\\\\.230$\"\n",
      "features: \"^data:0\\\\.231$\"\n",
      "features: \"^data:0\\\\.232$\"\n",
      "features: \"^data:0\\\\.233$\"\n",
      "features: \"^data:0\\\\.234$\"\n",
      "features: \"^data:0\\\\.235$\"\n",
      "features: \"^data:0\\\\.236$\"\n",
      "features: \"^data:0\\\\.237$\"\n",
      "features: \"^data:0\\\\.238$\"\n",
      "features: \"^data:0\\\\.239$\"\n",
      "features: \"^data:0\\\\.24$\"\n",
      "features: \"^data:0\\\\.240$\"\n",
      "features: \"^data:0\\\\.241$\"\n",
      "features: \"^data:0\\\\.242$\"\n",
      "features: \"^data:0\\\\.243$\"\n",
      "features: \"^data:0\\\\.244$\"\n",
      "features: \"^data:0\\\\.245$\"\n",
      "features: \"^data:0\\\\.246$\"\n",
      "features: \"^data:0\\\\.247$\"\n",
      "features: \"^data:0\\\\.248$\"\n",
      "features: \"^data:0\\\\.249$\"\n",
      "features: \"^data:0\\\\.25$\"\n",
      "features: \"^data:0\\\\.250$\"\n",
      "features: \"^data:0\\\\.251$\"\n",
      "features: \"^data:0\\\\.252$\"\n",
      "features: \"^data:0\\\\.253$\"\n",
      "features: \"^data:0\\\\.254$\"\n",
      "features: \"^data:0\\\\.255$\"\n",
      "features: \"^data:0\\\\.256$\"\n",
      "features: \"^data:0\\\\.257$\"\n",
      "features: \"^data:0\\\\.258$\"\n",
      "features: \"^data:0\\\\.259$\"\n",
      "features: \"^data:0\\\\.26$\"\n",
      "features: \"^data:0\\\\.260$\"\n",
      "features: \"^data:0\\\\.261$\"\n",
      "features: \"^data:0\\\\.262$\"\n",
      "features: \"^data:0\\\\.263$\"\n",
      "features: \"^data:0\\\\.264$\"\n",
      "features: \"^data:0\\\\.265$\"\n",
      "features: \"^data:0\\\\.266$\"\n",
      "features: \"^data:0\\\\.267$\"\n",
      "features: \"^data:0\\\\.268$\"\n",
      "features: \"^data:0\\\\.269$\"\n",
      "features: \"^data:0\\\\.27$\"\n",
      "features: \"^data:0\\\\.270$\"\n",
      "features: \"^data:0\\\\.271$\"\n",
      "features: \"^data:0\\\\.272$\"\n",
      "features: \"^data:0\\\\.273$\"\n",
      "features: \"^data:0\\\\.274$\"\n",
      "features: \"^data:0\\\\.275$\"\n",
      "features: \"^data:0\\\\.276$\"\n",
      "features: \"^data:0\\\\.277$\"\n",
      "features: \"^data:0\\\\.278$\"\n",
      "features: \"^data:0\\\\.279$\"\n",
      "features: \"^data:0\\\\.28$\"\n",
      "features: \"^data:0\\\\.280$\"\n",
      "features: \"^data:0\\\\.281$\"\n",
      "features: \"^data:0\\\\.282$\"\n",
      "features: \"^data:0\\\\.283$\"\n",
      "features: \"^data:0\\\\.284$\"\n",
      "features: \"^data:0\\\\.285$\"\n",
      "features: \"^data:0\\\\.286$\"\n",
      "features: \"^data:0\\\\.287$\"\n",
      "features: \"^data:0\\\\.288$\"\n",
      "features: \"^data:0\\\\.289$\"\n",
      "features: \"^data:0\\\\.29$\"\n",
      "features: \"^data:0\\\\.290$\"\n",
      "features: \"^data:0\\\\.291$\"\n",
      "features: \"^data:0\\\\.292$\"\n",
      "features: \"^data:0\\\\.293$\"\n",
      "features: \"^data:0\\\\.294$\"\n",
      "features: \"^data:0\\\\.295$\"\n",
      "features: \"^data:0\\\\.296$\"\n",
      "features: \"^data:0\\\\.297$\"\n",
      "features: \"^data:0\\\\.298$\"\n",
      "features: \"^data:0\\\\.299$\"\n",
      "features: \"^data:0\\\\.3$\"\n",
      "features: \"^data:0\\\\.30$\"\n",
      "features: \"^data:0\\\\.300$\"\n",
      "features: \"^data:0\\\\.301$\"\n",
      "features: \"^data:0\\\\.302$\"\n",
      "features: \"^data:0\\\\.303$\"\n",
      "features: \"^data:0\\\\.304$\"\n",
      "features: \"^data:0\\\\.305$\"\n",
      "features: \"^data:0\\\\.306$\"\n",
      "features: \"^data:0\\\\.307$\"\n",
      "features: \"^data:0\\\\.308$\"\n",
      "features: \"^data:0\\\\.309$\"\n",
      "features: \"^data:0\\\\.31$\"\n",
      "features: \"^data:0\\\\.310$\"\n",
      "features: \"^data:0\\\\.311$\"\n",
      "features: \"^data:0\\\\.312$\"\n",
      "features: \"^data:0\\\\.313$\"\n",
      "features: \"^data:0\\\\.314$\"\n",
      "features: \"^data:0\\\\.315$\"\n",
      "features: \"^data:0\\\\.316$\"\n",
      "features: \"^data:0\\\\.317$\"\n",
      "features: \"^data:0\\\\.318$\"\n",
      "features: \"^data:0\\\\.319$\"\n",
      "features: \"^data:0\\\\.32$\"\n",
      "features: \"^data:0\\\\.320$\"\n",
      "features: \"^data:0\\\\.321$\"\n",
      "features: \"^data:0\\\\.322$\"\n",
      "features: \"^data:0\\\\.323$\"\n",
      "features: \"^data:0\\\\.324$\"\n",
      "features: \"^data:0\\\\.325$\"\n",
      "features: \"^data:0\\\\.326$\"\n",
      "features: \"^data:0\\\\.327$\"\n",
      "features: \"^data:0\\\\.328$\"\n",
      "features: \"^data:0\\\\.329$\"\n",
      "features: \"^data:0\\\\.33$\"\n",
      "features: \"^data:0\\\\.330$\"\n",
      "features: \"^data:0\\\\.331$\"\n",
      "features: \"^data:0\\\\.332$\"\n",
      "features: \"^data:0\\\\.333$\"\n",
      "features: \"^data:0\\\\.334$\"\n",
      "features: \"^data:0\\\\.335$\"\n",
      "features: \"^data:0\\\\.336$\"\n",
      "features: \"^data:0\\\\.337$\"\n",
      "features: \"^data:0\\\\.338$\"\n",
      "features: \"^data:0\\\\.339$\"\n",
      "features: \"^data:0\\\\.34$\"\n",
      "features: \"^data:0\\\\.340$\"\n",
      "features: \"^data:0\\\\.341$\"\n",
      "features: \"^data:0\\\\.342$\"\n",
      "features: \"^data:0\\\\.343$\"\n",
      "features: \"^data:0\\\\.344$\"\n",
      "features: \"^data:0\\\\.345$\"\n",
      "features: \"^data:0\\\\.346$\"\n",
      "features: \"^data:0\\\\.347$\"\n",
      "features: \"^data:0\\\\.348$\"\n",
      "features: \"^data:0\\\\.349$\"\n",
      "features: \"^data:0\\\\.35$\"\n",
      "features: \"^data:0\\\\.350$\"\n",
      "features: \"^data:0\\\\.351$\"\n",
      "features: \"^data:0\\\\.352$\"\n",
      "features: \"^data:0\\\\.353$\"\n",
      "features: \"^data:0\\\\.354$\"\n",
      "features: \"^data:0\\\\.355$\"\n",
      "features: \"^data:0\\\\.356$\"\n",
      "features: \"^data:0\\\\.357$\"\n",
      "features: \"^data:0\\\\.358$\"\n",
      "features: \"^data:0\\\\.359$\"\n",
      "features: \"^data:0\\\\.36$\"\n",
      "features: \"^data:0\\\\.360$\"\n",
      "features: \"^data:0\\\\.361$\"\n",
      "features: \"^data:0\\\\.362$\"\n",
      "features: \"^data:0\\\\.363$\"\n",
      "features: \"^data:0\\\\.364$\"\n",
      "features: \"^data:0\\\\.365$\"\n",
      "features: \"^data:0\\\\.366$\"\n",
      "features: \"^data:0\\\\.367$\"\n",
      "features: \"^data:0\\\\.368$\"\n",
      "features: \"^data:0\\\\.369$\"\n",
      "features: \"^data:0\\\\.37$\"\n",
      "features: \"^data:0\\\\.370$\"\n",
      "features: \"^data:0\\\\.371$\"\n",
      "features: \"^data:0\\\\.372$\"\n",
      "features: \"^data:0\\\\.373$\"\n",
      "features: \"^data:0\\\\.374$\"\n",
      "features: \"^data:0\\\\.375$\"\n",
      "features: \"^data:0\\\\.376$\"\n",
      "features: \"^data:0\\\\.377$\"\n",
      "features: \"^data:0\\\\.378$\"\n",
      "features: \"^data:0\\\\.379$\"\n",
      "features: \"^data:0\\\\.38$\"\n",
      "features: \"^data:0\\\\.380$\"\n",
      "features: \"^data:0\\\\.381$\"\n",
      "features: \"^data:0\\\\.382$\"\n",
      "features: \"^data:0\\\\.383$\"\n",
      "features: \"^data:0\\\\.384$\"\n",
      "features: \"^data:0\\\\.385$\"\n",
      "features: \"^data:0\\\\.386$\"\n",
      "features: \"^data:0\\\\.387$\"\n",
      "features: \"^data:0\\\\.388$\"\n",
      "features: \"^data:0\\\\.389$\"\n",
      "features: \"^data:0\\\\.39$\"\n",
      "features: \"^data:0\\\\.390$\"\n",
      "features: \"^data:0\\\\.391$\"\n",
      "features: \"^data:0\\\\.392$\"\n",
      "features: \"^data:0\\\\.393$\"\n",
      "features: \"^data:0\\\\.394$\"\n",
      "features: \"^data:0\\\\.395$\"\n",
      "features: \"^data:0\\\\.396$\"\n",
      "features: \"^data:0\\\\.397$\"\n",
      "features: \"^data:0\\\\.398$\"\n",
      "features: \"^data:0\\\\.399$\"\n",
      "features: \"^data:0\\\\.4$\"\n",
      "features: \"^data:0\\\\.40$\"\n",
      "features: \"^data:0\\\\.400$\"\n",
      "features: \"^data:0\\\\.401$\"\n",
      "features: \"^data:0\\\\.402$\"\n",
      "features: \"^data:0\\\\.403$\"\n",
      "features: \"^data:0\\\\.404$\"\n",
      "features: \"^data:0\\\\.405$\"\n",
      "features: \"^data:0\\\\.406$\"\n",
      "features: \"^data:0\\\\.407$\"\n",
      "features: \"^data:0\\\\.408$\"\n",
      "features: \"^data:0\\\\.409$\"\n",
      "features: \"^data:0\\\\.41$\"\n",
      "features: \"^data:0\\\\.410$\"\n",
      "features: \"^data:0\\\\.411$\"\n",
      "features: \"^data:0\\\\.412$\"\n",
      "features: \"^data:0\\\\.413$\"\n",
      "features: \"^data:0\\\\.414$\"\n",
      "features: \"^data:0\\\\.415$\"\n",
      "features: \"^data:0\\\\.416$\"\n",
      "features: \"^data:0\\\\.417$\"\n",
      "features: \"^data:0\\\\.418$\"\n",
      "features: \"^data:0\\\\.419$\"\n",
      "features: \"^data:0\\\\.42$\"\n",
      "features: \"^data:0\\\\.420$\"\n",
      "features: \"^data:0\\\\.421$\"\n",
      "features: \"^data:0\\\\.422$\"\n",
      "features: \"^data:0\\\\.423$\"\n",
      "features: \"^data:0\\\\.424$\"\n",
      "features: \"^data:0\\\\.425$\"\n",
      "features: \"^data:0\\\\.426$\"\n",
      "features: \"^data:0\\\\.427$\"\n",
      "features: \"^data:0\\\\.428$\"\n",
      "features: \"^data:0\\\\.429$\"\n",
      "features: \"^data:0\\\\.43$\"\n",
      "features: \"^data:0\\\\.430$\"\n",
      "features: \"^data:0\\\\.431$\"\n",
      "features: \"^data:0\\\\.432$\"\n",
      "features: \"^data:0\\\\.433$\"\n",
      "features: \"^data:0\\\\.434$\"\n",
      "features: \"^data:0\\\\.435$\"\n",
      "features: \"^data:0\\\\.436$\"\n",
      "features: \"^data:0\\\\.437$\"\n",
      "features: \"^data:0\\\\.438$\"\n",
      "features: \"^data:0\\\\.439$\"\n",
      "features: \"^data:0\\\\.44$\"\n",
      "features: \"^data:0\\\\.440$\"\n",
      "features: \"^data:0\\\\.441$\"\n",
      "features: \"^data:0\\\\.442$\"\n",
      "features: \"^data:0\\\\.443$\"\n",
      "features: \"^data:0\\\\.444$\"\n",
      "features: \"^data:0\\\\.445$\"\n",
      "features: \"^data:0\\\\.446$\"\n",
      "features: \"^data:0\\\\.447$\"\n",
      "features: \"^data:0\\\\.448$\"\n",
      "features: \"^data:0\\\\.449$\"\n",
      "features: \"^data:0\\\\.45$\"\n",
      "features: \"^data:0\\\\.450$\"\n",
      "features: \"^data:0\\\\.451$\"\n",
      "features: \"^data:0\\\\.452$\"\n",
      "features: \"^data:0\\\\.453$\"\n",
      "features: \"^data:0\\\\.454$\"\n",
      "features: \"^data:0\\\\.455$\"\n",
      "features: \"^data:0\\\\.456$\"\n",
      "features: \"^data:0\\\\.457$\"\n",
      "features: \"^data:0\\\\.458$\"\n",
      "features: \"^data:0\\\\.459$\"\n",
      "features: \"^data:0\\\\.46$\"\n",
      "features: \"^data:0\\\\.460$\"\n",
      "features: \"^data:0\\\\.461$\"\n",
      "features: \"^data:0\\\\.462$\"\n",
      "features: \"^data:0\\\\.463$\"\n",
      "features: \"^data:0\\\\.464$\"\n",
      "features: \"^data:0\\\\.465$\"\n",
      "features: \"^data:0\\\\.466$\"\n",
      "features: \"^data:0\\\\.467$\"\n",
      "features: \"^data:0\\\\.468$\"\n",
      "features: \"^data:0\\\\.469$\"\n",
      "features: \"^data:0\\\\.47$\"\n",
      "features: \"^data:0\\\\.470$\"\n",
      "features: \"^data:0\\\\.471$\"\n",
      "features: \"^data:0\\\\.472$\"\n",
      "features: \"^data:0\\\\.473$\"\n",
      "features: \"^data:0\\\\.474$\"\n",
      "features: \"^data:0\\\\.475$\"\n",
      "features: \"^data:0\\\\.476$\"\n",
      "features: \"^data:0\\\\.477$\"\n",
      "features: \"^data:0\\\\.478$\"\n",
      "features: \"^data:0\\\\.479$\"\n",
      "features: \"^data:0\\\\.48$\"\n",
      "features: \"^data:0\\\\.480$\"\n",
      "features: \"^data:0\\\\.481$\"\n",
      "features: \"^data:0\\\\.482$\"\n",
      "features: \"^data:0\\\\.483$\"\n",
      "features: \"^data:0\\\\.484$\"\n",
      "features: \"^data:0\\\\.485$\"\n",
      "features: \"^data:0\\\\.486$\"\n",
      "features: \"^data:0\\\\.487$\"\n",
      "features: \"^data:0\\\\.488$\"\n",
      "features: \"^data:0\\\\.489$\"\n",
      "features: \"^data:0\\\\.49$\"\n",
      "features: \"^data:0\\\\.490$\"\n",
      "features: \"^data:0\\\\.491$\"\n",
      "features: \"^data:0\\\\.492$\"\n",
      "features: \"^data:0\\\\.493$\"\n",
      "features: \"^data:0\\\\.494$\"\n",
      "features: \"^data:0\\\\.495$\"\n",
      "features: \"^data:0\\\\.496$\"\n",
      "features: \"^data:0\\\\.497$\"\n",
      "features: \"^data:0\\\\.498$\"\n",
      "features: \"^data:0\\\\.499$\"\n",
      "features: \"^data:0\\\\.5$\"\n",
      "features: \"^data:0\\\\.50$\"\n",
      "features: \"^data:0\\\\.500$\"\n",
      "features: \"^data:0\\\\.501$\"\n",
      "features: \"^data:0\\\\.502$\"\n",
      "features: \"^data:0\\\\.503$\"\n",
      "features: \"^data:0\\\\.504$\"\n",
      "features: \"^data:0\\\\.505$\"\n",
      "features: \"^data:0\\\\.506$\"\n",
      "features: \"^data:0\\\\.507$\"\n",
      "features: \"^data:0\\\\.508$\"\n",
      "features: \"^data:0\\\\.509$\"\n",
      "features: \"^data:0\\\\.51$\"\n",
      "features: \"^data:0\\\\.510$\"\n",
      "features: \"^data:0\\\\.511$\"\n",
      "features: \"^data:0\\\\.512$\"\n",
      "features: \"^data:0\\\\.513$\"\n",
      "features: \"^data:0\\\\.514$\"\n",
      "features: \"^data:0\\\\.515$\"\n",
      "features: \"^data:0\\\\.516$\"\n",
      "features: \"^data:0\\\\.517$\"\n",
      "features: \"^data:0\\\\.518$\"\n",
      "features: \"^data:0\\\\.519$\"\n",
      "features: \"^data:0\\\\.52$\"\n",
      "features: \"^data:0\\\\.520$\"\n",
      "features: \"^data:0\\\\.521$\"\n",
      "features: \"^data:0\\\\.522$\"\n",
      "features: \"^data:0\\\\.523$\"\n",
      "features: \"^data:0\\\\.524$\"\n",
      "features: \"^data:0\\\\.525$\"\n",
      "features: \"^data:0\\\\.526$\"\n",
      "features: \"^data:0\\\\.527$\"\n",
      "features: \"^data:0\\\\.528$\"\n",
      "features: \"^data:0\\\\.529$\"\n",
      "features: \"^data:0\\\\.53$\"\n",
      "features: \"^data:0\\\\.530$\"\n",
      "features: \"^data:0\\\\.531$\"\n",
      "features: \"^data:0\\\\.532$\"\n",
      "features: \"^data:0\\\\.533$\"\n",
      "features: \"^data:0\\\\.534$\"\n",
      "features: \"^data:0\\\\.535$\"\n",
      "features: \"^data:0\\\\.536$\"\n",
      "features: \"^data:0\\\\.537$\"\n",
      "features: \"^data:0\\\\.538$\"\n",
      "features: \"^data:0\\\\.539$\"\n",
      "features: \"^data:0\\\\.54$\"\n",
      "features: \"^data:0\\\\.540$\"\n",
      "features: \"^data:0\\\\.541$\"\n",
      "features: \"^data:0\\\\.542$\"\n",
      "features: \"^data:0\\\\.543$\"\n",
      "features: \"^data:0\\\\.544$\"\n",
      "features: \"^data:0\\\\.545$\"\n",
      "features: \"^data:0\\\\.546$\"\n",
      "features: \"^data:0\\\\.547$\"\n",
      "features: \"^data:0\\\\.548$\"\n",
      "features: \"^data:0\\\\.549$\"\n",
      "features: \"^data:0\\\\.55$\"\n",
      "features: \"^data:0\\\\.550$\"\n",
      "features: \"^data:0\\\\.551$\"\n",
      "features: \"^data:0\\\\.552$\"\n",
      "features: \"^data:0\\\\.553$\"\n",
      "features: \"^data:0\\\\.554$\"\n",
      "features: \"^data:0\\\\.555$\"\n",
      "features: \"^data:0\\\\.556$\"\n",
      "features: \"^data:0\\\\.557$\"\n",
      "features: \"^data:0\\\\.558$\"\n",
      "features: \"^data:0\\\\.559$\"\n",
      "features: \"^data:0\\\\.56$\"\n",
      "features: \"^data:0\\\\.560$\"\n",
      "features: \"^data:0\\\\.561$\"\n",
      "features: \"^data:0\\\\.562$\"\n",
      "features: \"^data:0\\\\.563$\"\n",
      "features: \"^data:0\\\\.564$\"\n",
      "features: \"^data:0\\\\.565$\"\n",
      "features: \"^data:0\\\\.566$\"\n",
      "features: \"^data:0\\\\.567$\"\n",
      "features: \"^data:0\\\\.568$\"\n",
      "features: \"^data:0\\\\.569$\"\n",
      "features: \"^data:0\\\\.57$\"\n",
      "features: \"^data:0\\\\.570$\"\n",
      "features: \"^data:0\\\\.571$\"\n",
      "features: \"^data:0\\\\.572$\"\n",
      "features: \"^data:0\\\\.573$\"\n",
      "features: \"^data:0\\\\.574$\"\n",
      "features: \"^data:0\\\\.575$\"\n",
      "features: \"^data:0\\\\.576$\"\n",
      "features: \"^data:0\\\\.577$\"\n",
      "features: \"^data:0\\\\.578$\"\n",
      "features: \"^data:0\\\\.579$\"\n",
      "features: \"^data:0\\\\.58$\"\n",
      "features: \"^data:0\\\\.580$\"\n",
      "features: \"^data:0\\\\.581$\"\n",
      "features: \"^data:0\\\\.582$\"\n",
      "features: \"^data:0\\\\.583$\"\n",
      "features: \"^data:0\\\\.584$\"\n",
      "features: \"^data:0\\\\.585$\"\n",
      "features: \"^data:0\\\\.586$\"\n",
      "features: \"^data:0\\\\.587$\"\n",
      "features: \"^data:0\\\\.588$\"\n",
      "features: \"^data:0\\\\.589$\"\n",
      "features: \"^data:0\\\\.59$\"\n",
      "features: \"^data:0\\\\.590$\"\n",
      "features: \"^data:0\\\\.591$\"\n",
      "features: \"^data:0\\\\.592$\"\n",
      "features: \"^data:0\\\\.593$\"\n",
      "features: \"^data:0\\\\.594$\"\n",
      "features: \"^data:0\\\\.595$\"\n",
      "features: \"^data:0\\\\.596$\"\n",
      "features: \"^data:0\\\\.597$\"\n",
      "features: \"^data:0\\\\.598$\"\n",
      "features: \"^data:0\\\\.599$\"\n",
      "features: \"^data:0\\\\.6$\"\n",
      "features: \"^data:0\\\\.60$\"\n",
      "features: \"^data:0\\\\.600$\"\n",
      "features: \"^data:0\\\\.601$\"\n",
      "features: \"^data:0\\\\.602$\"\n",
      "features: \"^data:0\\\\.603$\"\n",
      "features: \"^data:0\\\\.604$\"\n",
      "features: \"^data:0\\\\.605$\"\n",
      "features: \"^data:0\\\\.606$\"\n",
      "features: \"^data:0\\\\.607$\"\n",
      "features: \"^data:0\\\\.608$\"\n",
      "features: \"^data:0\\\\.609$\"\n",
      "features: \"^data:0\\\\.61$\"\n",
      "features: \"^data:0\\\\.610$\"\n",
      "features: \"^data:0\\\\.611$\"\n",
      "features: \"^data:0\\\\.612$\"\n",
      "features: \"^data:0\\\\.613$\"\n",
      "features: \"^data:0\\\\.614$\"\n",
      "features: \"^data:0\\\\.615$\"\n",
      "features: \"^data:0\\\\.616$\"\n",
      "features: \"^data:0\\\\.617$\"\n",
      "features: \"^data:0\\\\.618$\"\n",
      "features: \"^data:0\\\\.619$\"\n",
      "features: \"^data:0\\\\.62$\"\n",
      "features: \"^data:0\\\\.620$\"\n",
      "features: \"^data:0\\\\.621$\"\n",
      "features: \"^data:0\\\\.622$\"\n",
      "features: \"^data:0\\\\.623$\"\n",
      "features: \"^data:0\\\\.624$\"\n",
      "features: \"^data:0\\\\.625$\"\n",
      "features: \"^data:0\\\\.626$\"\n",
      "features: \"^data:0\\\\.627$\"\n",
      "features: \"^data:0\\\\.628$\"\n",
      "features: \"^data:0\\\\.629$\"\n",
      "features: \"^data:0\\\\.63$\"\n",
      "features: \"^data:0\\\\.630$\"\n",
      "features: \"^data:0\\\\.631$\"\n",
      "features: \"^data:0\\\\.632$\"\n",
      "features: \"^data:0\\\\.633$\"\n",
      "features: \"^data:0\\\\.634$\"\n",
      "features: \"^data:0\\\\.635$\"\n",
      "features: \"^data:0\\\\.636$\"\n",
      "features: \"^data:0\\\\.637$\"\n",
      "features: \"^data:0\\\\.638$\"\n",
      "features: \"^data:0\\\\.639$\"\n",
      "features: \"^data:0\\\\.64$\"\n",
      "features: \"^data:0\\\\.640$\"\n",
      "features: \"^data:0\\\\.641$\"\n",
      "features: \"^data:0\\\\.642$\"\n",
      "features: \"^data:0\\\\.643$\"\n",
      "features: \"^data:0\\\\.644$\"\n",
      "features: \"^data:0\\\\.645$\"\n",
      "features: \"^data:0\\\\.646$\"\n",
      "features: \"^data:0\\\\.647$\"\n",
      "features: \"^data:0\\\\.648$\"\n",
      "features: \"^data:0\\\\.649$\"\n",
      "features: \"^data:0\\\\.65$\"\n",
      "features: \"^data:0\\\\.650$\"\n",
      "features: \"^data:0\\\\.651$\"\n",
      "features: \"^data:0\\\\.652$\"\n",
      "features: \"^data:0\\\\.653$\"\n",
      "features: \"^data:0\\\\.654$\"\n",
      "features: \"^data:0\\\\.655$\"\n",
      "features: \"^data:0\\\\.656$\"\n",
      "features: \"^data:0\\\\.657$\"\n",
      "features: \"^data:0\\\\.658$\"\n",
      "features: \"^data:0\\\\.659$\"\n",
      "features: \"^data:0\\\\.66$\"\n",
      "features: \"^data:0\\\\.660$\"\n",
      "features: \"^data:0\\\\.661$\"\n",
      "features: \"^data:0\\\\.662$\"\n",
      "features: \"^data:0\\\\.663$\"\n",
      "features: \"^data:0\\\\.664$\"\n",
      "features: \"^data:0\\\\.665$\"\n",
      "features: \"^data:0\\\\.666$\"\n",
      "features: \"^data:0\\\\.667$\"\n",
      "features: \"^data:0\\\\.668$\"\n",
      "features: \"^data:0\\\\.669$\"\n",
      "features: \"^data:0\\\\.67$\"\n",
      "features: \"^data:0\\\\.670$\"\n",
      "features: \"^data:0\\\\.671$\"\n",
      "features: \"^data:0\\\\.672$\"\n",
      "features: \"^data:0\\\\.673$\"\n",
      "features: \"^data:0\\\\.674$\"\n",
      "features: \"^data:0\\\\.675$\"\n",
      "features: \"^data:0\\\\.676$\"\n",
      "features: \"^data:0\\\\.677$\"\n",
      "features: \"^data:0\\\\.678$\"\n",
      "features: \"^data:0\\\\.679$\"\n",
      "features: \"^data:0\\\\.68$\"\n",
      "features: \"^data:0\\\\.680$\"\n",
      "features: \"^data:0\\\\.681$\"\n",
      "features: \"^data:0\\\\.682$\"\n",
      "features: \"^data:0\\\\.683$\"\n",
      "features: \"^data:0\\\\.684$\"\n",
      "features: \"^data:0\\\\.685$\"\n",
      "features: \"^data:0\\\\.686$\"\n",
      "features: \"^data:0\\\\.687$\"\n",
      "features: \"^data:0\\\\.688$\"\n",
      "features: \"^data:0\\\\.689$\"\n",
      "features: \"^data:0\\\\.69$\"\n",
      "features: \"^data:0\\\\.690$\"\n",
      "features: \"^data:0\\\\.691$\"\n",
      "features: \"^data:0\\\\.692$\"\n",
      "features: \"^data:0\\\\.693$\"\n",
      "features: \"^data:0\\\\.694$\"\n",
      "features: \"^data:0\\\\.695$\"\n",
      "features: \"^data:0\\\\.696$\"\n",
      "features: \"^data:0\\\\.697$\"\n",
      "features: \"^data:0\\\\.698$\"\n",
      "features: \"^data:0\\\\.699$\"\n",
      "features: \"^data:0\\\\.7$\"\n",
      "features: \"^data:0\\\\.70$\"\n",
      "features: \"^data:0\\\\.700$\"\n",
      "features: \"^data:0\\\\.701$\"\n",
      "features: \"^data:0\\\\.702$\"\n",
      "features: \"^data:0\\\\.703$\"\n",
      "features: \"^data:0\\\\.704$\"\n",
      "features: \"^data:0\\\\.705$\"\n",
      "features: \"^data:0\\\\.706$\"\n",
      "features: \"^data:0\\\\.707$\"\n",
      "features: \"^data:0\\\\.708$\"\n",
      "features: \"^data:0\\\\.709$\"\n",
      "features: \"^data:0\\\\.71$\"\n",
      "features: \"^data:0\\\\.710$\"\n",
      "features: \"^data:0\\\\.711$\"\n",
      "features: \"^data:0\\\\.712$\"\n",
      "features: \"^data:0\\\\.713$\"\n",
      "features: \"^data:0\\\\.714$\"\n",
      "features: \"^data:0\\\\.715$\"\n",
      "features: \"^data:0\\\\.716$\"\n",
      "features: \"^data:0\\\\.717$\"\n",
      "features: \"^data:0\\\\.718$\"\n",
      "features: \"^data:0\\\\.719$\"\n",
      "features: \"^data:0\\\\.72$\"\n",
      "features: \"^data:0\\\\.720$\"\n",
      "features: \"^data:0\\\\.721$\"\n",
      "features: \"^data:0\\\\.722$\"\n",
      "features: \"^data:0\\\\.723$\"\n",
      "features: \"^data:0\\\\.724$\"\n",
      "features: \"^data:0\\\\.725$\"\n",
      "features: \"^data:0\\\\.726$\"\n",
      "features: \"^data:0\\\\.727$\"\n",
      "features: \"^data:0\\\\.728$\"\n",
      "features: \"^data:0\\\\.729$\"\n",
      "features: \"^data:0\\\\.73$\"\n",
      "features: \"^data:0\\\\.730$\"\n",
      "features: \"^data:0\\\\.731$\"\n",
      "features: \"^data:0\\\\.732$\"\n",
      "features: \"^data:0\\\\.733$\"\n",
      "features: \"^data:0\\\\.734$\"\n",
      "features: \"^data:0\\\\.735$\"\n",
      "features: \"^data:0\\\\.736$\"\n",
      "features: \"^data:0\\\\.737$\"\n",
      "features: \"^data:0\\\\.738$\"\n",
      "features: \"^data:0\\\\.739$\"\n",
      "features: \"^data:0\\\\.74$\"\n",
      "features: \"^data:0\\\\.740$\"\n",
      "features: \"^data:0\\\\.741$\"\n",
      "features: \"^data:0\\\\.742$\"\n",
      "features: \"^data:0\\\\.743$\"\n",
      "features: \"^data:0\\\\.744$\"\n",
      "features: \"^data:0\\\\.745$\"\n",
      "features: \"^data:0\\\\.746$\"\n",
      "features: \"^data:0\\\\.747$\"\n",
      "features: \"^data:0\\\\.748$\"\n",
      "features: \"^data:0\\\\.749$\"\n",
      "features: \"^data:0\\\\.75$\"\n",
      "features: \"^data:0\\\\.750$\"\n",
      "features: \"^data:0\\\\.751$\"\n",
      "features: \"^data:0\\\\.752$\"\n",
      "features: \"^data:0\\\\.753$\"\n",
      "features: \"^data:0\\\\.754$\"\n",
      "features: \"^data:0\\\\.755$\"\n",
      "features: \"^data:0\\\\.756$\"\n",
      "features: \"^data:0\\\\.757$\"\n",
      "features: \"^data:0\\\\.758$\"\n",
      "features: \"^data:0\\\\.759$\"\n",
      "features: \"^data:0\\\\.76$\"\n",
      "features: \"^data:0\\\\.760$\"\n",
      "features: \"^data:0\\\\.761$\"\n",
      "features: \"^data:0\\\\.762$\"\n",
      "features: \"^data:0\\\\.763$\"\n",
      "features: \"^data:0\\\\.764$\"\n",
      "features: \"^data:0\\\\.765$\"\n",
      "features: \"^data:0\\\\.766$\"\n",
      "features: \"^data:0\\\\.767$\"\n",
      "features: \"^data:0\\\\.77$\"\n",
      "features: \"^data:0\\\\.78$\"\n",
      "features: \"^data:0\\\\.79$\"\n",
      "features: \"^data:0\\\\.8$\"\n",
      "features: \"^data:0\\\\.80$\"\n",
      "features: \"^data:0\\\\.81$\"\n",
      "features: \"^data:0\\\\.82$\"\n",
      "features: \"^data:0\\\\.83$\"\n",
      "features: \"^data:0\\\\.84$\"\n",
      "features: \"^data:0\\\\.85$\"\n",
      "features: \"^data:0\\\\.86$\"\n",
      "features: \"^data:0\\\\.87$\"\n",
      "features: \"^data:0\\\\.88$\"\n",
      "features: \"^data:0\\\\.89$\"\n",
      "features: \"^data:0\\\\.9$\"\n",
      "features: \"^data:0\\\\.90$\"\n",
      "features: \"^data:0\\\\.91$\"\n",
      "features: \"^data:0\\\\.92$\"\n",
      "features: \"^data:0\\\\.93$\"\n",
      "features: \"^data:0\\\\.94$\"\n",
      "features: \"^data:0\\\\.95$\"\n",
      "features: \"^data:0\\\\.96$\"\n",
      "features: \"^data:0\\\\.97$\"\n",
      "features: \"^data:0\\\\.98$\"\n",
      "features: \"^data:0\\\\.99$\"\n",
      "label: \"^__LABEL$\"\n",
      "task: CLASSIFICATION\n",
      "random_seed: 123456\n",
      "metadata {\n",
      "  framework: \"TF Keras\"\n",
      "}\n",
      "pure_serving_model: false\n",
      "[yggdrasil_decision_forests.model.random_forest.proto.random_forest_config] {\n",
      "  num_trees: 1000\n",
      "  decision_tree {\n",
      "    max_depth: 16\n",
      "    min_examples: 5\n",
      "    in_split_min_examples_check: true\n",
      "    keep_non_leaf_label_distribution: true\n",
      "    num_candidate_attributes: 0\n",
      "    missing_value_policy: GLOBAL_IMPUTATION\n",
      "    allow_na_conditions: false\n",
      "    categorical_set_greedy_forward {\n",
      "      sampling: 0.1\n",
      "      max_num_items: -1\n",
      "      min_item_frequency: 1\n",
      "    }\n",
      "    growing_strategy_local {\n",
      "    }\n",
      "    categorical {\n",
      "      cart {\n",
      "      }\n",
      "    }\n",
      "    axis_aligned_split {\n",
      "    }\n",
      "    internal {\n",
      "      sorting_strategy: PRESORTED\n",
      "    }\n",
      "    uplift {\n",
      "      min_examples_in_treatment: 5\n",
      "      split_score: KULLBACK_LEIBLER\n",
      "    }\n",
      "  }\n",
      "  winner_take_all_inference: true\n",
      "  compute_oob_performances: true\n",
      "  compute_oob_variable_importances: false\n",
      "  num_oob_variable_importances_permutations: 1\n",
      "  bootstrap_training_dataset: true\n",
      "  bootstrap_size_ratio: 1\n",
      "  adapt_bootstrap_size_ratio_for_maximum_training_duration: false\n",
      "  sampling_with_replacement: true\n",
      "}\n",
      "\n",
      "[INFO 23-07-29 19:58:09.7020 CEST kernel.cc:827] Deployment config:\n",
      "cache_path: \"/tmp/tmpqx6vxa1k/working_cache\"\n",
      "num_threads: 3\n",
      "try_resume_training: true\n",
      "\n",
      "[INFO 23-07-29 19:58:09.7022 CEST kernel.cc:889] Train model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 23-07-29 19:58:09.7489 CEST random_forest.cc:416] Training random forest on 22996 example(s) and 768 feature(s).\n",
      "[INFO 23-07-29 19:58:10.5784 CEST random_forest.cc:802] Training of tree  1/1000 (tree index:1) done accuracy:0.77013 logloss:8.28536\n",
      "[INFO 23-07-29 19:58:12.0560 CEST random_forest.cc:802] Training of tree  11/1000 (tree index:10) done accuracy:0.813373 logloss:1.9721\n",
      "[INFO 23-07-29 19:58:13.5863 CEST random_forest.cc:802] Training of tree  21/1000 (tree index:20) done accuracy:0.840082 logloss:0.714204\n",
      "[INFO 23-07-29 19:58:15.5199 CEST random_forest.cc:802] Training of tree  31/1000 (tree index:30) done accuracy:0.851626 logloss:0.487727\n",
      "[INFO 23-07-29 19:58:17.2217 CEST random_forest.cc:802] Training of tree  41/1000 (tree index:41) done accuracy:0.854062 logloss:0.415403\n",
      "[INFO 23-07-29 19:58:18.7162 CEST random_forest.cc:802] Training of tree  51/1000 (tree index:50) done accuracy:0.858628 logloss:0.374141\n",
      "[INFO 23-07-29 19:58:20.5330 CEST random_forest.cc:802] Training of tree  61/1000 (tree index:60) done accuracy:0.862628 logloss:0.351298\n",
      "[INFO 23-07-29 19:58:22.1025 CEST random_forest.cc:802] Training of tree  71/1000 (tree index:70) done accuracy:0.864324 logloss:0.341204\n",
      "[INFO 23-07-29 19:58:23.5780 CEST random_forest.cc:802] Training of tree  81/1000 (tree index:80) done accuracy:0.86502 logloss:0.337717\n",
      "[INFO 23-07-29 19:58:25.4154 CEST random_forest.cc:802] Training of tree  91/1000 (tree index:90) done accuracy:0.866585 logloss:0.335603\n",
      "[INFO 23-07-29 19:58:26.9492 CEST random_forest.cc:802] Training of tree  101/1000 (tree index:100) done accuracy:0.866846 logloss:0.334641\n",
      "[INFO 23-07-29 19:58:28.4281 CEST random_forest.cc:802] Training of tree  111/1000 (tree index:110) done accuracy:0.866803 logloss:0.332626\n",
      "[INFO 23-07-29 19:58:30.3023 CEST random_forest.cc:802] Training of tree  121/1000 (tree index:120) done accuracy:0.868542 logloss:0.330177\n",
      "[INFO 23-07-29 19:58:31.7367 CEST random_forest.cc:802] Training of tree  131/1000 (tree index:129) done accuracy:0.867934 logloss:0.328528\n",
      "[INFO 23-07-29 19:58:33.3067 CEST random_forest.cc:802] Training of tree  141/1000 (tree index:140) done accuracy:0.868803 logloss:0.325381\n",
      "[INFO 23-07-29 19:58:35.1718 CEST random_forest.cc:802] Training of tree  151/1000 (tree index:150) done accuracy:0.869064 logloss:0.323834\n",
      "[INFO 23-07-29 19:58:36.6955 CEST random_forest.cc:802] Training of tree  161/1000 (tree index:160) done accuracy:0.870238 logloss:0.323597\n",
      "[INFO 23-07-29 19:58:38.3388 CEST random_forest.cc:802] Training of tree  171/1000 (tree index:170) done accuracy:0.870847 logloss:0.321946\n",
      "[INFO 23-07-29 19:58:40.1318 CEST random_forest.cc:802] Training of tree  181/1000 (tree index:180) done accuracy:0.871108 logloss:0.321357\n",
      "[INFO 23-07-29 19:58:41.6028 CEST random_forest.cc:802] Training of tree  191/1000 (tree index:190) done accuracy:0.871412 logloss:0.321228\n",
      "[INFO 23-07-29 19:58:43.3345 CEST random_forest.cc:802] Training of tree  201/1000 (tree index:200) done accuracy:0.872152 logloss:0.320949\n",
      "[INFO 23-07-29 19:58:44.9487 CEST random_forest.cc:802] Training of tree  211/1000 (tree index:210) done accuracy:0.872456 logloss:0.319482\n",
      "[INFO 23-07-29 19:58:46.4678 CEST random_forest.cc:802] Training of tree  221/1000 (tree index:220) done accuracy:0.872369 logloss:0.319479\n",
      "[INFO 23-07-29 19:58:48.1870 CEST random_forest.cc:802] Training of tree  231/1000 (tree index:230) done accuracy:0.872978 logloss:0.319331\n",
      "[INFO 23-07-29 19:58:49.7330 CEST random_forest.cc:802] Training of tree  241/1000 (tree index:240) done accuracy:0.873761 logloss:0.319003\n",
      "[INFO 23-07-29 19:58:51.2487 CEST random_forest.cc:802] Training of tree  251/1000 (tree index:250) done accuracy:0.873369 logloss:0.31886\n",
      "[INFO 23-07-29 19:58:52.9973 CEST random_forest.cc:802] Training of tree  261/1000 (tree index:260) done accuracy:0.873761 logloss:0.318714\n",
      "[INFO 23-07-29 19:58:54.5258 CEST random_forest.cc:802] Training of tree  271/1000 (tree index:270) done accuracy:0.873891 logloss:0.318464\n",
      "[INFO 23-07-29 19:58:56.0418 CEST random_forest.cc:802] Training of tree  281/1000 (tree index:280) done accuracy:0.873413 logloss:0.318485\n",
      "[INFO 23-07-29 19:58:57.8636 CEST random_forest.cc:802] Training of tree  291/1000 (tree index:290) done accuracy:0.873891 logloss:0.318461\n",
      "[INFO 23-07-29 19:58:59.3378 CEST random_forest.cc:802] Training of tree  301/1000 (tree index:300) done accuracy:0.874109 logloss:0.31862\n",
      "[INFO 23-07-29 19:59:00.7993 CEST random_forest.cc:802] Training of tree  311/1000 (tree index:309) done accuracy:0.874326 logloss:0.318523\n",
      "[INFO 23-07-29 19:59:02.6690 CEST random_forest.cc:802] Training of tree  321/1000 (tree index:320) done accuracy:0.874109 logloss:0.318405\n",
      "[INFO 23-07-29 19:59:04.1255 CEST random_forest.cc:802] Training of tree  331/1000 (tree index:330) done accuracy:0.873891 logloss:0.318468\n",
      "[INFO 23-07-29 19:59:05.6698 CEST random_forest.cc:802] Training of tree  341/1000 (tree index:340) done accuracy:0.873674 logloss:0.318412\n",
      "[INFO 23-07-29 19:59:07.4706 CEST random_forest.cc:802] Training of tree  351/1000 (tree index:350) done accuracy:0.873717 logloss:0.318315\n",
      "[INFO 23-07-29 19:59:09.0113 CEST random_forest.cc:802] Training of tree  361/1000 (tree index:360) done accuracy:0.873195 logloss:0.318447\n",
      "[INFO 23-07-29 19:59:10.6465 CEST random_forest.cc:802] Training of tree  371/1000 (tree index:370) done accuracy:0.874109 logloss:0.31853\n",
      "[INFO 23-07-29 19:59:12.5534 CEST random_forest.cc:802] Training of tree  381/1000 (tree index:380) done accuracy:0.873804 logloss:0.318509\n",
      "[INFO 23-07-29 19:59:14.1441 CEST random_forest.cc:802] Training of tree  391/1000 (tree index:390) done accuracy:0.874587 logloss:0.318383\n",
      "[INFO 23-07-29 19:59:15.8334 CEST random_forest.cc:802] Training of tree  401/1000 (tree index:400) done accuracy:0.874804 logloss:0.318457\n",
      "[INFO 23-07-29 19:59:17.6550 CEST random_forest.cc:802] Training of tree  411/1000 (tree index:410) done accuracy:0.875109 logloss:0.318301\n",
      "[INFO 23-07-29 19:59:19.1820 CEST random_forest.cc:802] Training of tree  421/1000 (tree index:420) done accuracy:0.874848 logloss:0.318232\n",
      "[INFO 23-07-29 19:59:20.8269 CEST random_forest.cc:802] Training of tree  431/1000 (tree index:430) done accuracy:0.875065 logloss:0.318281\n",
      "[INFO 23-07-29 19:59:22.5609 CEST random_forest.cc:802] Training of tree  441/1000 (tree index:440) done accuracy:0.874282 logloss:0.318324\n",
      "[INFO 23-07-29 19:59:24.0702 CEST random_forest.cc:802] Training of tree  451/1000 (tree index:449) done accuracy:0.873978 logloss:0.318391\n",
      "[INFO 23-07-29 19:59:25.6910 CEST random_forest.cc:802] Training of tree  461/1000 (tree index:460) done accuracy:0.874761 logloss:0.316951\n",
      "[INFO 23-07-29 19:59:27.4591 CEST random_forest.cc:802] Training of tree  471/1000 (tree index:470) done accuracy:0.874848 logloss:0.316822\n",
      "[INFO 23-07-29 19:59:28.9280 CEST random_forest.cc:802] Training of tree  481/1000 (tree index:480) done accuracy:0.875196 logloss:0.316631\n",
      "[INFO 23-07-29 19:59:30.5555 CEST random_forest.cc:802] Training of tree  491/1000 (tree index:490) done accuracy:0.875587 logloss:0.316675\n",
      "[INFO 23-07-29 19:59:32.3647 CEST random_forest.cc:802] Training of tree  501/1000 (tree index:500) done accuracy:0.8755 logloss:0.316631\n",
      "[INFO 23-07-29 19:59:33.8455 CEST random_forest.cc:802] Training of tree  511/1000 (tree index:510) done accuracy:0.875239 logloss:0.316569\n",
      "[INFO 23-07-29 19:59:35.4432 CEST random_forest.cc:802] Training of tree  521/1000 (tree index:520) done accuracy:0.875239 logloss:0.316545\n",
      "[INFO 23-07-29 19:59:37.1775 CEST random_forest.cc:802] Training of tree  531/1000 (tree index:531) done accuracy:0.875239 logloss:0.316509\n",
      "[INFO 23-07-29 19:59:38.6102 CEST random_forest.cc:802] Training of tree  541/1000 (tree index:539) done accuracy:0.875283 logloss:0.316518\n",
      "[INFO 23-07-29 19:59:40.3010 CEST random_forest.cc:802] Training of tree  551/1000 (tree index:550) done accuracy:0.875413 logloss:0.316507\n",
      "[INFO 23-07-29 19:59:42.0405 CEST random_forest.cc:802] Training of tree  561/1000 (tree index:560) done accuracy:0.875413 logloss:0.316462\n",
      "[INFO 23-07-29 19:59:43.5554 CEST random_forest.cc:802] Training of tree  571/1000 (tree index:570) done accuracy:0.875587 logloss:0.316516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 23-07-29 19:59:45.2304 CEST random_forest.cc:802] Training of tree  581/1000 (tree index:580) done accuracy:0.875674 logloss:0.316409\n",
      "[INFO 23-07-29 19:59:46.8386 CEST random_forest.cc:802] Training of tree  591/1000 (tree index:590) done accuracy:0.875804 logloss:0.316367\n",
      "[INFO 23-07-29 19:59:48.3474 CEST random_forest.cc:802] Training of tree  601/1000 (tree index:600) done accuracy:0.876196 logloss:0.316349\n",
      "[INFO 23-07-29 19:59:50.0171 CEST random_forest.cc:802] Training of tree  611/1000 (tree index:610) done accuracy:0.876109 logloss:0.316353\n",
      "[INFO 23-07-29 19:59:51.6586 CEST random_forest.cc:802] Training of tree  621/1000 (tree index:620) done accuracy:0.875891 logloss:0.316302\n",
      "[INFO 23-07-29 19:59:53.1451 CEST random_forest.cc:802] Training of tree  631/1000 (tree index:630) done accuracy:0.876326 logloss:0.316282\n",
      "[INFO 23-07-29 19:59:54.8871 CEST random_forest.cc:802] Training of tree  641/1000 (tree index:640) done accuracy:0.87637 logloss:0.316257\n",
      "[INFO 23-07-29 19:59:56.5047 CEST random_forest.cc:802] Training of tree  651/1000 (tree index:650) done accuracy:0.876631 logloss:0.316243\n",
      "[INFO 23-07-29 19:59:58.0236 CEST random_forest.cc:802] Training of tree  661/1000 (tree index:660) done accuracy:0.8765 logloss:0.316206\n",
      "[INFO 23-07-29 19:59:59.6882 CEST random_forest.cc:802] Training of tree  671/1000 (tree index:670) done accuracy:0.8765 logloss:0.316203\n",
      "[INFO 23-07-29 20:00:01.3051 CEST random_forest.cc:802] Training of tree  681/1000 (tree index:680) done accuracy:0.876109 logloss:0.316272\n",
      "[INFO 23-07-29 20:00:02.8241 CEST random_forest.cc:802] Training of tree  691/1000 (tree index:690) done accuracy:0.876152 logloss:0.316257\n",
      "[INFO 23-07-29 20:00:04.4845 CEST random_forest.cc:802] Training of tree  701/1000 (tree index:700) done accuracy:0.876022 logloss:0.316214\n",
      "[INFO 23-07-29 20:00:06.1289 CEST random_forest.cc:802] Training of tree  711/1000 (tree index:710) done accuracy:0.876761 logloss:0.31624\n",
      "[INFO 23-07-29 20:00:07.6204 CEST random_forest.cc:802] Training of tree  721/1000 (tree index:720) done accuracy:0.876631 logloss:0.31618\n",
      "[INFO 23-07-29 20:00:09.3377 CEST random_forest.cc:802] Training of tree  731/1000 (tree index:730) done accuracy:0.876544 logloss:0.316215\n",
      "[INFO 23-07-29 20:00:11.0403 CEST random_forest.cc:802] Training of tree  741/1000 (tree index:740) done accuracy:0.876587 logloss:0.316188\n",
      "[INFO 23-07-29 20:00:12.6270 CEST random_forest.cc:802] Training of tree  751/1000 (tree index:750) done accuracy:0.875935 logloss:0.316161\n",
      "[INFO 23-07-29 20:00:14.3139 CEST random_forest.cc:802] Training of tree  761/1000 (tree index:760) done accuracy:0.875978 logloss:0.316093\n",
      "[INFO 23-07-29 20:00:15.9909 CEST random_forest.cc:802] Training of tree  771/1000 (tree index:770) done accuracy:0.876065 logloss:0.31609\n",
      "[INFO 23-07-29 20:00:17.6021 CEST random_forest.cc:802] Training of tree  781/1000 (tree index:780) done accuracy:0.875978 logloss:0.316036\n",
      "[INFO 23-07-29 20:00:19.2320 CEST random_forest.cc:802] Training of tree  791/1000 (tree index:790) done accuracy:0.876587 logloss:0.315958\n",
      "[INFO 23-07-29 20:00:20.8234 CEST random_forest.cc:802] Training of tree  801/1000 (tree index:800) done accuracy:0.876283 logloss:0.315909\n",
      "[INFO 23-07-29 20:00:22.4892 CEST random_forest.cc:802] Training of tree  811/1000 (tree index:810) done accuracy:0.876152 logloss:0.315903\n",
      "[INFO 23-07-29 20:00:24.0716 CEST random_forest.cc:802] Training of tree  821/1000 (tree index:820) done accuracy:0.876196 logloss:0.315925\n",
      "[INFO 23-07-29 20:00:25.5696 CEST random_forest.cc:802] Training of tree  831/1000 (tree index:830) done accuracy:0.876196 logloss:0.315877\n",
      "[INFO 23-07-29 20:00:27.2279 CEST random_forest.cc:802] Training of tree  841/1000 (tree index:840) done accuracy:0.876413 logloss:0.315877\n",
      "[INFO 23-07-29 20:00:28.9615 CEST random_forest.cc:802] Training of tree  851/1000 (tree index:850) done accuracy:0.876457 logloss:0.315904\n",
      "[INFO 23-07-29 20:00:30.5020 CEST random_forest.cc:802] Training of tree  861/1000 (tree index:860) done accuracy:0.876457 logloss:0.315845\n",
      "[INFO 23-07-29 20:00:32.1469 CEST random_forest.cc:802] Training of tree  871/1000 (tree index:870) done accuracy:0.876022 logloss:0.31587\n",
      "[INFO 23-07-29 20:00:33.7902 CEST random_forest.cc:802] Training of tree  881/1000 (tree index:880) done accuracy:0.876239 logloss:0.315919\n",
      "[INFO 23-07-29 20:00:35.3963 CEST random_forest.cc:802] Training of tree  891/1000 (tree index:890) done accuracy:0.87637 logloss:0.315892\n",
      "[INFO 23-07-29 20:00:36.9833 CEST random_forest.cc:802] Training of tree  901/1000 (tree index:900) done accuracy:0.876109 logloss:0.315894\n",
      "[INFO 23-07-29 20:00:38.5639 CEST random_forest.cc:802] Training of tree  911/1000 (tree index:910) done accuracy:0.876109 logloss:0.315859\n",
      "[INFO 23-07-29 20:00:40.1861 CEST random_forest.cc:802] Training of tree  921/1000 (tree index:920) done accuracy:0.876413 logloss:0.315832\n",
      "[INFO 23-07-29 20:00:41.8082 CEST random_forest.cc:802] Training of tree  931/1000 (tree index:930) done accuracy:0.876196 logloss:0.315874\n",
      "[INFO 23-07-29 20:00:43.2930 CEST random_forest.cc:802] Training of tree  941/1000 (tree index:940) done accuracy:0.876283 logloss:0.31584\n",
      "[INFO 23-07-29 20:00:45.0978 CEST random_forest.cc:802] Training of tree  951/1000 (tree index:950) done accuracy:0.876283 logloss:0.315829\n",
      "[INFO 23-07-29 20:00:46.6926 CEST random_forest.cc:802] Training of tree  961/1000 (tree index:960) done accuracy:0.876283 logloss:0.31588\n",
      "[INFO 23-07-29 20:00:48.2281 CEST random_forest.cc:802] Training of tree  971/1000 (tree index:970) done accuracy:0.876326 logloss:0.315885\n",
      "[INFO 23-07-29 20:00:49.9962 CEST random_forest.cc:802] Training of tree  981/1000 (tree index:980) done accuracy:0.876022 logloss:0.315876\n",
      "[INFO 23-07-29 20:00:51.5717 CEST random_forest.cc:802] Training of tree  991/1000 (tree index:990) done accuracy:0.876239 logloss:0.315887\n",
      "[INFO 23-07-29 20:00:53.0274 CEST random_forest.cc:802] Training of tree  1000/1000 (tree index:999) done accuracy:0.87637 logloss:0.3159\n",
      "[INFO 23-07-29 20:00:53.0276 CEST random_forest.cc:882] Final OOB metrics: accuracy:0.87637 logloss:0.3159\n",
      "[INFO 23-07-29 20:00:58.7600 CEST kernel.cc:926] Export model in log directory: /tmp/tmpqx6vxa1k with prefix 08988c24896346b7\n",
      "[INFO 23-07-29 20:00:59.7707 CEST kernel.cc:944] Save model in resources\n",
      "[INFO 23-07-29 20:00:59.7735 CEST abstract_model.cc:849] Model self evaluation:\n",
      "Number of predictions (without weights): 22996\n",
      "Number of predictions (with weights): 22996\n",
      "Task: CLASSIFICATION\n",
      "Label: __LABEL\n",
      "\n",
      "Accuracy: 0.87637  CI95[W][0.872743 0.879927]\n",
      "LogLoss: : 0.3159\n",
      "ErrorRate: : 0.12363\n",
      "\n",
      "Default Accuracy: : 0.609367\n",
      "Default LogLoss: : 0.66903\n",
      "Default ErrorRate: : 0.390633\n",
      "\n",
      "Confusion Table:\n",
      "truth\\prediction\n",
      "   0      1     2\n",
      "0  0      0     0\n",
      "1  0  12742  1271\n",
      "2  0   1572  7411\n",
      "Total: 22996\n",
      "\n",
      "One vs other classes:\n",
      "\n",
      "[INFO 23-07-29 20:01:00.2124 CEST kernel.cc:1243] Loading model from path /tmp/tmpqx6vxa1k/model/ with prefix 08988c24896346b7\n",
      "[INFO 23-07-29 20:01:06.1861 CEST decision_forest.cc:660] Model loaded with 1000 root(s), 1990518 node(s), and 768 input feature(s).\n",
      "[INFO 23-07-29 20:01:06.1862 CEST abstract_model.cc:1311] Engine \"RandomForestOptPred\" built\n",
      "[INFO 23-07-29 20:01:06.1865 CEST kernel.cc:1075] Use fast generic engine\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained in 0:02:56.954935\n",
      "Compiling model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-29 20:01:06.708733: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype int64 and shape [22996]\n",
      "\t [[{{node Placeholder/_1}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model compiled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-29 20:01:11.487997: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype int64 and shape [8679]\n",
      "\t [[{{node Placeholder/_1}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "290/290 [==============================] - 4s 6ms/step - loss: 0.0000e+00 - accuracy: 0.9054\n",
      "BinaryCrossentropyloss: 0.0\n",
      "Accuracy: 0.9054038524627686\n"
     ]
    }
   ],
   "source": [
    "def prepare_dataset4(s):\n",
    "    m = json.loads(s)\n",
    "    return m\n",
    "\n",
    "test_cases = list(map(prepare_dataset4, test_lines))\n",
    "print(len(test_lines))\n",
    "print(\"Data loaded.\")\n",
    "\n",
    "# Specify the model.\n",
    "model_1 = tfdf.keras.RandomForestModel(num_trees=1000, verbose=2, num_threads=3, check_dataset=False)\n",
    "\n",
    "# Train the model.\n",
    "model_1.fit(x=train_ds, batch_size=None)\n",
    "model_1.compile(metrics=[\"accuracy\"])\n",
    "evaluation = model_1.evaluate(test_ds, batch_size=None)\n",
    "\n",
    "print(f\"BinaryCrossentropyloss: {evaluation[0]}\")\n",
    "print(f\"Accuracy: {evaluation[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "290/290 [==============================] - 3s 6ms/step\n",
      "cut: 0.25, acc: 0.7452471482889734, prec: 0.5036248300860897, rec: 0.9910833704859563, f1: 0.6678684091933303\n",
      "cut: 0.3, acc: 0.8034335752966931, prec: 0.5693619219839835, rec: 0.9826125724476148, f1: 0.7209682695453059\n",
      "cut: 0.35, acc: 0.8492913930176288, prec: 0.6363371245261009, rec: 0.9728042799821668, f1: 0.7693935119887166\n",
      "cut: 0.39999999999999997, acc: 0.8778661136075585, prec: 0.692733789507983, rec: 0.9478377173428444, f1: 0.8004518072289155\n",
      "cut: 0.44999999999999996, acc: 0.897223182394285, prec: 0.7477081041437477, rec: 0.9090503789567543, f1: 0.820523138832998\n",
      "cut: 0.49999999999999994, acc: 0.9054038483696278, prec: 0.7933168316831684, rec: 0.8573339277753009, f1: 0.8240839940004286\n",
      "cut: 0.5499999999999999, acc: 0.906095172254868, prec: 0.8352112676056338, rec: 0.7931341952741864, f1: 0.8136290875828951\n",
      "cut: 0.5999999999999999, acc: 0.899988477935246, prec: 0.8706199460916442, rec: 0.7200178332590281, f1: 0.7881893606637385\n",
      "cut: 0.6499999999999999, acc: 0.8891577370664823, prec: 0.906150919467343, rec: 0.6370931787784218, f1: 0.7481675392670156\n",
      "cut: 0.7, acc: 0.8731420670584169, prec: 0.9312688821752266, rec: 0.5497102095407935, f1: 0.6913372582001681\n",
      "cut: 0.7499999999999999, acc: 0.8526327917962899, prec: 0.9530075187969925, rec: 0.45207311636201514, f1: 0.6132446325975204\n"
     ]
    }
   ],
   "source": [
    "p = model_1.predict(test_ds)\n",
    "\n",
    "#Fish for best cut\n",
    "for cut in np.arange(0.25, .8, .05):\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    N = len(p)\n",
    "    for i in range(N):\n",
    "        tru = test_cases[i][\"label\"] == '1'\n",
    "        prd = p[i] > cut\n",
    "        if tru and prd == tru:\n",
    "            tp = tp + 1\n",
    "        if tru and prd != tru:\n",
    "            fn = fn + 1\n",
    "        if tru == False and prd == tru:\n",
    "            tn = tn + 1\n",
    "        if tru == False and prd != tru:\n",
    "            fp = fp + 1\n",
    "    precision = tp/(tp+fp)\n",
    "    recall = tp/(tp+fn)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    print(\"cut: {}, acc: {}, prec: {}, rec: {}, f1: {}\".format(cut, (tp+tn)/N, precision, recall, f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
